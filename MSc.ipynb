{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f315f24b",
   "metadata": {
    "papermill": {
     "duration": 0.010384,
     "end_time": "2024-08-09T16:44:23.188179",
     "exception": false,
     "start_time": "2024-08-09T16:44:23.177795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Install the following libraries\n",
    "\n",
    "1. pip install efficientnet_pytorch\n",
    "\n",
    "2. pip install ivtmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da64b14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:44:23.211118Z",
     "iopub.status.busy": "2024-08-09T16:44:23.210679Z",
     "iopub.status.idle": "2024-08-09T16:45:00.254922Z",
     "shell.execute_reply": "2024-08-09T16:45:00.253833Z"
    },
    "papermill": {
     "duration": 37.057772,
     "end_time": "2024-08-09T16:45:00.257739",
     "exception": false,
     "start_time": "2024-08-09T16:44:23.199967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet_pytorch) (2.1.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (2024.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\r\n",
      "Building wheels for collected packages: efficientnet_pytorch\r\n",
      "  Building wheel for efficientnet_pytorch (setup.py) ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=4f48d13dc4993a635125b8e64c2b0f07c226d5c4111b5e208f20cb64bd52109b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\r\n",
      "Successfully built efficientnet_pytorch\r\n",
      "Installing collected packages: efficientnet_pytorch\r\n",
      "Successfully installed efficientnet_pytorch-0.7.1\r\n",
      "Collecting ivtmetrics\r\n",
      "  Downloading ivtmetrics-0.1.2.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l-\b \b\\\b \bdone\r\n",
      "\u001B[?25hRequirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from ivtmetrics) (1.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from ivtmetrics) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.2->ivtmetrics) (1.11.4)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.2->ivtmetrics) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.2->ivtmetrics) (3.2.0)\r\n",
      "Building wheels for collected packages: ivtmetrics\r\n",
      "  Building wheel for ivtmetrics (setup.py) ... \u001B[?25l-\b \b\\\b \bdone\r\n",
      "\u001B[?25h  Created wheel for ivtmetrics: filename=ivtmetrics-0.1.2-py3-none-any.whl size=15830 sha256=d963c6f00b1f6b7e06c1ab04bd11c8bc5020fa182c724f61a79ecd525e64e62c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/6e/d3/85e46407b1dbbf553b1730f4a3b10504a7fdf3e667637b965e\r\n",
      "Successfully built ivtmetrics\r\n",
      "Installing collected packages: ivtmetrics\r\n",
      "Successfully installed ivtmetrics-0.1.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch\n",
    "!pip install ivtmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "207fe82e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:45:00.283725Z",
     "iopub.status.busy": "2024-08-09T16:45:00.283308Z",
     "iopub.status.idle": "2024-08-09T16:45:07.659501Z",
     "shell.execute_reply": "2024-08-09T16:45:07.658341Z"
    },
    "papermill": {
     "duration": 7.392437,
     "end_time": "2024-08-09T16:45:07.662344",
     "exception": false,
     "start_time": "2024-08-09T16:45:00.269907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for dataset reading\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "\n",
    "# for model developing\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as basemodels\n",
    "import torchvision.transforms as transforms\n",
    "from efficientnet_pytorch import EfficientNet # pip install efficientnet_pytorch\n",
    "import math\n",
    "\n",
    "# for dataset loading and training, parameters\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "# import network\n",
    "import argparse\n",
    "import platform\n",
    "import ivtmetrics\n",
    "# import dataloader\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "# from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4075bcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:45:07.689391Z",
     "iopub.status.busy": "2024-08-09T16:45:07.688470Z",
     "iopub.status.idle": "2024-08-09T16:45:07.757538Z",
     "shell.execute_reply": "2024-08-09T16:45:07.756380Z"
    },
    "papermill": {
     "duration": 0.084849,
     "end_time": "2024-08-09T16:45:07.759940",
     "exception": false,
     "start_time": "2024-08-09T16:45:07.675091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# enable gpu of kaggle\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb5fab",
   "metadata": {
    "papermill": {
     "duration": 0.011876,
     "end_time": "2024-08-09T16:45:07.783982",
     "exception": false,
     "start_time": "2024-08-09T16:45:07.772106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Process for the CholecT50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bcd867",
   "metadata": {
    "papermill": {
     "duration": 0.011555,
     "end_time": "2024-08-09T16:45:07.808147",
     "exception": false,
     "start_time": "2024-08-09T16:45:07.796592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Original Dataloader for CholecT50, but change to T45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3143373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:45:07.835707Z",
     "iopub.status.busy": "2024-08-09T16:45:07.835312Z",
     "iopub.status.idle": "2024-08-09T16:45:07.852503Z",
     "shell.execute_reply": "2024-08-09T16:45:07.851418Z"
    },
    "papermill": {
     "duration": 0.033791,
     "end_time": "2024-08-09T16:45:07.855140",
     "exception": false,
     "start_time": "2024-08-09T16:45:07.821349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 加载单个视频数据及其标注(即读图片和JSON标注)\n",
    "# class CholecT50Video(Dataset):\n",
    "#     # 读取JSON文件\n",
    "#     def __init__(self, img_dir, label_file, transform=None, target_transform=None, frame_interval=25): # 注意每隔25帧取一张图\n",
    "#         with open(label_file, 'r') as f:\n",
    "#             self.labels = json.load(f) # 读取JSON文件，获取标签信息\n",
    "#         self.img_dir = img_dir # 图像文件所在目录\n",
    "#         self.transform = transform # 图像变换函数\n",
    "#         self.target_transform = target_transform # 目标变换函数（可选）\n",
    "#         self.num_frames = self.labels['num_frames'] # 获取帧数\n",
    "#         self.valid_indices = self._get_valid_indices() # 跳过triplet_label 为-1的图片\n",
    "    \n",
    "#     def _get_valid_indices(self):\n",
    "#         valid_indices = []\n",
    "#         for index in range(self.num_frames):\n",
    "#             frame_id = str(index)\n",
    "#             annotation_instances = self.labels['annotations'].get(frame_id, [])\n",
    "#             triplet_labels = [inst[0] for inst in annotation_instances]\n",
    "#             if all(label != -1 for label in triplet_labels):\n",
    "#                 valid_indices.append(index)\n",
    "#         return valid_indices\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.valid_indices)\n",
    "    \n",
    "#     # 根据帧 ID 获取对应的标注。\n",
    "#     def __getitem__(self, index):\n",
    "# #         frame_id = str(index) # 将索引转换为字符串形式\n",
    "#         valid_index = self.valid_indices[index] # 只挑选可用图片\n",
    "#         frame_id = str(valid_index)\n",
    "#         annotation_instances = self.labels['annotations'].get(frame_id, []) # 获取该帧对应的标签实例列表\n",
    "#         basename = \"{}.png\".format(str(index).zfill(6)) # 构建图像文件名，格式为6位数的字符串\n",
    "#         img_path = os.path.join(self.img_dir, basename) # 构建图像文件的完整路径\n",
    "# #         image = Image.open(img_path) # 打开图像\n",
    "#         image = Image.open(img_path).convert('RGB') # 打开图像（更新）\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image) # 图像增强\n",
    "\n",
    "#         # 更正：提取不同类型的标签\n",
    "# #         triplet_labels = [inst[0] for inst in annotation_instances]  # 三元组标签 (0)：如果为-1，则代表图片中没有完整的工具动作和目标，此类图片需要跳过\n",
    "# #         instrument_labels = [inst[1] for inst in annotation_instances]  # 工具标签 (1)\n",
    "# #         verb_labels = [inst[7] for inst in annotation_instances]  # 动作标签 (7)\n",
    "# # #         target_labels = [inst[8:14] for inst in annotation_instances]  # 目标标签 (8 到 12)\n",
    "# #         phase_labels = [inst[14] for inst in annotation_instances]  # 阶段标签 (14)\n",
    "\n",
    "# # Select only one annotation per frame, here we take the first one\n",
    "#         if annotation_instances:\n",
    "#             annotation = annotation_instances[0]\n",
    "#             instrument_labels = annotation[1]\n",
    "#             verb_labels = annotation[7]\n",
    "#             phase_labels = annotation[14]\n",
    "#         else:\n",
    "#             instrument_labels = []\n",
    "#             verb_labels = []\n",
    "#             phase_labels = []\n",
    "\n",
    "#         return image, (instrument_labels, verb_labels, phase_labels)\n",
    "\n",
    "\n",
    "# # 数据集读取\n",
    "# class CholecT50():\n",
    "#     def __init__(self, \n",
    "#                 dataset_dir, # dataset_dir：数据集的根目录\n",
    "#                 dataset_variant=\"cholect50-crossval\", # dataset_variant：数据集的变体。\n",
    "#                 test_fold=1, # test_fold：交叉验证时的测试集折数。 \n",
    "#                 augmentation_list=['original', 'vflip', 'hflip', 'contrast', 'rot90']): # augmentation_list：数据增强列表    \n",
    "#         self.dataset_dir = dataset_dir\n",
    "#         self.list_dataset_variant = { # self.list_dataset_variant：保存数据集变体的描述\n",
    "#             \"cholect50-crossval\": \"for CholecT50 dataset variant with the official cross-validation splits\",\n",
    "#             \"cholect50\": \"for the CholecT50 dataset with original splits used in rendezvous paper\",\n",
    "#         }\n",
    "        \n",
    "#         # 检查 dataset_variant 是否有效。\n",
    "#         # 根据数据集变体选择数据分割。\n",
    "#         assert dataset_variant in self.list_dataset_variant.keys(), f\"{dataset_variant} is not a valid dataset variant\"\n",
    "#         video_split  = self.split_selector(case=dataset_variant)\n",
    "#         train_videos = sum([v for k,v in video_split.items() if k!=test_fold], []) if 'crossval' in dataset_variant else video_split['train']\n",
    "#         test_videos  = sum([v for k,v in video_split.items() if k==test_fold], []) if 'crossval' in dataset_variant else video_split['test']\n",
    "        \n",
    "#         #根据是否是交叉验证，划分训练集、验证集和测试集的视频。随后将视频编号格式化为 VIDxx。\n",
    "#         if 'crossval' in dataset_variant:\n",
    "#             val_videos   = train_videos[-5:]\n",
    "#             train_videos = train_videos[:-5]\n",
    "#         else:\n",
    "#             val_videos   = video_split['val']\n",
    "#         self.train_records = ['VID{}'.format(str(v).zfill(2)) for v in train_videos]\n",
    "#         self.val_records   = ['VID{}'.format(str(v).zfill(2)) for v in val_videos]\n",
    "#         self.test_records  = ['VID{}'.format(str(v).zfill(2)) for v in test_videos]\n",
    "        \n",
    "#         # 定义各种数据增强方法。根据 augmentation_list 选择要应用的增强方法。\n",
    "#         self.augmentations = {\n",
    "#             'original': self.no_augmentation,\n",
    "#             'vflip': transforms.RandomVerticalFlip(0.4),\n",
    "#             'hflip': transforms.RandomHorizontalFlip(0.4),\n",
    "#             'contrast': transforms.ColorJitter(brightness=0.1, contrast=0.2, saturation=0, hue=0),\n",
    "#             'rot90': transforms.RandomRotation(90,expand=True),\n",
    "#             'brightness': transforms.RandomAdjustSharpness(sharpness_factor=1.6, p=0.5),\n",
    "#             'contrast': transforms.RandomAutocontrast(p=0.5),\n",
    "#         }\n",
    "#         self.augmentation_list = []\n",
    "#         for aug in augmentation_list:\n",
    "#             self.augmentation_list.append(self.augmentations[aug])\n",
    "            \n",
    "#         # 获取训练和测试图像的变换方法\n",
    "#         trainform, testform = self.transform()\n",
    "        \n",
    "#         # 调用函数构建训练、验证和测试数据集\n",
    "#         self.build_train_dataset(trainform)\n",
    "#         self.build_val_dataset(trainform)\n",
    "#         self.build_test_dataset(testform)\n",
    "    \n",
    "#     # 列出数据集变体和数据增强方法。\n",
    "#     def list_dataset_variants(self):\n",
    "#         print(self.list_dataset_variant)\n",
    "        \n",
    "#     def list_augmentations(self):\n",
    "#         print(self.augmentations.keys())\n",
    "\n",
    "#     # 据不同的 case 返回相应的数据分割\n",
    "#     def split_selector(self, case='cholect50'):\n",
    "#         switcher = {\n",
    "#             'cholect50': {\n",
    "#                 'train': [1, 15, 26, 40, 52, 65, 79, 2, 18, 27, 43, 56, 66, 92, 4, 22, 31, 47, 57, 68, 96, 5, 23, 35, 48, 60, 70, 103, 13, 25, 36, 49, 62, 75, 110],\n",
    "#                 'val'  : [8, 12, 29, 50, 78],\n",
    "#                 'test' : [6, 51, 10, 73, 14, 74, 32, 80, 42, 111]\n",
    "#             },\n",
    "#             'cholect50-crossval': {\n",
    "#                 1: [79,  2, 51,  6, 25, 14, 66, 23, 50, 111],\n",
    "#                 2: [80, 32,  5, 15, 40, 47, 26, 48, 70,  96],\n",
    "#                 3: [31, 57, 36, 18, 52, 68, 10,  8, 73, 103],\n",
    "#                 4: [42, 29, 60, 27, 65, 75, 22, 49, 12, 110],\n",
    "#                 5: [78, 43, 62, 35, 74,  1, 56,  4, 13,  92],\n",
    "#             },\n",
    "#         }\n",
    "#         return switcher.get(case)\n",
    "\n",
    "#     # 定义不进行数据增强的方法。\n",
    "#     def no_augmentation(self, x):\n",
    "#         return x\n",
    "\n",
    "#     # 定义训练和测试数据的图像变换\n",
    "#     def transform(self):\n",
    "#         normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#         op_test   = [transforms.Resize((256, 448)), transforms.ToTensor(), normalize,]\n",
    "#         op_train  = [transforms.Resize((256, 448))] + self.augmentation_list + [transforms.Resize((256, 448)), transforms.ToTensor(), normalize,]\n",
    "#         testform  = transforms.Compose(op_test)\n",
    "#         trainform = transforms.Compose(op_train)\n",
    "#         return trainform, testform\n",
    "\n",
    "#     # 构建训练数据集\n",
    "#     def build_train_dataset(self, transform):\n",
    "#         iterable_dataset = [] # 创建一个空列表，用于存储每个视频的数据集对象\n",
    "#         for video in self.train_records:\n",
    "#             label_file = os.path.join(self.dataset_dir, 'labels', '{}.json'.format(video)) # 构建对应视频的标签文件路径\n",
    "#             if not os.path.exists(label_file):\n",
    "#                 print(f\"Label file not found: {label_file}\")\n",
    "#                 continue\n",
    "\n",
    "#             dataset = CholecT50Video(\n",
    "#                 img_dir=os.path.join(self.dataset_dir, 'videos', video),\n",
    "#                 label_file=label_file,\n",
    "#                 transform=transform\n",
    "#             )\n",
    "#             iterable_dataset.append(dataset) # 将数据集对象添加到 iterable_dataset 列表中。\n",
    "#         self.train_dataset = ConcatDataset(iterable_dataset) # 使用 ConcatDataset 将所有视频的数据集对象合并成一个训练数据集\n",
    "\n",
    "#     # 构建验证数据集\n",
    "#     def build_val_dataset(self, transform):\n",
    "#         iterable_dataset = []\n",
    "#         for video in self.val_records:\n",
    "#             label_file = os.path.join(self.dataset_dir, 'labels', '{}.json'.format(video))\n",
    "#             if not os.path.exists(label_file):\n",
    "#                 print(f\"Label file not found: {label_file}\")\n",
    "#                 continue\n",
    "\n",
    "#             dataset = CholecT50Video(\n",
    "#                 img_dir=os.path.join(self.dataset_dir, 'videos', video),\n",
    "#                 label_file=label_file,\n",
    "#                 transform=transform\n",
    "#             )\n",
    "#             iterable_dataset.append(dataset) \n",
    "#         self.val_dataset = ConcatDataset(iterable_dataset) \n",
    "\n",
    "#     # 构建测试数据集\n",
    "#     def build_test_dataset(self, transform):\n",
    "#         iterable_dataset = []\n",
    "#         for video in self.test_records:\n",
    "#             label_file = os.path.join(self.dataset_dir, 'labels', '{}.json'.format(video))\n",
    "#             if not os.path.exists(label_file):\n",
    "#                 print(f\"Label file not found: {label_file}\")\n",
    "#                 continue\n",
    "\n",
    "#             dataset = CholecT50Video(\n",
    "#                 img_dir=os.path.join(self.dataset_dir, 'videos', video),\n",
    "#                 label_file=label_file,\n",
    "#                 transform=transform\n",
    "#             )\n",
    "#             iterable_dataset.append(dataset)\n",
    "#         self.test_dataset = iterable_dataset # 注意，这里 self.test_dataset 并未使用 ConcatDataset，这可能是因为测试集在某些情况下需要保持单独的视频数据集。\n",
    "        \n",
    "#     def build(self):\n",
    "#         return (self.train_dataset, self.val_dataset, self.test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a7cd41",
   "metadata": {
    "papermill": {
     "duration": 0.011897,
     "end_time": "2024-08-09T16:45:07.879274",
     "exception": false,
     "start_time": "2024-08-09T16:45:07.867377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Process for the CholecT45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c665371",
   "metadata": {
    "papermill": {
     "duration": 0.01163,
     "end_time": "2024-08-09T16:45:07.903013",
     "exception": false,
     "start_time": "2024-08-09T16:45:07.891383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data loader of the CholecT45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44239c83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:45:07.929367Z",
     "iopub.status.busy": "2024-08-09T16:45:07.929000Z",
     "iopub.status.idle": "2024-08-09T16:45:07.971996Z",
     "shell.execute_reply": "2024-08-09T16:45:07.970829Z"
    },
    "papermill": {
     "duration": 0.059432,
     "end_time": "2024-08-09T16:45:07.974657",
     "exception": false,
     "start_time": "2024-08-09T16:45:07.915225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading, augmentation, and preparation for CholecT45\n",
    "class CholecT50():\n",
    "    def __init__(self, \n",
    "                dataset_dir, \n",
    "                dataset_variant=\"cholect45-crossval\",\n",
    "                test_fold=1,  # choose a data fold as the test set.\n",
    "                augmentation_list=['original', 'vflip', 'hflip', 'contrast', 'rot90']):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.list_dataset_variant = {\n",
    "            \"cholect45-crossval\": \"for CholecT45 dataset variant with the official cross-validation splits.\",\n",
    "            \"cholect45\": \"a pointer to cholect45-crossval\",\n",
    "        }\n",
    "        assert dataset_variant in self.list_dataset_variant.keys(), print(dataset_variant, \"is not a valid dataset variant\")\n",
    "        \n",
    "        # split the dataset into training, validation, and testing\n",
    "        video_split  = self.split_selector(case=dataset_variant)\n",
    "        train_videos = sum([v for k,v in video_split.items() if k!=test_fold], []) if 'crossval' in dataset_variant else video_split['train']\n",
    "        test_videos  = sum([v for k,v in video_split.items() if k==test_fold], []) if 'crossval' in dataset_variant else video_split['test']\n",
    "        if 'crossval' in dataset_variant:\n",
    "            val_videos   = train_videos[-5:]\n",
    "            train_videos = train_videos[:-5]\n",
    "        else:\n",
    "            val_videos   = video_split['val']\n",
    "        self.train_records = ['VID{}'.format(str(v).zfill(2)) for v in train_videos]\n",
    "        self.val_records   = ['VID{}'.format(str(v).zfill(2)) for v in val_videos]\n",
    "        self.test_records  = ['VID{}'.format(str(v).zfill(2)) for v in test_videos]\n",
    "        self.augmentations = {\n",
    "            'original': self.no_augumentation,\n",
    "            'hflip': transforms.RandomHorizontalFlip(0.4),\n",
    "            'rot90': transforms.RandomRotation(90,expand=True),\n",
    "            'contrast': transforms.RandomAutocontrast(p=0.5),\n",
    "            'vflip': transforms.RandomVerticalFlip(0.4),\n",
    "            'contrast': transforms.ColorJitter(brightness=0.1, contrast=0.2, saturation=0, hue=0),\n",
    "            'brightness': transforms.RandomAdjustSharpness(sharpness_factor=1.6, p=0.5),\n",
    "        }\n",
    "        self.augmentation_list = []\n",
    "        for aug in augmentation_list:\n",
    "            self.augmentation_list.append(self.augmentations[aug])\n",
    "        trainform, testform = self.transform()\n",
    "        self.build_train_dataset(trainform)\n",
    "        self.build_val_dataset(trainform)\n",
    "        self.build_test_dataset(testform)\n",
    "    \n",
    "    def list_dataset_variants(self):\n",
    "        print(self.list_dataset_variant)\n",
    "\n",
    "    def list_augmentations(self):\n",
    "        print(self.augmentations.keys())\n",
    "\n",
    "    def split_selector(self, case='cholect50'):\n",
    "        switcher = {\n",
    "            'cholect50': {\n",
    "                'train': [1, 15, 26, 40, 52, 65, 79, 2, 18, 27, 43, 56, 66, 92, 4, 22, 31, 47, 57, 68, 96, 5, 23, 35, 48, 60, 70, 103, 13, 25, 36, 49, 62, 75, 110],\n",
    "                'val'  : [8, 12, 29, 50, 78],\n",
    "                'test' : [6, 51, 10, 73, 14, 74, 32, 80, 42, 111]\n",
    "            },\n",
    "            'cholect45-crossval': {\n",
    "                1: [79,  2, 51,  6, 25, 14, 66, 23, 50,],\n",
    "                2: [80, 32,  5, 15, 40, 47, 26, 48, 70,],\n",
    "                3: [31, 57, 36, 18, 52, 68, 10,  8, 73,],\n",
    "                4: [42, 29, 60, 27, 65, 75, 22, 49, 12,],\n",
    "                5: [78, 43, 62, 35, 74,  1, 56,  4, 13,],\n",
    "            },\n",
    "        }\n",
    "        return switcher.get(case)\n",
    "    \n",
    "    # data transformation\n",
    "    def transform(self):\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        op_test   = [transforms.Resize((256, 448)), transforms.ToTensor(), normalize,]\n",
    "        op_train  = [transforms.Resize((256, 448))] + self.augmentation_list + [transforms.Resize((256, 448)), transforms.ToTensor(), normalize,]\n",
    "        testform  = transforms.Compose(op_test)\n",
    "        trainform = transforms.Compose(op_train)\n",
    "        return trainform, testform\n",
    "\n",
    "    def build_train_dataset(self, transform):\n",
    "        iterable_dataset = []\n",
    "        for video in self.train_records:\n",
    "            dataset = T50(img_dir = os.path.join(self.dataset_dir, 'data', video), \n",
    "                        triplet_file = os.path.join(self.dataset_dir, 'triplet', '{}.txt'.format(video)), \n",
    "                        tool_file = os.path.join(self.dataset_dir, 'instrument', '{}.txt'.format(video)),  \n",
    "                        verb_file = os.path.join(self.dataset_dir, 'verb', '{}.txt'.format(video)),  \n",
    "                        target_file = os.path.join(self.dataset_dir, 'target', '{}.txt'.format(video)), \n",
    "                        transform=transform)\n",
    "            iterable_dataset.append(dataset)\n",
    "        self.train_dataset = ConcatDataset(iterable_dataset)\n",
    "\n",
    "    def build_val_dataset(self, transform):\n",
    "        iterable_dataset = []\n",
    "        for video in self.val_records:\n",
    "            dataset = T50(img_dir = os.path.join(self.dataset_dir, 'data', video), \n",
    "                        triplet_file = os.path.join(self.dataset_dir, 'triplet', '{}.txt'.format(video)), \n",
    "                        tool_file = os.path.join(self.dataset_dir, 'instrument', '{}.txt'.format(video)),  \n",
    "                        verb_file = os.path.join(self.dataset_dir, 'verb', '{}.txt'.format(video)),  \n",
    "                        target_file = os.path.join(self.dataset_dir, 'target', '{}.txt'.format(video)), \n",
    "                        transform=transform)\n",
    "            iterable_dataset.append(dataset)\n",
    "        self.val_dataset = ConcatDataset(iterable_dataset)\n",
    "\n",
    "    def build_test_dataset(self, transform):\n",
    "        iterable_dataset = []\n",
    "        for video in self.test_records:\n",
    "            dataset = T50(img_dir = os.path.join(self.dataset_dir, 'data', video), \n",
    "                triplet_file = os.path.join(self.dataset_dir, 'triplet', '{}.txt'.format(video)), \n",
    "                tool_file = os.path.join(self.dataset_dir, 'instrument', '{}.txt'.format(video)),  \n",
    "                verb_file = os.path.join(self.dataset_dir, 'verb', '{}.txt'.format(video)),  \n",
    "                target_file = os.path.join(self.dataset_dir, 'target', '{}.txt'.format(video)), \n",
    "                transform=transform)\n",
    "            iterable_dataset.append(dataset)\n",
    "        self.test_dataset = iterable_dataset\n",
    "        \n",
    "    def build(self):\n",
    "        return (self.train_dataset, self.val_dataset, self.test_dataset)\n",
    "    \n",
    "    def no_augumentation(self, x):\n",
    "        return x\n",
    "\n",
    "    \n",
    "class T50(Dataset):\n",
    "    def __init__(self, img_dir, triplet_file, tool_file, verb_file, target_file, transform=None, target_transform=None):\n",
    "        # fix the error of \"dtype=np.int\" in Kaggle\n",
    "#         self.triplet_labels = np.loadtxt(triplet_file, dtype=np.int, delimiter=',',)\n",
    "#         self.tool_labels = np.loadtxt(tool_file, dtype=np.int, delimiter=',',)\n",
    "#         self.verb_labels = np.loadtxt(verb_file, dtype=np.int, delimiter=',',)\n",
    "#         self.target_labels = np.loadtxt(target_file, dtype=np.int, delimiter=',',)\n",
    "        # load the label data\n",
    "        self.triplet_labels = np.loadtxt(triplet_file, dtype=int, delimiter=',',)\n",
    "        self.tool_labels = np.loadtxt(tool_file, dtype=int, delimiter=',',)\n",
    "        self.verb_labels = np.loadtxt(verb_file, dtype=int, delimiter=',',)\n",
    "        self.target_labels = np.loadtxt(target_file, dtype=int, delimiter=',',)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    # number of samples\n",
    "    def __len__(self):\n",
    "        return len(self.triplet_labels)\n",
    "    \n",
    "    # get an image and its label\n",
    "    def __getitem__(self, index):\n",
    "        triplet_label = self.triplet_labels[index, 1:]\n",
    "        tool_label = self.tool_labels[index, 1:]\n",
    "        verb_label = self.verb_labels[index, 1:]\n",
    "        target_label = self.target_labels[index, 1:]\n",
    "        basename = \"{}.png\".format(str(self.triplet_labels[index, 0]).zfill(6))\n",
    "        img_path = os.path.join(self.img_dir, basename)\n",
    "        image    = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            triplet_label = self.target_transform(triplet_label)\n",
    "        return image, (tool_label, verb_label, target_label, triplet_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd8f6d",
   "metadata": {
    "papermill": {
     "duration": 0.011952,
     "end_time": "2024-08-09T16:45:07.999131",
     "exception": false,
     "start_time": "2024-08-09T16:45:07.987179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Rendezvous with improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4fcbff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:45:08.026387Z",
     "iopub.status.busy": "2024-08-09T16:45:08.025646Z",
     "iopub.status.idle": "2024-08-09T16:45:08.150117Z",
     "shell.execute_reply": "2024-08-09T16:45:08.148963Z"
    },
    "papermill": {
     "duration": 0.141556,
     "end_time": "2024-08-09T16:45:08.152954",
     "exception": false,
     "start_time": "2024-08-09T16:45:08.011398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rendezvous class: the main model class, consist of an encoder and a decoder\n",
    "# encoder: modules for feature extraction and processing\n",
    "# decoder: processes the encoded features to generate predictions\n",
    "\n",
    "# the dimensions of the output feature maps\n",
    "OUT_HEIGHT = 8\n",
    "OUT_WIDTH  = 14\n",
    "\n",
    "class Rendezvous(nn.Module):\n",
    "    # hr_output: whether to increase resolution\n",
    "    # use_ln: whether to use layer normalization\n",
    "    def __init__(self, basename=\"resnet18\", num_tool=6, num_verb=10, num_target=15, num_triplet=100, layer_size=8, num_heads=4, d_model=128, hr_output=False, use_ln=False):\n",
    "        super(Rendezvous, self).__init__()\n",
    "        self.encoder = Encoder(basename, num_tool, num_verb, num_target, num_triplet, hr_output=hr_output)\n",
    "        self.decoder = Decoder(layer_size, d_model, num_heads, num_triplet, use_ln=use_ln)    \n",
    "     \n",
    "    def forward(self, inputs):\n",
    "        enc_i, enc_v, enc_t, enc_ivt = self.encoder(inputs)\n",
    "        dec_ivt = self.decoder(enc_i, enc_v, enc_t, enc_ivt)\n",
    "        return enc_i, enc_v, enc_t, dec_ivt\n",
    "    \n",
    "\n",
    "# Encoder: process input images\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, basename='resnet18', num_tool=6,  num_verb=10, num_target=15, num_triplet=100, hr_output=False):\n",
    "    # def after modify basemodel\n",
    "#     def __init__(self, basename='efficientnet-b0', num_tool=6, num_verb=10, num_target=15, num_triplet=100, hr_output=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        # base model of resnet18\n",
    "        depth = 64 if basename == 'resnet18' else 128\n",
    "        # base model of efficientnet-b0\n",
    "#         depth = 128 if basename == 'efficientnet-b0' else 64\n",
    "        self.basemodel  = BaseModel(basename, hr_output)\n",
    "        self.wsl        = WSL(num_tool, depth)\n",
    "        self.cagam      = CAGAM(num_tool, num_verb, num_target)\n",
    "        self.bottleneck = Bottleneck(num_triplet) # reduce dimensionality of features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        high_x, low_x = self.basemodel(x)\n",
    "        enc_i         = self.wsl(high_x)\n",
    "        enc_v, enc_t  = self.cagam(high_x, enc_i[0])\n",
    "        enc_ivt       = self.bottleneck(low_x)\n",
    "        return enc_i, enc_v, enc_t, enc_ivt\n",
    "\n",
    "\n",
    "# #%%  Decoder with the original MultiHead Attention Decoder\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, layer_size, d_model, num_heads, num_class=100, use_ln=False):\n",
    "#         super(Decoder, self).__init__()        \n",
    "#         self.projection = nn.ModuleList([Projection(num_triplet=num_class, out_depth=d_model) for i in range(layer_size)])\n",
    "#         self.mhma       = nn.ModuleList([MHMA(num_class=num_class, depth=d_model, num_heads=num_heads, use_ln=use_ln) for i in range(layer_size)])\n",
    "#         self.ffnet      = nn.ModuleList([FFN(k=layer_size-i-1, num_class=num_class, use_ln=use_ln) for i in range(layer_size)])\n",
    "#         self.classifier = Classifier(num_class)\n",
    "        \n",
    "#     def forward(self, enc_i, enc_v, enc_t, enc_ivt):\n",
    "#         X = enc_ivt.clone()\n",
    "#         for P, M, F in zip(self.projection, self.mhma, self.ffnet):\n",
    "#             X = P(enc_i[0], enc_v[0], enc_t[0], X)\n",
    "#             X = M(X)\n",
    "#             X = F(X)\n",
    "#         logits = self.classifier(X)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# feature extraction backbone\n",
    "class BaseModel(nn.Module):   \n",
    "    def __init__(self, basename='resnet18', hr_output=False, *args):\n",
    "        super(BaseModel, self).__init__(*args)\n",
    "        self.output_feature = {} \n",
    "#         if basename == 'resnet18':\n",
    "        self.basemodel      = basemodels.resnet18(pretrained=True)     \n",
    "        if hr_output: self.increase_resolution() # increase resolution of the output feature maps\n",
    "        self.basemodel.layer1[1].bn2.register_forward_hook(self.get_activation('low_level_feature'))\n",
    "        self.basemodel.layer4[1].bn2.register_forward_hook(self.get_activation('high_level_feature'))        \n",
    "#         if basename == 'resnet50':\n",
    "#             self.basemodel      = basemodels.resnet50(pretrained=True)\n",
    "#             self.basemodel.layer1[2].bn2.register_forward_hook(self.get_activation('low_level_feature'))\n",
    "#             self.basemodel.layer4[2].bn2.register_forward_hook(self.get_activation('high_level_feature'))\n",
    "        \n",
    "    # output higher resolution features\n",
    "    def increase_resolution(self):  \n",
    "        global OUT_HEIGHT, OUT_WIDTH  \n",
    "        self.basemodel.layer3[0].conv1.stride = (1,1)\n",
    "        self.basemodel.layer3[0].downsample[0].stride=(1,1)  \n",
    "        self.basemodel.layer4[0].conv1.stride = (1,1)\n",
    "        self.basemodel.layer4[0].downsample[0].stride=(1,1)\n",
    "        OUT_HEIGHT *= 4\n",
    "        OUT_WIDTH  *= 4\n",
    "        print(\"using high resolution output ({}x{})\".format(OUT_HEIGHT,OUT_WIDTH))        \n",
    "\n",
    "    def get_activation(self, layer_name):\n",
    "        def hook(module, input, output):\n",
    "            self.output_feature[layer_name] = output\n",
    "        return hook\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _ = self.basemodel(x)\n",
    "        return self.output_feature['high_level_feature'], self.output_feature['low_level_feature']\n",
    "\n",
    "\n",
    "# modified WSL\n",
    "# multiple convolutional layers: extract more features, helps improve localization accuracy\n",
    "# attention mechanism: enhance important features, especially for complex or multi-target\n",
    "class WSL(nn.Module):\n",
    "    def __init__(self, num_class, depth=64):\n",
    "#     def __init__(self, num_class, depth=128): # 6.27 modify basemodel\n",
    "        super(WSL, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=512, out_channels=depth, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=depth, out_channels=depth, kernel_size=3, padding=1)\n",
    "        self.cam = nn.Conv2d(in_channels=depth, out_channels=num_class, kernel_size=1)\n",
    "        self.elu = nn.ELU()\n",
    "        self.bn1 = nn.BatchNorm2d(depth)\n",
    "        self.bn2 = nn.BatchNorm2d(depth)\n",
    "        self.gmp = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        \n",
    "        # attention mechanism\n",
    "        self.attention_conv = nn.Conv2d(in_channels=depth, out_channels=depth, kernel_size=1)\n",
    "        self.attention_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature = self.conv1(x)\n",
    "        feature = self.bn1(feature)\n",
    "        feature = self.elu(feature)\n",
    "        \n",
    "        feature = self.conv2(feature)\n",
    "        feature = self.bn2(feature)\n",
    "        feature = self.elu(feature)\n",
    "        \n",
    "        # add the attention mechanism\n",
    "        attn = self.attention_conv(feature)\n",
    "        attn = self.attention_sigmoid(attn)\n",
    "        feature = feature * attn\n",
    "        \n",
    "        cam = self.cam(feature)\n",
    "        logits = self.gmp(cam).squeeze(-1).squeeze(-1)\n",
    "        return cam, logits\n",
    "\n",
    "# 6.28 new: fix the error of channel number\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # flatten the input\n",
    "        query = self.query(query.view(batch_size, -1, self.embed_dim)).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key = self.key(key.view(batch_size, -1, self.embed_dim)).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = self.value(value.view(batch_size, -1, self.embed_dim)).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # scaled dot-product\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, value)\n",
    "\n",
    "        # concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "        out = self.out(context)\n",
    "\n",
    "        return out\n",
    "\n",
    "# add multi-head attention to the CAGAM \n",
    "class CAGAM(nn.Module):\n",
    "    def __init__(self, num_tool, num_verb, num_target, in_depth=512, embed_dim=128, num_heads=4):\n",
    "        super(CAGAM, self).__init__()\n",
    "        self.num_tool = num_tool\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # initialize convolutional layers\n",
    "        # for verb\n",
    "        self.verb_context = nn.Conv2d(in_channels=in_depth, out_channels=embed_dim, kernel_size=3, padding=1)\n",
    "        self.verb_query = nn.Conv2d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=1)\n",
    "        self.verb_tool_query = nn.Conv2d(in_channels=num_tool, out_channels=embed_dim, kernel_size=1)\n",
    "        self.verb_key = nn.Conv2d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=1)\n",
    "        self.verb_tool_key = nn.Conv2d(in_channels=num_tool, out_channels=embed_dim, kernel_size=1)\n",
    "        self.verb_cmap = nn.Conv2d(in_channels=embed_dim, out_channels=num_verb, kernel_size=1)\n",
    "        \n",
    "        # for target\n",
    "        self.target_context = nn.Conv2d(in_channels=in_depth, out_channels=embed_dim, kernel_size=3, padding=1)\n",
    "        self.target_query = nn.Conv2d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=1)\n",
    "        self.target_tool_query = nn.Conv2d(in_channels=num_tool, out_channels=embed_dim, kernel_size=1)\n",
    "        self.target_key = nn.Conv2d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=1)\n",
    "        self.target_tool_key = nn.Conv2d(in_channels=num_tool, out_channels=embed_dim, kernel_size=1)\n",
    "        self.target_cmap = nn.Conv2d(in_channels=embed_dim, out_channels=num_target, kernel_size=1)\n",
    "        \n",
    "        self.gmp = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.elu = nn.ELU()\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        self.flat = nn.Flatten(2, 3)\n",
    "        \n",
    "        # batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm2d(embed_dim)\n",
    "        self.bn2 = nn.BatchNorm2d(embed_dim)\n",
    "        self.bn3 = nn.BatchNorm2d(embed_dim)\n",
    "        self.bn4 = nn.BatchNorm2d(embed_dim)\n",
    "        self.bn5 = nn.BatchNorm2d(embed_dim)\n",
    "        self.bn6 = nn.BatchNorm2d(embed_dim)\n",
    "        self.bn7 = nn.BatchNorm2d(embed_dim)\n",
    "        self.bn8 = nn.BatchNorm2d(embed_dim)\n",
    "        self.bn9 = nn.BatchNorm2d(embed_dim)\n",
    "        self.bn10 = nn.BatchNorm2d(embed_dim)\n",
    "        self.bn11 = nn.BatchNorm2d(embed_dim)\n",
    "        self.bn12 = nn.BatchNorm2d(embed_dim)\n",
    "        \n",
    "        self.encoder_cagam_verb_beta = torch.nn.Parameter(torch.randn(1))\n",
    "        self.encoder_cagam_target_beta = torch.nn.Parameter(torch.randn(1))\n",
    "        \n",
    "        self.multihead_attention = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "\n",
    "    def get_verb(self, raw, cam):\n",
    "        x = self.elu(self.bn1(self.verb_context(raw)))\n",
    "        z = x.clone()\n",
    "        sh = list(z.shape)\n",
    "        sh[0] = -1\n",
    "        q1 = self.elu(self.bn2(self.verb_query(x)))\n",
    "        k1 = self.elu(self.bn3(self.verb_key(x)))\n",
    "        w1 = self.flat(k1).matmul(self.flat(q1).transpose(-1, -2))\n",
    "        q2 = self.elu(self.bn4(self.verb_tool_query(cam)))\n",
    "        k2 = self.elu(self.bn5(self.verb_tool_key(cam)))\n",
    "        w2 = self.flat(k2).matmul(self.flat(q2).transpose(-1, -2))\n",
    "        attention = (w1 * w2) / torch.sqrt(torch.tensor(sh[-1], dtype=torch.float32))\n",
    "        attention = self.soft(attention)\n",
    "        v = self.flat(z)\n",
    "        e = (attention.matmul(v) * self.encoder_cagam_verb_beta).reshape(sh)\n",
    "        e = self.bn6(e + z)\n",
    "        e = self.multihead_attention(e.view(-1, self.embed_dim), e.view(-1, self.embed_dim), e.view(-1, self.embed_dim))  # Apply multi-head attention\n",
    "        cmap = self.verb_cmap(e.view(sh))\n",
    "        y = self.gmp(cmap).squeeze(-1).squeeze(-1)\n",
    "        return cmap, y\n",
    "\n",
    "    def get_target(self, raw, cam):\n",
    "        x = self.elu(self.bn7(self.target_context(raw)))\n",
    "        z = x.clone()\n",
    "        sh = list(z.shape)\n",
    "        sh[0] = -1\n",
    "        q1 = self.elu(self.bn8(self.target_query(x)))\n",
    "        k1 = self.elu(self.bn9(self.target_key(x)))\n",
    "        w1 = self.flat(k1).transpose(-1, -2).matmul(self.flat(q1))\n",
    "        q2 = self.elu(self.bn10(self.target_tool_query(cam)))\n",
    "        k2 = self.elu(self.bn11(self.target_tool_key(cam)))\n",
    "        w2 = self.flat(k2).transpose(-1, -2).matmul(self.flat(q2))\n",
    "        attention = (w1 * w2) / torch.sqrt(torch.tensor(sh[-1], dtype=torch.float32))\n",
    "        attention = self.soft(attention)\n",
    "        v = self.flat(z)\n",
    "        e = (v.matmul(attention) * self.encoder_cagam_target_beta).reshape(sh)\n",
    "        e = self.bn12(e + z)\n",
    "        e = self.multihead_attention(e.view(-1, self.embed_dim), e.view(-1, self.embed_dim), e.view(-1, self.embed_dim))  # Apply multi-head attention\n",
    "        cmap = self.target_cmap(e.view(sh))\n",
    "        y = self.gmp(cmap).squeeze(-1).squeeze(-1)\n",
    "        return cmap, y\n",
    "\n",
    "    def forward(self, x, cam):\n",
    "        cam_v, logit_v = self.get_verb(x, cam)\n",
    "        cam_t, logit_t = self.get_target(x, cam)\n",
    "        return (cam_v, logit_v), (cam_t, logit_t)\n",
    "\n",
    "# reduce dimensionality of features\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=64, out_channels=256, stride=(2,2), kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=num_class, kernel_size=1)\n",
    "        self.elu   = nn.ELU()\n",
    "        self.bn1   = nn.BatchNorm2d(256)\n",
    "        self.bn2   = nn.BatchNorm2d(num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature = self.conv1(x)\n",
    "        feature = self.bn1(feature)\n",
    "        feature = self.elu(feature)\n",
    "        feature = self.conv2(feature)\n",
    "        feature = self.bn2(feature)\n",
    "        feature = self.elu(feature)\n",
    "        return feature\n",
    "\n",
    "# projection function\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self, num_tool=6, num_verb=10, num_target=15, num_triplet=100, out_depth=128):\n",
    "        super(Projection, self).__init__()\n",
    "        self.ivt_value = nn.Conv2d(in_channels=num_triplet, out_channels=out_depth, kernel_size=1)\n",
    "        self.i_value   = nn.Conv2d(in_channels=num_tool, out_channels=out_depth, kernel_size=1)        \n",
    "        self.v_value   = nn.Conv2d(in_channels=num_verb, out_channels=out_depth, kernel_size=1)\n",
    "        self.t_value   = nn.Conv2d(in_channels=num_target, out_channels=out_depth, kernel_size=1)       \n",
    "        self.ivt_query = nn.Linear(in_features=num_triplet, out_features=out_depth)\n",
    "        self.dropout   = nn.Dropout(p=0.3)\n",
    "        self.ivt_key   = nn.Linear(in_features=num_triplet, out_features=out_depth)\n",
    "        self.i_key     = nn.Linear(in_features=num_tool, out_features=out_depth)\n",
    "        self.v_key     = nn.Linear(in_features=num_verb, out_features=out_depth)\n",
    "        self.t_key     = nn.Linear(in_features=num_target, out_features=out_depth)\n",
    "        self.gap       = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.elu       = nn.ELU()          \n",
    "        self.bn1       = nn.BatchNorm1d(out_depth)\n",
    "        self.bn2       = nn.BatchNorm1d(out_depth)\n",
    "        self.bn3       = nn.BatchNorm2d(out_depth)\n",
    "        self.bn4       = nn.BatchNorm1d(out_depth)\n",
    "        self.bn5       = nn.BatchNorm2d(out_depth)\n",
    "        self.bn6       = nn.BatchNorm1d(out_depth)\n",
    "        self.bn7       = nn.BatchNorm2d(out_depth)\n",
    "        self.bn8       = nn.BatchNorm1d(out_depth)\n",
    "        self.bn9       = nn.BatchNorm2d(out_depth) \n",
    "        \n",
    "    def forward(self, cam_i, cam_v, cam_t, X):  \n",
    "        q = self.elu(self.bn1(self.ivt_query(self.dropout(self.gap(X).squeeze(-1).squeeze(-1)))))  \n",
    "        k = self.elu(self.bn2(self.ivt_key(self.gap(X).squeeze(-1).squeeze(-1))) )\n",
    "        v = self.bn3(self.ivt_value(X)) \n",
    "        k1 = self.elu(self.bn4(self.i_key(self.gap(cam_i).squeeze(-1).squeeze(-1))) )\n",
    "        v1 = self.elu(self.bn5(self.i_value(cam_i)) )\n",
    "        k2 = self.elu(self.bn6(self.v_key(self.gap(cam_v).squeeze(-1).squeeze(-1))))\n",
    "        v2 = self.elu(self.bn7(self.v_value(cam_v)) )\n",
    "        k3 = self.elu(self.bn8(self.t_key(self.gap(cam_t).squeeze(-1).squeeze(-1))))\n",
    "        v3 = self.elu(self.bn9(self.t_value(cam_t)))\n",
    "        sh = list(v1.shape)\n",
    "        v  = self.elu(F.interpolate(v, (sh[2],sh[3])))\n",
    "        X  = self.elu(F.interpolate(X, (sh[2],sh[3])))\n",
    "        return (X, (k1,v1), (k2,v2), (k3,v3), (q,k,v))\n",
    "\n",
    "# Original multi-head of self and cross attention\n",
    "# class MHMA(nn.Module):\n",
    "#     def __init__(self, depth, num_class=100, num_heads=4, use_ln=False):\n",
    "#         super(MHMA, self).__init__()        \n",
    "#         self.concat = nn.Conv2d(in_channels=depth*num_heads, out_channels=num_class, kernel_size=3, padding=1)\n",
    "#         self.bn     = nn.BatchNorm2d(num_class)\n",
    "#         self.ln     = nn.LayerNorm([num_class, OUT_HEIGHT, OUT_WIDTH]) if use_ln else nn.BatchNorm2d(num_class)\n",
    "#         self.elu    = nn.ELU()    \n",
    "#         self.soft   = nn.Softmax(dim=1) \n",
    "#         self.heads  = num_heads\n",
    "        \n",
    "#     def scale_dot_product(self, key, value, query):\n",
    "#         dk        = torch.sqrt(torch.tensor(list(key.shape)[-2], dtype=torch.float32))\n",
    "#         affinity  = key.matmul(query.transpose(-1,-2))                        \n",
    "#         attn_w    = affinity / dk              \n",
    "#         attn_w    = self.soft(attn_w)\n",
    "#         attention = attn_w.matmul(value) \n",
    "#         return attention\n",
    "    \n",
    "#     def forward(self, inputs):\n",
    "#         (X, (k1,v1), (k2,v2), (k3,v3), (q,k,v)) = inputs     \n",
    "#         query = torch.stack([q]*self.heads, dim=1) # [B,Head,D]\n",
    "#         query = query.unsqueeze(dim=-1) # [B,Head,D,1]        \n",
    "#         key   = torch.stack([k,k1,k2,k3], dim=1) # [B,Head,D]\n",
    "#         key   = key.unsqueeze(dim=-1) # [B,Head,D,1]        \n",
    "#         value = torch.stack([v,v1,v2,v3], dim=1) # [B,Head,D,H,W]\n",
    "#         dims  = list(value.shape) # [B,Head,D,H,W]\n",
    "#         value = value.reshape([-1,dims[1],dims[2],dims[3]*dims[4]])# [B,Head,D,HW]          \n",
    "#         attn  = self.scale_dot_product(key, value, query)  # [B,Head,D,HW]\n",
    "#         attn  = attn.reshape([-1,dims[1]*dims[2],dims[3],dims[4]]) # [B,DHead,H,W]\n",
    "#         mha   = self.elu(self.bn(self.concat(attn)))\n",
    "#         mha   = self.ln(mha + X.clone())  \n",
    "#         return mha\n",
    "\n",
    "# MHMA with LSTM\n",
    "class MHMA(nn.Module):\n",
    "    def __init__(self, depth, num_class=100, num_heads=4, use_ln=True, lstm_hidden_size=128, lstm_layers=1):\n",
    "        super(MHMA, self).__init__()\n",
    "        self.concat = nn.Conv2d(in_channels=depth*num_heads, out_channels=num_class, kernel_size=3, padding=1)\n",
    "        self.bn     = nn.BatchNorm2d(num_class)\n",
    "        self.ln     = nn.LayerNorm([num_class, OUT_HEIGHT, OUT_WIDTH]) if use_ln else nn.BatchNorm2d(num_class)\n",
    "        self.elu    = nn.ELU()\n",
    "        self.soft   = nn.Softmax(dim=1)\n",
    "        self.heads  = num_heads\n",
    "        self.dropout = nn.Dropout(0.1)  # add Dropout\n",
    "\n",
    "        # define LSTM\n",
    "        self.lstm = nn.LSTM(input_size=num_class * OUT_HEIGHT * OUT_WIDTH,\n",
    "                            hidden_size=lstm_hidden_size,\n",
    "                            num_layers=lstm_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # LSTM Hidden Size for convolutional output\n",
    "        self.fc = nn.Linear(lstm_hidden_size, num_class * OUT_HEIGHT * OUT_WIDTH)\n",
    "\n",
    "    def scale_dot_product(self, key, value, query):\n",
    "        dk        = torch.sqrt(torch.tensor(list(key.shape)[-2], dtype=torch.float32))\n",
    "        affinity  = key.matmul(query.transpose(-1,-2))\n",
    "        attn_w    = affinity / dk\n",
    "        attn_w    = self.soft(attn_w)\n",
    "        attention = attn_w.matmul(value)\n",
    "        attention = self.dropout(attention)  # add Dropout\n",
    "        return attention\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        (X, (k1,v1), (k2,v2), (k3,v3), (q,k,v)) = inputs\n",
    "        query = torch.stack([q]*self.heads, dim=1)\n",
    "        query = query.unsqueeze(dim=-1)\n",
    "        key   = torch.stack([k,k1,k2,k3], dim=1) \n",
    "        key   = key.unsqueeze(dim=-1)\n",
    "        value = torch.stack([v,v1,v2,v3], dim=1)\n",
    "        dims  = list(value.shape)\n",
    "        value = value.reshape([-1,dims[1],dims[2],dims[3]*dims[4]])\n",
    "        attn  = self.scale_dot_product(key, value, query)\n",
    "        attn  = attn.reshape([-1,dims[1]*dims[2],dims[3],dims[4]])\n",
    "        mha   = self.elu(self.bn(self.concat(attn)))\n",
    "        mha   = self.ln(mha + X.clone())\n",
    "\n",
    "        # flat the output of the multi-head attention as the input of the LSTM\n",
    "        batch_size = mha.size(0)\n",
    "        mha_flatten = mha.view(batch_size, -1)  # [B, num_class * H * W]\n",
    "        mha_flatten = mha_flatten.unsqueeze(1)  # [B, 1, num_class * H * W]\n",
    "\n",
    "        # go through LSTM\n",
    "        lstm_out, _ = self.lstm(mha_flatten)  # [B, 1, lstm_hidden_size]\n",
    "        lstm_out = lstm_out.squeeze(1)  # [B, lstm_hidden_size]\n",
    "\n",
    "        # fcl to resize the output\n",
    "        lstm_out = self.fc(lstm_out)  # [B, num_class * H * W]\n",
    "        lstm_out = lstm_out.view(batch_size, -1, OUT_HEIGHT, OUT_WIDTH)  # [B, num_class, H, W]\n",
    "        \n",
    "        return lstm_out\n",
    "    \n",
    "# feed-forward layer\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, k, num_class=100, use_ln=False):\n",
    "        super(FFN, self).__init__()\n",
    "        def Ignore(x): return x\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_class, out_channels=num_class, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=num_class, out_channels=num_class, kernel_size=1) \n",
    "        self.elu1  = nn.ELU() \n",
    "        self.elu2  = nn.ELU() if k>0 else Ignore     \n",
    "        self.bn1   = nn.BatchNorm2d(num_class)    \n",
    "        self.bn2   = nn.BatchNorm2d(num_class)\n",
    "        self.ln    = nn.LayerNorm([num_class, OUT_HEIGHT, OUT_WIDTH]) if use_ln else nn.BatchNorm2d(num_class)\n",
    "        \n",
    "    def forward(self, inputs,):\n",
    "        x  = self.elu1(self.bn1(self.conv1(inputs)))\n",
    "        x  = self.elu2(self.bn2(self.conv2(x)))\n",
    "        x  = self.ln(x + inputs.clone())\n",
    "        return x\n",
    "\n",
    "# classification layer\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, layer_size, num_class=100):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.gmp = nn.AdaptiveMaxPool2d((1,1)) \n",
    "        self.mlp = nn.Linear(in_features=num_class, out_features=num_class)     \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.gmp(inputs).squeeze(-1).squeeze(-1)\n",
    "        y = self.mlp(x)\n",
    "        return y\n",
    "    \n",
    "# DC\n",
    "# add dynamic convolution module\n",
    "# combines multiple kernels in an adaptive manner using learned weights\n",
    "class DynamicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, num_kernels=4):\n",
    "        super(DynamicConv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.num_kernels = num_kernels\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(num_kernels, out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(num_kernels, out_channels))\n",
    "\n",
    "        self.kernel_weights = nn.Parameter(torch.randn(num_kernels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_outs = []\n",
    "        for i in range(self.num_kernels):\n",
    "            conv_out = F.conv2d(x, self.weight[i], self.bias[i], stride=self.stride, padding=self.padding)\n",
    "            conv_outs.append(conv_out.unsqueeze(0))\n",
    "\n",
    "        conv_outs = torch.cat(conv_outs, dim=0)\n",
    "\n",
    "        kernel_weights = F.softmax(self.kernel_weights, dim=0)\n",
    "\n",
    "        out = 0\n",
    "        for i in range(self.num_kernels):\n",
    "            out += kernel_weights[i] * conv_outs[i]\n",
    "\n",
    "        return out\n",
    "\n",
    "# extend the dynamic convolution by adding batch normalization and ELU\n",
    "class DynamicConvolution(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, num_kernels=4):\n",
    "        super(DynamicConvolution, self).__init__()\n",
    "        self.dynamic_conv = DynamicConv2d(in_channels, out_channels, kernel_size, stride, padding, num_kernels=num_kernels)\n",
    "        self.elu = nn.ELU()\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dynamic_conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.elu(x)\n",
    "        return x\n",
    "\n",
    "# add DC to the decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer_size, d_model, num_heads, num_class=100, use_ln=False):\n",
    "        super(Decoder, self).__init__()        \n",
    "        self.projection = nn.ModuleList([Projection(num_triplet=num_class, out_depth=d_model) for i in range(layer_size)])\n",
    "        self.mhma = nn.ModuleList([MHMA(num_class=num_class, depth=d_model, num_heads=num_heads, use_ln=use_ln) for i in range(layer_size)])\n",
    "        self.dynamic_conv = nn.ModuleList([DynamicConvolution(num_class, num_class) for i in range(layer_size)])\n",
    "        self.ffnet = nn.ModuleList([FFN(k=layer_size-i-1, num_class=num_class, use_ln=use_ln) for i in range(layer_size)])\n",
    "        self.classifier = Classifier(num_class)\n",
    "        \n",
    "    def forward(self, enc_i, enc_v, enc_t, enc_ivt):\n",
    "        X = enc_ivt.clone()\n",
    "        for P, M, D, F in zip(self.projection, self.mhma, self.dynamic_conv, self.ffnet):\n",
    "            X = P(enc_i[0], enc_v[0], enc_t[0], X)\n",
    "            X = M(X)\n",
    "            X = D(X)\n",
    "            X = F(X)\n",
    "        logits = self.classifier(X)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2717b",
   "metadata": {
    "papermill": {
     "duration": 0.011643,
     "end_time": "2024-08-09T16:45:08.177326",
     "exception": false,
     "start_time": "2024-08-09T16:45:08.165683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set up some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e437bd5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:45:08.203918Z",
     "iopub.status.busy": "2024-08-09T16:45:08.203512Z",
     "iopub.status.idle": "2024-08-09T16:45:08.267273Z",
     "shell.execute_reply": "2024-08-09T16:45:08.266017Z"
    },
    "papermill": {
     "duration": 0.08008,
     "end_time": "2024-08-09T16:45:08.269621",
     "exception": false,
     "start_time": "2024-08-09T16:45:08.189541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring network ...\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser() # set up a command-line argument parser, also for Kaggle\n",
    "# for the model\n",
    "parser.add_argument('--model', type=str, default='rendezvous', choices=['rendezvous'], help='Model name?')\n",
    "parser.add_argument('--version', type=int, default=0,  help='Model version control (for keeping several versions)') \n",
    "parser.add_argument('--hr_output', action='store_true', help='Whether to use higher resolution output (32x56) or not (8x14). Default: False')\n",
    "parser.add_argument('--use_ln', action='store_true', help='Whether to use layer norm or batch norm in AddNorm() function. Default: False')\n",
    "parser.add_argument('--decoder_layer', type=int, default=8, help='Number of MHMA layers ') \n",
    "# for tasks\n",
    "parser.add_argument('-t', '--train', action='store_true', help='to train.')\n",
    "parser.add_argument('-e', '--test',  action='store_true', help='to test')\n",
    "parser.add_argument('--val_interval', type=int, default=1,  help='(for hp tuning). Epoch interval to evaluate on validation data. set -1 for only after final epoch, or a number higher than the total epochs to not validate.')\n",
    "# for data\n",
    "parser.add_argument('--data_dir', type=str, default='/kaggle/input/cholect45/CholecT45', help='path to dataset?')\n",
    "parser.add_argument('--dataset_variant', type=str, default='cholect45-crossval', choices=['cholect50', 'cholect45', 'cholect50-challenge', 'cholect50-crossval', 'cholect45-crossval'], help='Variant of the dataset to use')\n",
    "parser.add_argument('-k', '--kfold', type=int, default=1,  choices=[1,2,3,4,5,], help='The test split in k-fold cross-validation')\n",
    "parser.add_argument('--image_width', type=int, default=448, help='Image width ')  \n",
    "parser.add_argument('--image_height', type=int, default=256, help='Image height ')  \n",
    "parser.add_argument('--image_channel', type=int, default=3, help='Image channels ')  \n",
    "parser.add_argument('--num_tool_classes', type=int, default=6, help='Number of tool categories')\n",
    "parser.add_argument('--num_verb_classes', type=int, default=10, help='Number of verb categories')\n",
    "parser.add_argument('--num_target_classes', type=int, default=15, help='Number of target categories')\n",
    "parser.add_argument('--num_triplet_classes', type=int, default=100, help='Number of triplet categories')\n",
    "parser.add_argument('--augmentation_list', type=str, nargs='*', default=['original', 'vflip', 'hflip', 'contrast', 'rot90'], help='List augumentation styles (see dataloader.py for list of supported styles).')\n",
    "# hp\n",
    "parser.add_argument('-b', '--batch', type=int, default=32,  help='The size of sample training batch')\n",
    "# parser.add_argument('--epochs', type=int, default=100,  help='How many training epochs?')\n",
    "parser.add_argument('--epochs', type=int, default=1,  help='How many training epochs?') # test for 1 epoch\n",
    "parser.add_argument('-w', '--warmups', type=int, nargs='+', default=[9,18,58], help='List warmup epochs for tool, verb-target, triplet respectively')\n",
    "parser.add_argument('-l', '--initial_learning_rates', type=float, nargs='+', default=[0.01, 0.01, 0.01], help='List learning rates for tool, verb-target, triplet respectively')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-5,  help='L2 regularization weight decay constant')\n",
    "parser.add_argument('--decay_steps', type=int, default=10,  help='Step to exponentially decay')\n",
    "parser.add_argument('--decay_rate', type=float, default=0.99,  help='Learning rates weight decay rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.95,  help=\"Optimizer's momentum\")\n",
    "parser.add_argument('--power', type=float, default=0.1,  help='Learning rates weight decay power')\n",
    "# weights\n",
    "parser.add_argument('--pretrain_dir', type=str, default='', help='path to pretrain_weight?')\n",
    "parser.add_argument('--test_ckpt', type=str, default=None, help='path to model weight for testing')\n",
    "# device (kaggle only has one gpu)\n",
    "parser.add_argument('--gpu', type=str, default=\"0\",  help='The gpu device to use. To use multiple gpu put all the device ids comma-separated, e.g: \"0,1,2\" ')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# set up global variables\n",
    "is_train        = FLAGS.train\n",
    "is_test         = FLAGS.test\n",
    "dataset_variant = FLAGS.dataset_variant\n",
    "data_dir        = FLAGS.data_dir\n",
    "kfold           = FLAGS.kfold if \"crossval\" in dataset_variant else 0\n",
    "version         = FLAGS.version\n",
    "hr_output       = FLAGS.hr_output\n",
    "use_ln          = FLAGS.use_ln\n",
    "batch_size      = FLAGS.batch\n",
    "pretrain_dir    = FLAGS.pretrain_dir\n",
    "test_ckpt       = FLAGS.test_ckpt\n",
    "weight_decay    = FLAGS.weight_decay\n",
    "learning_rates  = FLAGS.initial_learning_rates\n",
    "warmups         = FLAGS.warmups\n",
    "decay_steps     = FLAGS.decay_steps\n",
    "decay_rate      = FLAGS.decay_rate\n",
    "power           = FLAGS.power\n",
    "momentum        = FLAGS.momentum\n",
    "epochs          = FLAGS.epochs\n",
    "gpu             = FLAGS.gpu\n",
    "image_height    = FLAGS.image_height\n",
    "image_width     = FLAGS.image_width\n",
    "image_channel   = FLAGS.image_channel\n",
    "num_triplet     = FLAGS.num_triplet_classes\n",
    "num_tool        = FLAGS.num_tool_classes\n",
    "num_verb        = FLAGS.num_verb_classes\n",
    "num_target      = FLAGS.num_target_classes\n",
    "val_interval    = FLAGS.epochs-1 if FLAGS.val_interval==-1 else FLAGS.val_interval\n",
    "set_chlg_eval   = True if \"challenge\" in dataset_variant else False # To observe challenge evaluation protocol\n",
    "gpu             = \",\".join(str(FLAGS.gpu).split(\",\"))\n",
    "decodelayer     = FLAGS.decoder_layer\n",
    "addnorm         = \"layer\" if use_ln else \"batch\"\n",
    "modelsize       = \"high\" if hr_output else \"low\"\n",
    "FLAGS.multigpu  = len(gpu) > 1  # not yet implemented!\n",
    "mheaders        = [\"\",\"l\", \"cholect\", \"k\"]\n",
    "margs           = [FLAGS.model, decodelayer, dataset_variant, kfold]\n",
    "wheaders        = [\"norm\", \"res\"]\n",
    "wargs           = [addnorm, modelsize]\n",
    "modelname       = \"_\".join([\"{}{}\".format(x,y) for x,y in zip(mheaders, margs) if len(str(y))])+\"_\"+\\\n",
    "                  \"_\".join([\"{}{}\".format(x,y) for x,y in zip(wargs, wheaders) if len(str(x))])\n",
    "model_dir       = \"./__checkpoint__/run_{}\".format(version)\n",
    "if not os.path.exists(model_dir): os.makedirs(model_dir)\n",
    "resume_ckpt     = None\n",
    "# ckpt_path       = os.path.join(model_dir, '{}.pth'.format(modelname))\n",
    "ckpt_path = os.path.join('/kaggle/working', '{}.pth'.format(modelname))\n",
    "logfile         = os.path.join(model_dir, '{}.log'.format(modelname))\n",
    "data_augmentations      = FLAGS.augmentation_list \n",
    "iterable_augmentations  = []\n",
    "print(\"Configuring network ...\")\n",
    "\n",
    "# helpers\n",
    "def assign_gpu(gpu=None):  \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu) \n",
    "    os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1' \n",
    "    \n",
    "\n",
    "def get_weight_balancing(case='cholect50'):\n",
    "    # 50:   cholecT50, data splits as used in rendezvous paper\n",
    "    # 50ch: cholecT50, data splits as used in CholecTriplet challenge\n",
    "    # 45cv: cholecT45, official data splits (cross-val)\n",
    "    # 50cv: cholecT50, official data splits (cross-val)\n",
    "    switcher = {\n",
    "        'cholect50': {\n",
    "            'tool'  :   [0.08084519, 0.81435289, 0.10459284, 2.55976864, 1.630372490, 1.29528455],\n",
    "            'verb'  :   [0.31956735, 0.07252306, 0.08111481, 0.81137309, 1.302895320, 2.12264151, 1.54109589, 8.86363636, 12.13692946, 0.40462028],\n",
    "            'target':   [0.06246232, 1.00000000, 0.34266478, 0.84750219, 14.80102041, 8.73795181, 1.52845100, 5.74455446, 0.285756500, 12.72368421, 0.6250808,  3.85771277, 6.95683453, 0.84923888, 0.40130032]\n",
    "        },\n",
    "        'cholect50-challenge': {\n",
    "            'tool':     [0.08495163, 0.88782288, 0.11259564, 2.61948830, 1.784866470, 1.144624170],\n",
    "            'verb':     [0.39862805, 0.06981640, 0.08332925, 0.81876204, 1.415868390, 2.269359150, 1.28428410, 7.35822511, 18.67857143, 0.45704490],\n",
    "            'target':   [0.07333818, 0.87139287, 0.42853950, 1.00000000, 17.67281106, 13.94545455, 1.44880997, 6.04889590, 0.326188650, 16.82017544, 0.63577586, 6.79964539, 6.19547658, 0.96284208, 0.51559559]\n",
    "        },\n",
    "        'cholect45-crossval': {\n",
    "            1: {\n",
    "                'tool':     [0.08165644, 0.91226868, 0.10674758, 2.85418156, 1.60554885, 1.10640067],\n",
    "                'verb':     [0.37870137, 0.06836869, 0.07931255, 0.84780024, 1.21880342, 2.52836879, 1.30765704, 6.88888889, 17.07784431, 0.45241117],\n",
    "                'target':   [0.07149629, 1.0, 0.41013597, 0.90458015, 13.06299213, 12.06545455, 1.5213205, 5.04255319, 0.35808332, 45.45205479, 0.67493897, 7.04458599, 9.14049587, 0.97330595, 0.52633249]\n",
    "                },\n",
    "            2: {\n",
    "                'tool':     [0.0854156, 0.89535362, 0.10995253, 2.74936869, 1.78264429, 1.13234529],\n",
    "                'verb':     [0.36346863, 0.06771776, 0.07893261, 0.82842725, 1.33892161, 2.13049748, 1.26120359, 5.72674419, 19.7, 0.43189126],\n",
    "                'target':   [0.07530655, 0.97961957, 0.4325135, 0.99393438, 15.5387931, 14.5951417, 1.53862569, 6.01836394, 0.35184462, 15.81140351, 0.709506, 5.79581994, 8.08295964, 1.0, 0.52689272]\n",
    "            },\n",
    "            3: {\n",
    "                \"tool\" :   [0.0915228, 0.89714969, 0.12057004, 2.72128174, 1.94092281, 1.12948557],\n",
    "                \"verb\" :   [0.43636862, 0.07558554, 0.0891017, 0.81820519, 1.53645582, 2.31924198, 1.28565657, 6.49387755, 18.28735632, 0.48676763],\n",
    "                \"target\" : [0.06841828, 0.90980736, 0.38826607, 1.0, 14.3640553, 12.9875, 1.25939394, 5.38341969, 0.29060227, 13.67105263, 0.59168565, 6.58985201, 5.72977941, 0.86824513, 0.47682423]\n",
    "\n",
    "            },\n",
    "            4: {\n",
    "                'tool':     [0.08222218, 0.85414117, 0.10948695, 2.50868784, 1.63235867, 1.20593318],\n",
    "                'verb':     [0.41154261, 0.0692142, 0.08427214, 0.79895288, 1.33625219, 2.2624166, 1.35343681, 7.63, 17.84795322, 0.43970609],\n",
    "                'target':   [0.07536126, 0.85398445, 0.4085784, 0.95464422, 15.90497738, 18.5978836, 1.55875831, 5.52672956, 0.33700863, 15.41666667, 0.74755423, 5.4921875, 6.11304348, 1.0, 0.50641118],\n",
    "            },\n",
    "            5: {\n",
    "                'tool':     [0.0804654, 0.92271157, 0.10489631, 2.52302243, 1.60074906, 1.09141982],\n",
    "                'verb':     [0.50710436, 0.06590258, 0.07981184, 0.81538866, 1.29267277, 2.20525568, 1.29699248, 7.32311321, 25.45081967, 0.46733895],\n",
    "                'target':   [0.07119395, 0.87450495, 0.43043372, 0.86465981, 14.01984127, 23.7114094, 1.47577277, 5.81085526, 0.32129865, 22.79354839, 0.63304067, 6.92745098, 5.88833333, 1.0, 0.53175798]\n",
    "            }\n",
    "        },\n",
    "        'cholect50-crossval': {\n",
    "            1:{\n",
    "                'tool':     [0.0828851, 0.8876, 0.10830995, 2.93907285, 1.63884786, 1.14499484],\n",
    "                'verb':     [0.29628942, 0.07366916, 0.08267971, 0.83155428, 1.25402434, 2.38358209, 1.34938741, 7.56872038, 12.98373984, 0.41502079],\n",
    "                'target':   [0.06551745, 1.0, 0.36345711, 0.82434783, 13.06299213, 8.61818182, 1.4017744, 4.62116992, 0.32822238, 45.45205479, 0.67343211, 4.13200498, 8.23325062, 0.88527215, 0.43113306],\n",
    "\n",
    "            },\n",
    "            2:{\n",
    "                'tool':     [0.08586283, 0.87716737, 0.11068887, 2.84210526, 1.81016949, 1.16283571],\n",
    "                'verb':     [0.30072757, 0.07275414, 0.08350168, 0.80694143, 1.39209979, 2.22754491, 1.31448763, 6.38931298, 13.89211618, 0.39397505],\n",
    "                'target':   [0.07056703, 1.0, 0.39451115, 0.91977006, 15.86206897, 9.68421053, 1.44483706, 5.44378698, 0.31858714, 16.14035088, 0.7238395, 4.20571429, 7.98264642, 0.91360477, 0.43304307],\n",
    "            },\n",
    "            3:{\n",
    "            'tool':      [0.09225068, 0.87856006, 0.12195811, 2.82669323, 1.97710987, 1.1603972],\n",
    "                'verb':     [0.34285159, 0.08049804, 0.0928239, 0.80685714, 1.56125608, 2.23984772, 1.31471136, 7.08835341, 12.17241379, 0.43180428],\n",
    "                'target':   [0.06919395, 1.0, 0.37532866, 0.9830703, 15.78801843, 8.99212598, 1.27597765, 5.36990596, 0.29177312, 15.02631579, 0.64935557, 5.08308605, 5.86643836, 0.86580743, 0.41908257], \n",
    "            },\n",
    "            4:{\n",
    "                'tool':     [0.08247885, 0.83095539, 0.11050268, 2.58193042, 1.64497676, 1.25538881],\n",
    "                'verb':     [0.31890981, 0.07380354, 0.08804592, 0.79094077, 1.35928144, 2.17017208, 1.42947103, 8.34558824, 13.19767442, 0.40666428],\n",
    "                'target':   [0.07777646, 0.95894072, 0.41993829, 0.95592153, 17.85972851, 12.49050633, 1.65701092, 5.74526929, 0.33763901, 17.31140351, 0.83747083, 3.95490982, 6.57833333, 1.0, 0.47139615],\n",
    "            },\n",
    "            5:{\n",
    "                'tool':     [0.07891691, 0.89878025, 0.10267677, 2.53805556, 1.60636428, 1.12691169],\n",
    "                'verb':     [0.36420961, 0.06825313, 0.08060635, 0.80956984, 1.30757221, 2.09375, 1.33625848, 7.9009434, 14.1350211, 0.41429631],\n",
    "                'target':   [0.07300329, 0.97128713, 0.42084942, 0.8829883, 15.57142857, 19.42574257, 1.56521739, 5.86547085, 0.32732733, 25.31612903, 0.70171674, 4.55220418, 6.13125, 1.0, 0.48528321],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return switcher.get(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f72660a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:45:08.296587Z",
     "iopub.status.busy": "2024-08-09T16:45:08.296174Z",
     "iopub.status.idle": "2024-08-09T16:45:08.339844Z",
     "shell.execute_reply": "2024-08-09T16:45:08.338860Z"
    },
    "papermill": {
     "duration": 0.059983,
     "end_time": "2024-08-09T16:45:08.342167",
     "exception": false,
     "start_time": "2024-08-09T16:45:08.282184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.profiler.emit_nvtx at 0x7fdd693151b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_loop(dataloader, model, activation, loss_fn_i, loss_fn_v, loss_fn_t, loss_fn_ivt, optimizers, scheduler, epoch):\n",
    "    start = time.time() \n",
    "    total_loss_i = 0.0\n",
    "    total_loss_v = 0.0\n",
    "    total_loss_t = 0.0\n",
    "    total_loss_ivt = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch, (img, (y1, y2, y3, y4)) in enumerate(dataloader):\n",
    "        img, y1, y2, y3, y4 = img.cuda(), y1.cuda(), y2.cuda(), y3.cuda(), y4.cuda()        \n",
    "        model.train()        \n",
    "        tool, verb, target, triplet = model(img)\n",
    "        cam_i, logit_i  = tool\n",
    "        cam_v, logit_v  = verb\n",
    "        cam_t, logit_t  = target\n",
    "        logit_ivt       = triplet                \n",
    "        loss_i          = loss_fn_i(logit_i, y1.float())\n",
    "        loss_v          = loss_fn_v(logit_v, y2.float())\n",
    "        loss_t          = loss_fn_t(logit_t, y3.float())\n",
    "        loss_ivt        = loss_fn_ivt(logit_ivt, y4.float())  \n",
    "        loss            = (loss_i) + (loss_v) + (loss_t) + loss_ivt \n",
    "        \n",
    "        # total loss\n",
    "        batch_size = img.size(0)\n",
    "        total_loss_i += loss_i.item() * batch_size\n",
    "        total_loss_v += loss_v.item() * batch_size\n",
    "        total_loss_t += loss_t.item() * batch_size\n",
    "        total_loss_ivt += loss_ivt.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        # back-propagation \n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        loss.backward()\n",
    "        for opt in optimizers:\n",
    "            opt.step()\n",
    "\n",
    "        print(f'Batch {batch+1}/{len(dataloader)} | Losses => i: [{loss_i.item():.4f}] v: [{loss_v.item():.4f}] t: [{loss_t.item():.4f}] ivt: [{loss_ivt.item():.4f}]')\n",
    "\n",
    "    # update learning rate\n",
    "    for sch in scheduler:\n",
    "        sch.step()\n",
    "    \n",
    "    # average losses for each epoch\n",
    "    avg_loss_i = total_loss_i / total_samples\n",
    "    avg_loss_v = total_loss_v / total_samples\n",
    "    avg_loss_t = total_loss_t / total_samples\n",
    "    avg_loss_ivt = total_loss_ivt / total_samples\n",
    "\n",
    "    print(f'Epoch {epoch+1} completed | Average Losses--- i: [{avg_loss_i:.4f}] v: [{avg_loss_v:.4f}] t: [{avg_loss_t:.4f}] ivt: [{avg_loss_ivt:.4f}] | eta: {(time.time() - start):.2f} secs')\n",
    "\n",
    "    # average precision for each epoch\n",
    "    with torch.no_grad():\n",
    "        mAP.reset_global()\n",
    "        for img, (y1, y2, y3, y4) in dataloader:\n",
    "            img, y1, y2, y3, y4 = img.cuda(), y1.cuda(), y2.cuda(), y3.cuda(), y4.cuda()\n",
    "            tool, verb, target, triplet = model(img)\n",
    "            logit_ivt = triplet\n",
    "            mAP.update(y4.float().detach().cpu(), activation(logit_ivt).detach().cpu())\n",
    "        avg_precision = mAP.compute_video_AP()['mAP']\n",
    "        print(f'Average Precision for Epoch {epoch+1}: {avg_precision:.4f}')\n",
    "\n",
    "\n",
    "def calculate_top_n_accuracy(logits, targets, n):\n",
    "    # print log information\n",
    "#     print(f\"Logits shape: {logits.shape}\")\n",
    "#     print(f\"Targets shape: {targets.shape}\")\n",
    "\n",
    "    _, top_n_pred = logits.topk(n, dim=1)\n",
    "#     print(f\"Top-{n} predictions shape: {top_n_pred.shape}\")\n",
    "\n",
    "    # convert targets from one-hot encoding to class indices. \n",
    "    targets = torch.argmax(targets, dim=1)\n",
    "#     print(f\"Converted targets shape: {targets.shape}\")\n",
    "\n",
    "    if top_n_pred.size(0) != targets.size(0):\n",
    "        raise ValueError(\"The batch size does not match between the Logits and the target\")\n",
    "\n",
    "    # calculate top-n accuracy\n",
    "    correct_top_n = top_n_pred.eq(targets.view(-1, 1).expand_as(top_n_pred))\n",
    "    top_n_accuracy = correct_top_n.any(dim=1).float().mean().item()\n",
    "    return top_n_accuracy\n",
    "\n",
    "def test_loop(dataloader, model, activation, final_eval=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)   \n",
    "    mAP.reset()  \n",
    "    \n",
    "    top5_accuracy = 0.0\n",
    "    top10_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    if final_eval and not set_chlg_eval:\n",
    "        mAPv.reset() \n",
    "        mAPt.reset() \n",
    "        mAPi.reset() \n",
    "    with torch.no_grad():\n",
    "        for batch, (img, (y1, y2, y3, y4)) in enumerate(dataloader):\n",
    "            img, y1, y2, y3, y4 = img.cuda(), y1.cuda(), y2.cuda(), y3.cuda(), y4.cuda()            \n",
    "            model.eval()  \n",
    "            tool, verb, target, triplet = model(img)\n",
    "            \n",
    "            # calculate Top-5 and Top-10 accuracies for triplet predictions\n",
    "            triplet_logits = activation(triplet)\n",
    "            top5_accuracy += calculate_top_n_accuracy(triplet_logits, y4, 5) * img.size(0)\n",
    "            top10_accuracy += calculate_top_n_accuracy(triplet_logits, y4, 10) * img.size(0)\n",
    "            total_samples += img.size(0)\n",
    "            \n",
    "            if final_eval and not set_chlg_eval:\n",
    "                cam_i, logit_i = tool\n",
    "                cam_v, logit_v = verb\n",
    "                cam_t, logit_t = target\n",
    "                mAPi.update(y1.float().detach().cpu(), activation(logit_i).detach().cpu())\n",
    "                mAPv.update(y2.float().detach().cpu(), activation(logit_v).detach().cpu())\n",
    "                mAPt.update(y3.float().detach().cpu(), activation(logit_t).detach().cpu())\n",
    "            mAP.update(y4.float().detach().cpu(), activation(triplet).detach().cpu())\n",
    "    mAP.video_end() \n",
    "    if final_eval and not set_chlg_eval:\n",
    "        mAPv.video_end()\n",
    "        mAPt.video_end()\n",
    "        mAPi.video_end()\n",
    "        \n",
    "    # Top-5 and Top-10 accuracies\n",
    "    top5_accuracy /= total_samples\n",
    "    top10_accuracy /= total_samples\n",
    "#     print(f\"Top-5 Accuracy: {top5_accuracy:.4f}\")\n",
    "#     print(f\"Top-10 Accuracy: {top10_accuracy:.4f}\")\n",
    "    return top5_accuracy, top10_accuracy, total_samples\n",
    "\n",
    "\n",
    "def weight_mgt(score, epoch):\n",
    "    # hyperparameter selection based on validation set\n",
    "    global benchmark\n",
    "    if score > benchmark.item():\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        benchmark = score\n",
    "#         print(f'>>> Saving checkpoint for epoch {epoch+1} at {ckpt_path}, time {time.ctime()} ', file=open(logfile, 'a+'))  \n",
    "        return \"increased\"\n",
    "    else:\n",
    "        return \"decreased\"\n",
    "\n",
    "# during testing or resuming\n",
    "if os.path.exists(ckpt_path):\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    print(f'Model loaded from checkpoint at {ckpt_path}')\n",
    "\n",
    "##%% assign device and set debugger options\n",
    "assign_gpu(gpu=gpu)\n",
    "np.seterr(divide='ignore', invalid='ignore') # ignore divide-by-zero and invalid operations.\n",
    "torch.autograd.set_detect_anomaly(False) # disable anomaly detection\n",
    "\n",
    "# disable profiling and NVTX emission for debugging\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)\n",
    "# torch.set_deterministic(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6661b83",
   "metadata": {
    "papermill": {
     "duration": 0.011921,
     "end_time": "2024-08-09T16:45:08.366127",
     "exception": false,
     "start_time": "2024-08-09T16:45:08.354206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f896a07c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:45:08.393625Z",
     "iopub.status.busy": "2024-08-09T16:45:08.393233Z",
     "iopub.status.idle": "2024-08-09T16:45:10.025393Z",
     "shell.execute_reply": "2024-08-09T16:45:10.024035Z"
    },
    "papermill": {
     "duration": 1.649548,
     "end_time": "2024-08-09T16:45:10.027933",
     "exception": false,
     "start_time": "2024-08-09T16:45:08.378385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded ...\n"
     ]
    }
   ],
   "source": [
    "# variant and split selection (original paper used different augumentation per epoch)\n",
    "# dataset = dataloader.CholecT50( \n",
    "dataset = CholecT50( \n",
    "            dataset_dir=data_dir, \n",
    "            dataset_variant=dataset_variant,\n",
    "            test_fold=kfold,\n",
    "            augmentation_list=data_augmentations,\n",
    "            )\n",
    "\n",
    "# build dataset\n",
    "train_dataset, val_dataset, test_dataset = dataset.build()\n",
    "\n",
    "# train and val data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, prefetch_factor=3*batch_size, num_workers=3, pin_memory=True, persistent_workers=True, drop_last=False)\n",
    "val_dataloader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, prefetch_factor=3*batch_size, num_workers=3, pin_memory=True, persistent_workers=True, drop_last=False)\n",
    " \n",
    "# test data set is built per video, so load differently\n",
    "test_dataloaders = []\n",
    "for video_dataset in test_dataset:\n",
    "    test_dataloader = DataLoader(video_dataset, batch_size=batch_size, shuffle=False, prefetch_factor=3*batch_size, num_workers=3, pin_memory=True, persistent_workers=True, drop_last=False)\n",
    "    test_dataloaders.append(test_dataloader)\n",
    "print(\"Dataset loaded ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413944e",
   "metadata": {
    "papermill": {
     "duration": 0.012328,
     "end_time": "2024-08-09T16:45:10.053136",
     "exception": false,
     "start_time": "2024-08-09T16:45:10.040808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Build some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "518bea13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:45:10.079969Z",
     "iopub.status.busy": "2024-08-09T16:45:10.079118Z",
     "iopub.status.idle": "2024-08-09T16:45:12.284822Z",
     "shell.execute_reply": "2024-08-09T16:45:12.283606Z"
    },
    "papermill": {
     "duration": 2.221648,
     "end_time": "2024-08-09T16:45:12.287221",
     "exception": false,
     "start_time": "2024-08-09T16:45:10.065573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 134MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built ...\n",
      "Metrics built ...\n"
     ]
    }
   ],
   "source": [
    "# solve class imbalance\n",
    "class_weights = get_weight_balancing(case=dataset_variant)\n",
    "if 'crossval' in dataset_variant:\n",
    "    tool_weight   = class_weights[kfold]['tool']\n",
    "    verb_weight   = class_weights[kfold]['verb']\n",
    "    target_weight = class_weights[kfold]['target']\n",
    "else:\n",
    "    tool_weight   = class_weights['tool']\n",
    "    verb_weight   = class_weights['verb']\n",
    "    target_weight = class_weights['target']\n",
    "\n",
    "# Or constant weights from average of the random sampling of the dataset: we found this to produce better result.\n",
    "tool_weight     = [0.93487068, 0.94234964, 0.93487068, 1.18448115, 1.02368339, 0.97974447]\n",
    "verb_weight     = [0.60002400, 0.60002400, 0.60002400, 0.61682467, 0.67082683, 0.80163207, 0.70562823, 2.11208448, 2.69230769, 0.60062402]\n",
    "target_weight   = [0.49752894, 0.52041527, 0.49752894, 0.51394739, 2.71899565, 1.75577963, 0.58509403, 1.25228034, 0.49752894, 2.42993134, 0.49802647, 0.87266576, 1.36074165, 0.50150917, 0.49802647]\n",
    "\n",
    "\n",
    "# model initialization\n",
    "# model = network.Rendezvous('resnet18', hr_output=hr_output, use_ln=use_ln).cuda()\n",
    "model = Rendezvous('resnet18', hr_output=hr_output, use_ln=use_ln).cuda()\n",
    "# model = Rendezvous('efficientnet-b0', hr_output=hr_output, use_ln=use_ln).cuda() # another basemodel\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters()) # total number of parameters in the model\n",
    "pytorch_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad) # total number of trainable parameters in the model\n",
    "\n",
    "\n",
    "# performance tracker for hyper-parameter tuning\n",
    "benchmark = torch.nn.Parameter(torch.tensor([0.0]), requires_grad=False)\n",
    "print(\"Model built ...\")\n",
    "\n",
    "# loss functions\n",
    "activation  = nn.Sigmoid()\n",
    "\n",
    "# BCEWithLogitsLoss: \n",
    "# Binary Cross-Entropy Loss with logits, which combines a sigmoid layer and the binary cross-entropy loss in one function\n",
    "# used for multi-label classification tasks\n",
    "loss_fn_i   = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(tool_weight).cuda())\n",
    "loss_fn_v   = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(verb_weight).cuda())\n",
    "loss_fn_t   = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(target_weight).cuda())\n",
    "\n",
    "# loss function for the triplet without class weights\n",
    "loss_fn_ivt = nn.BCEWithLogitsLoss()\n",
    "\n",
    "import numpy as np\n",
    "np.float = np.float64\n",
    "np.int = int\n",
    "\n",
    "##%% evaluation metrics\n",
    "mAP = ivtmetrics.Recognition(100)\n",
    "mAP.reset_global()\n",
    "if not set_chlg_eval:\n",
    "    mAPi = ivtmetrics.Recognition(6)\n",
    "    mAPv = ivtmetrics.Recognition(10)\n",
    "    mAPt = ivtmetrics.Recognition(15)\n",
    "    mAPi.reset_global()\n",
    "    mAPv.reset_global()\n",
    "    mAPt.reset_global()\n",
    "print(\"Metrics built ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6c6b2",
   "metadata": {
    "papermill": {
     "duration": 0.013694,
     "end_time": "2024-08-09T16:45:12.314384",
     "exception": false,
     "start_time": "2024-08-09T16:45:12.300690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Optimizer and load model's weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99d0e333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:45:12.342765Z",
     "iopub.status.busy": "2024-08-09T16:45:12.342335Z",
     "iopub.status.idle": "2024-08-09T16:45:12.371554Z",
     "shell.execute_reply": "2024-08-09T16:45:12.370177Z"
    },
    "papermill": {
     "duration": 0.046404,
     "end_time": "2024-08-09T16:45:12.374159",
     "exception": false,
     "start_time": "2024-08-09T16:45:12.327755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's weight loaded ...\n"
     ]
    }
   ],
   "source": [
    "# optimizer and lr scheduler\n",
    "wp_lr           = [lr/power for lr in learning_rates] # scaling the initial learning rates\n",
    "\n",
    "module_i        = list(set(model.parameters()) - set(model.encoder.cagam.parameters()) - set(model.encoder.bottleneck.parameters()) - set(model.decoder.parameters()))\n",
    "module_ivt      = list(set(model.encoder.bottleneck.parameters()).union(set(model.decoder.parameters())))\n",
    "module_vt       = model.encoder.cagam.parameters()\n",
    "\n",
    "optimizer_i     = torch.optim.SGD(module_i, lr=wp_lr[0], weight_decay=weight_decay)\n",
    "scheduler_ia    = torch.optim.lr_scheduler.LinearLR(optimizer_i, start_factor=power, total_iters=warmups[0])\n",
    "scheduler_ib    = torch.optim.lr_scheduler.ExponentialLR(optimizer_i, gamma=decay_rate)\n",
    "scheduler_i     = torch.optim.lr_scheduler.SequentialLR(optimizer_i, schedulers=[scheduler_ia, scheduler_ib], milestones=[warmups[0]+1])\n",
    "\n",
    "optimizer_vt    = torch.optim.SGD(module_vt, lr=wp_lr[1], weight_decay=weight_decay)\n",
    "scheduler_vta   = torch.optim.lr_scheduler.LinearLR(optimizer_vt, start_factor=power, total_iters=warmups[1])\n",
    "scheduler_vtb   = torch.optim.lr_scheduler.ExponentialLR(optimizer_vt, gamma=decay_rate)\n",
    "scheduler_vt    = torch.optim.lr_scheduler.SequentialLR(optimizer_vt, schedulers=[scheduler_vta, scheduler_vtb], milestones=[warmups[1]+1])\n",
    "\n",
    "optimizer_ivt   = torch.optim.SGD(module_ivt, lr=wp_lr[2], weight_decay=weight_decay)\n",
    "scheduler_ivta  = torch.optim.lr_scheduler.LinearLR(optimizer_ivt, start_factor=power, total_iters=warmups[2])\n",
    "scheduler_ivtb  = torch.optim.lr_scheduler.ExponentialLR(optimizer_ivt, gamma=decay_rate)\n",
    "scheduler_ivt   = torch.optim.lr_scheduler.SequentialLR(optimizer_ivt, schedulers=[scheduler_ivta, scheduler_ivtb], milestones=[warmups[2]+1])\n",
    "\n",
    "lr_schedulers   = [scheduler_i, scheduler_vt, scheduler_ivt]\n",
    "optimizers      = [optimizer_i, optimizer_vt, optimizer_ivt]\n",
    "\n",
    "\n",
    "# 6.27: after changing basemodel, filter the unmatched weights and load them\n",
    "# prevent loading mismatched parameters and ensure only compatible weights\n",
    "def load_filtered_state_dict(model, state_dict):\n",
    "    model_state_dict = model.state_dict()\n",
    "    filtered_state_dict = {k: v for k, v in state_dict.items() if k in model_state_dict and model_state_dict[k].shape == v.shape}\n",
    "    model_state_dict.update(filtered_state_dict)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "##%% checkpoints/weights\n",
    "if os.path.exists(ckpt_path):\n",
    "    # modified\n",
    "    state_dict = torch.load(ckpt_path)\n",
    "    load_filtered_state_dict(model, state_dict)\n",
    "#     model.load_state_dict(torch.load(ckpt_path)) # original\n",
    "#     model.load_state_dict(torch.load(ckpt_path), strict=False)  # modify basemodel: strict=False: ignore mismatches.\n",
    "\n",
    "    resume_ckpt = ckpt_path # original\n",
    "    \n",
    "elif os.path.exists(pretrain_dir):\n",
    "    pretrained_dict = torch.load(pretrain_dir)\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    model.state_dict().update(pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    resume_ckpt = pretrain_dir\n",
    "    \n",
    "# modify basemodel\n",
    "# if os.path.exists(ckpt_path):\n",
    "#     state_dict = torch.load(ckpt_path)\n",
    "#     load_filtered_state_dict(model, state_dict)\n",
    "#     resume_ckpt = ckpt_path\n",
    "# else:\n",
    "#     print(\"未找到检查点文件，使用默认的预训练 EfficientNet 权重。\")\n",
    "\n",
    "print(\"Model's weight loaded ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f952a04",
   "metadata": {
    "papermill": {
     "duration": 0.014614,
     "end_time": "2024-08-09T16:45:12.402142",
     "exception": false,
     "start_time": "2024-08-09T16:45:12.387528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5ac9a",
   "metadata": {
    "papermill": {
     "duration": 0.013107,
     "end_time": "2024-08-09T16:45:12.428713",
     "exception": false,
     "start_time": "2024-08-09T16:45:12.415606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Due to the extensive output content from the training and testing functions, only the results from 1 epoch is used below to demonstrate the training and testing process of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "707712b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:45:12.457637Z",
     "iopub.status.busy": "2024-08-09T16:45:12.457232Z",
     "iopub.status.idle": "2024-08-09T17:23:58.308477Z",
     "shell.execute_reply": "2024-08-09T17:23:58.306813Z"
    },
    "papermill": {
     "duration": 2326.149338,
     "end_time": "2024-08-09T17:23:58.591624",
     "exception": false,
     "start_time": "2024-08-09T16:45:12.442286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_train: False, is_test: False\n",
      "Training | lr: [[0.01], [0.01], [0.01]] | epoch 0 | \n",
      "Batch 1/1970 | Losses => i: [0.8923] v: [0.8756] t: [0.9071] ivt: [0.9911]\n",
      "Batch 2/1970 | Losses => i: [1.2241] v: [0.8715] t: [0.9329] ivt: [0.9731]\n",
      "Batch 3/1970 | Losses => i: [1.3250] v: [0.8860] t: [0.9399] ivt: [0.9397]\n",
      "Batch 4/1970 | Losses => i: [1.3352] v: [0.8727] t: [0.9348] ivt: [0.9154]\n",
      "Batch 5/1970 | Losses => i: [1.2936] v: [0.8617] t: [0.9531] ivt: [0.8872]\n",
      "Batch 6/1970 | Losses => i: [1.4102] v: [0.8699] t: [0.9457] ivt: [0.8835]\n",
      "Batch 7/1970 | Losses => i: [1.3589] v: [0.8744] t: [0.9568] ivt: [0.8450]\n",
      "Batch 8/1970 | Losses => i: [1.3948] v: [0.8647] t: [0.9543] ivt: [0.8268]\n",
      "Batch 9/1970 | Losses => i: [1.3099] v: [0.8654] t: [0.9586] ivt: [0.8053]\n",
      "Batch 10/1970 | Losses => i: [1.2676] v: [0.8586] t: [0.9465] ivt: [0.7818]\n",
      "Batch 11/1970 | Losses => i: [1.2741] v: [0.8509] t: [0.9353] ivt: [0.7539]\n",
      "Batch 12/1970 | Losses => i: [1.2427] v: [0.8525] t: [0.9351] ivt: [0.7338]\n",
      "Batch 13/1970 | Losses => i: [1.1379] v: [0.8593] t: [0.9225] ivt: [0.7231]\n",
      "Batch 14/1970 | Losses => i: [1.1477] v: [0.8579] t: [0.9220] ivt: [0.7095]\n",
      "Batch 15/1970 | Losses => i: [1.1971] v: [0.8579] t: [0.9227] ivt: [0.6923]\n",
      "Batch 16/1970 | Losses => i: [1.1520] v: [0.8549] t: [0.9244] ivt: [0.6636]\n",
      "Batch 17/1970 | Losses => i: [1.1663] v: [0.8766] t: [0.9239] ivt: [0.6494]\n",
      "Batch 18/1970 | Losses => i: [1.1349] v: [0.8598] t: [0.9216] ivt: [0.6319]\n",
      "Batch 19/1970 | Losses => i: [1.1433] v: [0.8561] t: [0.9168] ivt: [0.6279]\n",
      "Batch 20/1970 | Losses => i: [1.0739] v: [0.8506] t: [0.9163] ivt: [0.6078]\n",
      "Batch 21/1970 | Losses => i: [1.0604] v: [0.8623] t: [0.9117] ivt: [0.5963]\n",
      "Batch 22/1970 | Losses => i: [1.0772] v: [0.8643] t: [0.9147] ivt: [0.5805]\n",
      "Batch 23/1970 | Losses => i: [1.0244] v: [0.8426] t: [0.8991] ivt: [0.5642]\n",
      "Batch 24/1970 | Losses => i: [1.0467] v: [0.8534] t: [0.9050] ivt: [0.5491]\n",
      "Batch 25/1970 | Losses => i: [1.0685] v: [0.8620] t: [0.8987] ivt: [0.5333]\n",
      "Batch 26/1970 | Losses => i: [1.0233] v: [0.8381] t: [0.8912] ivt: [0.5242]\n",
      "Batch 27/1970 | Losses => i: [1.0238] v: [0.8427] t: [0.8868] ivt: [0.5161]\n",
      "Batch 28/1970 | Losses => i: [0.9833] v: [0.8410] t: [0.8847] ivt: [0.5056]\n",
      "Batch 29/1970 | Losses => i: [1.0301] v: [0.8476] t: [0.8771] ivt: [0.4932]\n",
      "Batch 30/1970 | Losses => i: [1.0337] v: [0.8442] t: [0.8844] ivt: [0.4823]\n",
      "Batch 31/1970 | Losses => i: [1.0127] v: [0.8468] t: [0.8838] ivt: [0.4621]\n",
      "Batch 32/1970 | Losses => i: [0.9554] v: [0.8245] t: [0.8658] ivt: [0.4574]\n",
      "Batch 33/1970 | Losses => i: [0.9350] v: [0.8293] t: [0.8700] ivt: [0.4459]\n",
      "Batch 34/1970 | Losses => i: [0.9504] v: [0.8346] t: [0.8632] ivt: [0.4474]\n",
      "Batch 35/1970 | Losses => i: [0.9458] v: [0.8356] t: [0.8647] ivt: [0.4312]\n",
      "Batch 36/1970 | Losses => i: [0.9484] v: [0.8462] t: [0.8794] ivt: [0.4151]\n",
      "Batch 37/1970 | Losses => i: [0.9442] v: [0.8391] t: [0.8660] ivt: [0.4097]\n",
      "Batch 38/1970 | Losses => i: [0.9287] v: [0.8225] t: [0.8596] ivt: [0.4055]\n",
      "Batch 39/1970 | Losses => i: [0.9473] v: [0.8330] t: [0.8584] ivt: [0.3959]\n",
      "Batch 40/1970 | Losses => i: [0.9231] v: [0.8234] t: [0.8589] ivt: [0.3829]\n",
      "Batch 41/1970 | Losses => i: [0.8966] v: [0.8209] t: [0.8534] ivt: [0.3811]\n",
      "Batch 42/1970 | Losses => i: [0.8981] v: [0.8116] t: [0.8416] ivt: [0.3713]\n",
      "Batch 43/1970 | Losses => i: [0.8874] v: [0.8187] t: [0.8519] ivt: [0.3653]\n",
      "Batch 44/1970 | Losses => i: [0.8854] v: [0.8182] t: [0.8471] ivt: [0.3557]\n",
      "Batch 45/1970 | Losses => i: [0.8662] v: [0.8185] t: [0.8486] ivt: [0.3461]\n",
      "Batch 46/1970 | Losses => i: [0.8756] v: [0.8107] t: [0.8425] ivt: [0.3405]\n",
      "Batch 47/1970 | Losses => i: [0.8536] v: [0.8114] t: [0.8383] ivt: [0.3300]\n",
      "Batch 48/1970 | Losses => i: [0.8598] v: [0.8227] t: [0.8490] ivt: [0.3278]\n",
      "Batch 49/1970 | Losses => i: [0.8845] v: [0.8330] t: [0.8532] ivt: [0.3223]\n",
      "Batch 50/1970 | Losses => i: [0.9472] v: [0.8121] t: [0.8408] ivt: [0.3226]\n",
      "Batch 51/1970 | Losses => i: [0.9472] v: [0.7941] t: [0.8341] ivt: [0.3141]\n",
      "Batch 52/1970 | Losses => i: [0.9534] v: [0.8030] t: [0.8351] ivt: [0.3041]\n",
      "Batch 53/1970 | Losses => i: [0.9188] v: [0.8068] t: [0.8327] ivt: [0.3030]\n",
      "Batch 54/1970 | Losses => i: [0.9026] v: [0.8154] t: [0.8388] ivt: [0.2964]\n",
      "Batch 55/1970 | Losses => i: [0.9200] v: [0.8026] t: [0.8333] ivt: [0.2919]\n",
      "Batch 56/1970 | Losses => i: [0.8816] v: [0.7958] t: [0.8194] ivt: [0.2875]\n",
      "Batch 57/1970 | Losses => i: [0.8491] v: [0.8082] t: [0.8347] ivt: [0.2863]\n",
      "Batch 58/1970 | Losses => i: [0.8740] v: [0.8105] t: [0.8348] ivt: [0.2828]\n",
      "Batch 59/1970 | Losses => i: [0.8790] v: [0.8069] t: [0.8249] ivt: [0.2758]\n",
      "Batch 60/1970 | Losses => i: [0.8737] v: [0.8097] t: [0.8274] ivt: [0.2769]\n",
      "Batch 61/1970 | Losses => i: [0.8453] v: [0.7923] t: [0.8176] ivt: [0.2679]\n",
      "Batch 62/1970 | Losses => i: [0.8644] v: [0.8062] t: [0.8365] ivt: [0.2627]\n",
      "Batch 63/1970 | Losses => i: [0.8622] v: [0.7949] t: [0.8258] ivt: [0.2585]\n",
      "Batch 64/1970 | Losses => i: [0.8545] v: [0.7976] t: [0.8265] ivt: [0.2539]\n",
      "Batch 65/1970 | Losses => i: [0.8928] v: [0.8002] t: [0.8269] ivt: [0.2519]\n",
      "Batch 66/1970 | Losses => i: [0.8341] v: [0.7940] t: [0.8171] ivt: [0.2497]\n",
      "Batch 67/1970 | Losses => i: [0.8248] v: [0.8046] t: [0.8219] ivt: [0.2499]\n",
      "Batch 68/1970 | Losses => i: [0.8171] v: [0.7954] t: [0.8198] ivt: [0.2421]\n",
      "Batch 69/1970 | Losses => i: [0.8342] v: [0.7870] t: [0.8133] ivt: [0.2351]\n",
      "Batch 70/1970 | Losses => i: [0.8103] v: [0.7842] t: [0.8020] ivt: [0.2391]\n",
      "Batch 71/1970 | Losses => i: [0.8156] v: [0.7870] t: [0.8050] ivt: [0.2290]\n",
      "Batch 72/1970 | Losses => i: [0.8174] v: [0.7927] t: [0.8089] ivt: [0.2295]\n",
      "Batch 73/1970 | Losses => i: [0.8006] v: [0.7911] t: [0.8091] ivt: [0.2286]\n",
      "Batch 74/1970 | Losses => i: [0.8202] v: [0.7850] t: [0.7960] ivt: [0.2300]\n",
      "Batch 75/1970 | Losses => i: [0.7859] v: [0.7752] t: [0.7943] ivt: [0.2214]\n",
      "Batch 76/1970 | Losses => i: [0.7951] v: [0.7785] t: [0.7925] ivt: [0.2239]\n",
      "Batch 77/1970 | Losses => i: [0.8081] v: [0.7860] t: [0.8093] ivt: [0.2166]\n",
      "Batch 78/1970 | Losses => i: [0.8839] v: [0.7753] t: [0.7968] ivt: [0.2175]\n",
      "Batch 79/1970 | Losses => i: [0.8667] v: [0.7821] t: [0.7924] ivt: [0.2129]\n",
      "Batch 80/1970 | Losses => i: [0.9059] v: [0.7841] t: [0.8010] ivt: [0.2074]\n",
      "Batch 81/1970 | Losses => i: [0.8361] v: [0.7739] t: [0.7895] ivt: [0.2076]\n",
      "Batch 82/1970 | Losses => i: [0.8576] v: [0.7701] t: [0.7904] ivt: [0.2071]\n",
      "Batch 83/1970 | Losses => i: [0.8659] v: [0.7806] t: [0.7981] ivt: [0.1972]\n",
      "Batch 84/1970 | Losses => i: [0.8461] v: [0.7666] t: [0.7837] ivt: [0.2010]\n",
      "Batch 85/1970 | Losses => i: [0.8367] v: [0.7864] t: [0.7951] ivt: [0.1980]\n",
      "Batch 86/1970 | Losses => i: [0.8423] v: [0.7662] t: [0.7808] ivt: [0.1945]\n",
      "Batch 87/1970 | Losses => i: [0.8133] v: [0.7675] t: [0.7877] ivt: [0.1926]\n",
      "Batch 88/1970 | Losses => i: [0.8230] v: [0.7716] t: [0.7834] ivt: [0.1882]\n",
      "Batch 89/1970 | Losses => i: [0.8312] v: [0.7705] t: [0.7818] ivt: [0.1859]\n",
      "Batch 90/1970 | Losses => i: [0.8532] v: [0.7748] t: [0.7832] ivt: [0.1899]\n",
      "Batch 91/1970 | Losses => i: [0.8425] v: [0.7648] t: [0.7744] ivt: [0.1844]\n",
      "Batch 92/1970 | Losses => i: [0.8002] v: [0.7661] t: [0.7759] ivt: [0.1836]\n",
      "Batch 93/1970 | Losses => i: [0.8032] v: [0.7599] t: [0.7732] ivt: [0.1821]\n",
      "Batch 94/1970 | Losses => i: [0.7985] v: [0.7675] t: [0.7763] ivt: [0.1833]\n",
      "Batch 95/1970 | Losses => i: [0.7845] v: [0.7582] t: [0.7710] ivt: [0.1817]\n",
      "Batch 96/1970 | Losses => i: [0.7729] v: [0.7651] t: [0.7890] ivt: [0.1765]\n",
      "Batch 97/1970 | Losses => i: [0.8087] v: [0.7637] t: [0.7793] ivt: [0.1812]\n",
      "Batch 98/1970 | Losses => i: [0.8034] v: [0.7514] t: [0.7674] ivt: [0.1747]\n",
      "Batch 99/1970 | Losses => i: [0.7775] v: [0.7548] t: [0.7720] ivt: [0.1719]\n",
      "Batch 100/1970 | Losses => i: [0.7875] v: [0.7716] t: [0.7804] ivt: [0.1657]\n",
      "Batch 101/1970 | Losses => i: [0.7752] v: [0.7607] t: [0.7666] ivt: [0.1697]\n",
      "Batch 102/1970 | Losses => i: [0.7871] v: [0.7594] t: [0.7680] ivt: [0.1672]\n",
      "Batch 103/1970 | Losses => i: [0.7889] v: [0.7687] t: [0.7643] ivt: [0.1675]\n",
      "Batch 104/1970 | Losses => i: [0.7571] v: [0.7556] t: [0.7684] ivt: [0.1650]\n",
      "Batch 105/1970 | Losses => i: [0.7537] v: [0.7591] t: [0.7646] ivt: [0.1645]\n",
      "Batch 106/1970 | Losses => i: [0.7650] v: [0.7494] t: [0.7670] ivt: [0.1646]\n",
      "Batch 107/1970 | Losses => i: [0.7417] v: [0.7400] t: [0.7585] ivt: [0.1639]\n",
      "Batch 108/1970 | Losses => i: [0.7650] v: [0.7639] t: [0.7689] ivt: [0.1592]\n",
      "Batch 109/1970 | Losses => i: [0.7519] v: [0.7441] t: [0.7566] ivt: [0.1602]\n",
      "Batch 110/1970 | Losses => i: [0.7402] v: [0.7417] t: [0.7644] ivt: [0.1615]\n",
      "Batch 111/1970 | Losses => i: [0.9921] v: [0.7576] t: [0.7774] ivt: [0.1583]\n",
      "Batch 112/1970 | Losses => i: [0.8215] v: [0.7495] t: [0.7685] ivt: [0.1536]\n",
      "Batch 113/1970 | Losses => i: [1.0482] v: [0.7683] t: [0.7868] ivt: [0.1525]\n",
      "Batch 114/1970 | Losses => i: [0.9488] v: [0.7491] t: [0.7675] ivt: [0.1544]\n",
      "Batch 115/1970 | Losses => i: [1.0224] v: [0.7651] t: [0.7744] ivt: [0.1479]\n",
      "Batch 116/1970 | Losses => i: [0.9881] v: [0.7583] t: [0.7695] ivt: [0.1499]\n",
      "Batch 117/1970 | Losses => i: [0.9430] v: [0.7452] t: [0.7697] ivt: [0.1500]\n",
      "Batch 118/1970 | Losses => i: [0.8123] v: [0.7419] t: [0.7610] ivt: [0.1498]\n",
      "Batch 119/1970 | Losses => i: [0.8239] v: [0.7464] t: [0.7673] ivt: [0.1461]\n",
      "Batch 120/1970 | Losses => i: [0.8263] v: [0.7384] t: [0.7611] ivt: [0.1455]\n",
      "Batch 121/1970 | Losses => i: [0.9338] v: [0.7529] t: [0.7684] ivt: [0.1419]\n",
      "Batch 122/1970 | Losses => i: [0.8299] v: [0.7387] t: [0.7553] ivt: [0.1451]\n",
      "Batch 123/1970 | Losses => i: [0.8125] v: [0.7460] t: [0.7589] ivt: [0.1447]\n",
      "Batch 124/1970 | Losses => i: [0.8144] v: [0.7471] t: [0.7614] ivt: [0.1394]\n",
      "Batch 125/1970 | Losses => i: [0.8627] v: [0.7383] t: [0.7545] ivt: [0.1434]\n",
      "Batch 126/1970 | Losses => i: [0.8266] v: [0.7397] t: [0.7544] ivt: [0.1448]\n",
      "Batch 127/1970 | Losses => i: [0.8306] v: [0.7255] t: [0.7544] ivt: [0.1400]\n",
      "Batch 128/1970 | Losses => i: [0.7805] v: [0.7267] t: [0.7487] ivt: [0.1400]\n",
      "Batch 129/1970 | Losses => i: [0.8383] v: [0.7287] t: [0.7523] ivt: [0.1397]\n",
      "Batch 130/1970 | Losses => i: [0.8356] v: [0.7399] t: [0.7517] ivt: [0.1384]\n",
      "Batch 131/1970 | Losses => i: [0.8567] v: [0.7375] t: [0.7602] ivt: [0.1360]\n",
      "Batch 132/1970 | Losses => i: [0.8051] v: [0.7420] t: [0.7552] ivt: [0.1311]\n",
      "Batch 133/1970 | Losses => i: [0.7917] v: [0.7349] t: [0.7520] ivt: [0.1355]\n",
      "Batch 134/1970 | Losses => i: [0.8694] v: [0.7458] t: [0.7556] ivt: [0.1335]\n",
      "Batch 135/1970 | Losses => i: [0.7674] v: [0.7225] t: [0.7416] ivt: [0.1304]\n",
      "Batch 136/1970 | Losses => i: [0.7553] v: [0.7355] t: [0.7363] ivt: [0.1376]\n",
      "Batch 137/1970 | Losses => i: [0.7628] v: [0.7307] t: [0.7434] ivt: [0.1340]\n",
      "Batch 138/1970 | Losses => i: [0.7607] v: [0.7285] t: [0.7409] ivt: [0.1312]\n",
      "Batch 139/1970 | Losses => i: [0.7657] v: [0.7323] t: [0.7482] ivt: [0.1295]\n",
      "Batch 140/1970 | Losses => i: [0.7567] v: [0.7227] t: [0.7417] ivt: [0.1281]\n",
      "Batch 141/1970 | Losses => i: [0.7293] v: [0.7235] t: [0.7376] ivt: [0.1254]\n",
      "Batch 142/1970 | Losses => i: [0.7599] v: [0.7303] t: [0.7444] ivt: [0.1253]\n",
      "Batch 143/1970 | Losses => i: [0.7256] v: [0.7141] t: [0.7330] ivt: [0.1327]\n",
      "Batch 144/1970 | Losses => i: [0.7359] v: [0.7127] t: [0.7318] ivt: [0.1264]\n",
      "Batch 145/1970 | Losses => i: [0.7764] v: [0.7231] t: [0.7352] ivt: [0.1312]\n",
      "Batch 146/1970 | Losses => i: [0.7540] v: [0.7244] t: [0.7405] ivt: [0.1287]\n",
      "Batch 147/1970 | Losses => i: [0.7923] v: [0.7258] t: [0.7377] ivt: [0.1270]\n",
      "Batch 148/1970 | Losses => i: [0.7288] v: [0.7174] t: [0.7288] ivt: [0.1267]\n",
      "Batch 149/1970 | Losses => i: [0.7275] v: [0.7118] t: [0.7268] ivt: [0.1264]\n",
      "Batch 150/1970 | Losses => i: [0.7560] v: [0.7132] t: [0.7410] ivt: [0.1276]\n",
      "Batch 151/1970 | Losses => i: [0.7519] v: [0.7237] t: [0.7363] ivt: [0.1230]\n",
      "Batch 152/1970 | Losses => i: [0.7982] v: [0.7311] t: [0.7479] ivt: [0.1176]\n",
      "Batch 153/1970 | Losses => i: [0.7103] v: [0.7159] t: [0.7296] ivt: [0.1204]\n",
      "Batch 154/1970 | Losses => i: [0.7162] v: [0.7101] t: [0.7300] ivt: [0.1211]\n",
      "Batch 155/1970 | Losses => i: [0.7198] v: [0.7114] t: [0.7273] ivt: [0.1202]\n",
      "Batch 156/1970 | Losses => i: [0.7192] v: [0.7148] t: [0.7264] ivt: [0.1178]\n",
      "Batch 157/1970 | Losses => i: [0.7252] v: [0.7147] t: [0.7243] ivt: [0.1180]\n",
      "Batch 158/1970 | Losses => i: [0.7044] v: [0.7163] t: [0.7370] ivt: [0.1164]\n",
      "Batch 159/1970 | Losses => i: [0.7100] v: [0.6997] t: [0.7178] ivt: [0.1227]\n",
      "Batch 160/1970 | Losses => i: [0.7500] v: [0.7264] t: [0.7310] ivt: [0.1193]\n",
      "Batch 161/1970 | Losses => i: [0.7628] v: [0.7169] t: [0.7264] ivt: [0.1156]\n",
      "Batch 162/1970 | Losses => i: [0.7070] v: [0.7099] t: [0.7192] ivt: [0.1225]\n",
      "Batch 163/1970 | Losses => i: [0.7163] v: [0.7112] t: [0.7310] ivt: [0.1141]\n",
      "Batch 164/1970 | Losses => i: [0.7041] v: [0.7169] t: [0.7252] ivt: [0.1185]\n",
      "Batch 165/1970 | Losses => i: [0.6977] v: [0.7014] t: [0.7137] ivt: [0.1212]\n",
      "Batch 166/1970 | Losses => i: [0.6904] v: [0.7002] t: [0.7281] ivt: [0.1198]\n",
      "Batch 167/1970 | Losses => i: [0.7234] v: [0.7054] t: [0.7179] ivt: [0.1135]\n",
      "Batch 168/1970 | Losses => i: [0.6611] v: [0.7005] t: [0.7161] ivt: [0.1152]\n",
      "Batch 169/1970 | Losses => i: [0.7055] v: [0.7074] t: [0.7207] ivt: [0.1106]\n",
      "Batch 170/1970 | Losses => i: [0.7420] v: [0.7225] t: [0.7339] ivt: [0.1154]\n",
      "Batch 171/1970 | Losses => i: [0.7023] v: [0.7081] t: [0.7219] ivt: [0.1092]\n",
      "Batch 172/1970 | Losses => i: [0.7275] v: [0.7046] t: [0.7236] ivt: [0.1100]\n",
      "Batch 173/1970 | Losses => i: [0.7184] v: [0.7014] t: [0.7217] ivt: [0.1135]\n",
      "Batch 174/1970 | Losses => i: [0.6951] v: [0.7075] t: [0.7115] ivt: [0.1124]\n",
      "Batch 175/1970 | Losses => i: [0.7567] v: [0.7188] t: [0.7246] ivt: [0.1127]\n",
      "Batch 176/1970 | Losses => i: [0.6900] v: [0.7002] t: [0.7178] ivt: [0.1115]\n",
      "Batch 177/1970 | Losses => i: [0.6880] v: [0.7020] t: [0.7079] ivt: [0.1147]\n",
      "Batch 178/1970 | Losses => i: [0.6950] v: [0.7105] t: [0.7207] ivt: [0.1134]\n",
      "Batch 179/1970 | Losses => i: [0.7296] v: [0.7024] t: [0.7188] ivt: [0.1086]\n",
      "Batch 180/1970 | Losses => i: [0.7025] v: [0.7113] t: [0.7164] ivt: [0.1118]\n",
      "Batch 181/1970 | Losses => i: [0.6860] v: [0.6940] t: [0.7087] ivt: [0.1092]\n",
      "Batch 182/1970 | Losses => i: [0.6982] v: [0.7018] t: [0.7136] ivt: [0.1140]\n",
      "Batch 183/1970 | Losses => i: [0.7332] v: [0.7159] t: [0.7398] ivt: [0.1079]\n",
      "Batch 184/1970 | Losses => i: [0.6936] v: [0.7149] t: [0.7257] ivt: [0.1079]\n",
      "Batch 185/1970 | Losses => i: [0.7087] v: [0.7077] t: [0.7250] ivt: [0.1068]\n",
      "Batch 186/1970 | Losses => i: [0.7108] v: [0.7188] t: [0.7296] ivt: [0.1056]\n",
      "Batch 187/1970 | Losses => i: [0.6845] v: [0.7047] t: [0.7269] ivt: [0.1068]\n",
      "Batch 188/1970 | Losses => i: [0.6886] v: [0.7115] t: [0.7245] ivt: [0.1018]\n",
      "Batch 189/1970 | Losses => i: [0.6792] v: [0.6990] t: [0.7167] ivt: [0.1074]\n",
      "Batch 190/1970 | Losses => i: [0.7074] v: [0.7041] t: [0.7324] ivt: [0.1075]\n",
      "Batch 191/1970 | Losses => i: [0.6904] v: [0.7081] t: [0.7268] ivt: [0.1007]\n",
      "Batch 192/1970 | Losses => i: [0.6707] v: [0.6979] t: [0.7118] ivt: [0.1033]\n",
      "Batch 193/1970 | Losses => i: [0.6838] v: [0.7045] t: [0.7205] ivt: [0.1045]\n",
      "Batch 194/1970 | Losses => i: [0.6985] v: [0.7076] t: [0.7338] ivt: [0.1040]\n",
      "Batch 195/1970 | Losses => i: [0.6720] v: [0.6922] t: [0.7091] ivt: [0.1069]\n",
      "Batch 196/1970 | Losses => i: [0.6612] v: [0.6889] t: [0.7093] ivt: [0.1038]\n",
      "Batch 197/1970 | Losses => i: [0.7081] v: [0.7074] t: [0.7223] ivt: [0.1021]\n",
      "Batch 198/1970 | Losses => i: [0.6705] v: [0.7025] t: [0.7197] ivt: [0.1015]\n",
      "Batch 199/1970 | Losses => i: [0.6748] v: [0.6940] t: [0.7166] ivt: [0.1054]\n",
      "Batch 200/1970 | Losses => i: [0.6766] v: [0.7055] t: [0.7152] ivt: [0.1009]\n",
      "Batch 201/1970 | Losses => i: [0.6713] v: [0.6933] t: [0.7142] ivt: [0.0986]\n",
      "Batch 202/1970 | Losses => i: [0.6717] v: [0.6967] t: [0.7075] ivt: [0.1035]\n",
      "Batch 203/1970 | Losses => i: [0.6720] v: [0.7009] t: [0.7133] ivt: [0.1036]\n",
      "Batch 204/1970 | Losses => i: [0.6719] v: [0.6981] t: [0.7084] ivt: [0.0953]\n",
      "Batch 205/1970 | Losses => i: [0.6732] v: [0.7042] t: [0.7207] ivt: [0.0923]\n",
      "Batch 206/1970 | Losses => i: [0.6767] v: [0.7002] t: [0.7225] ivt: [0.1031]\n",
      "Batch 207/1970 | Losses => i: [0.6528] v: [0.6914] t: [0.7084] ivt: [0.0981]\n",
      "Batch 208/1970 | Losses => i: [0.6945] v: [0.7070] t: [0.7220] ivt: [0.0941]\n",
      "Batch 209/1970 | Losses => i: [0.6685] v: [0.6888] t: [0.7057] ivt: [0.0970]\n",
      "Batch 210/1970 | Losses => i: [0.6515] v: [0.6951] t: [0.7082] ivt: [0.1004]\n",
      "Batch 211/1970 | Losses => i: [0.6664] v: [0.6905] t: [0.7075] ivt: [0.1021]\n",
      "Batch 212/1970 | Losses => i: [0.6575] v: [0.6824] t: [0.6999] ivt: [0.0994]\n",
      "Batch 213/1970 | Losses => i: [0.6661] v: [0.6891] t: [0.7049] ivt: [0.1018]\n",
      "Batch 214/1970 | Losses => i: [0.6696] v: [0.7032] t: [0.7151] ivt: [0.0950]\n",
      "Batch 215/1970 | Losses => i: [0.6580] v: [0.6804] t: [0.6983] ivt: [0.1020]\n",
      "Batch 216/1970 | Losses => i: [0.6798] v: [0.6965] t: [0.7091] ivt: [0.0924]\n",
      "Batch 217/1970 | Losses => i: [0.6752] v: [0.7004] t: [0.7075] ivt: [0.0921]\n",
      "Batch 218/1970 | Losses => i: [0.6517] v: [0.6848] t: [0.6930] ivt: [0.0992]\n",
      "Batch 219/1970 | Losses => i: [0.6529] v: [0.6807] t: [0.6908] ivt: [0.0998]\n",
      "Batch 220/1970 | Losses => i: [0.6473] v: [0.6761] t: [0.6977] ivt: [0.0918]\n",
      "Batch 221/1970 | Losses => i: [0.6458] v: [0.6887] t: [0.6973] ivt: [0.0952]\n",
      "Batch 222/1970 | Losses => i: [0.6305] v: [0.6776] t: [0.6968] ivt: [0.0992]\n",
      "Batch 223/1970 | Losses => i: [0.6471] v: [0.6896] t: [0.7053] ivt: [0.0906]\n",
      "Batch 224/1970 | Losses => i: [0.6624] v: [0.6981] t: [0.7105] ivt: [0.0935]\n",
      "Batch 225/1970 | Losses => i: [0.6446] v: [0.6885] t: [0.7058] ivt: [0.0954]\n",
      "Batch 226/1970 | Losses => i: [0.6599] v: [0.6842] t: [0.6966] ivt: [0.0970]\n",
      "Batch 227/1970 | Losses => i: [0.6598] v: [0.6896] t: [0.7003] ivt: [0.0958]\n",
      "Batch 228/1970 | Losses => i: [0.6491] v: [0.6906] t: [0.7025] ivt: [0.0985]\n",
      "Batch 229/1970 | Losses => i: [0.6493] v: [0.6775] t: [0.6941] ivt: [0.0929]\n",
      "Batch 230/1970 | Losses => i: [0.6501] v: [0.6878] t: [0.7038] ivt: [0.0931]\n",
      "Batch 231/1970 | Losses => i: [0.6409] v: [0.6901] t: [0.7057] ivt: [0.0913]\n",
      "Batch 232/1970 | Losses => i: [0.6368] v: [0.6788] t: [0.6981] ivt: [0.0974]\n",
      "Batch 233/1970 | Losses => i: [0.6402] v: [0.6789] t: [0.6984] ivt: [0.0946]\n",
      "Batch 234/1970 | Losses => i: [0.6568] v: [0.6994] t: [0.7150] ivt: [0.0894]\n",
      "Batch 235/1970 | Losses => i: [0.6450] v: [0.6777] t: [0.6946] ivt: [0.0939]\n",
      "Batch 236/1970 | Losses => i: [0.6361] v: [0.6713] t: [0.6995] ivt: [0.0936]\n",
      "Batch 237/1970 | Losses => i: [0.6613] v: [0.6982] t: [0.7047] ivt: [0.0885]\n",
      "Batch 238/1970 | Losses => i: [0.6382] v: [0.6641] t: [0.6892] ivt: [0.0965]\n",
      "Batch 239/1970 | Losses => i: [0.6526] v: [0.6964] t: [0.7035] ivt: [0.0888]\n",
      "Batch 240/1970 | Losses => i: [0.6420] v: [0.6755] t: [0.6920] ivt: [0.0930]\n",
      "Batch 241/1970 | Losses => i: [0.6513] v: [0.6793] t: [0.6945] ivt: [0.0932]\n",
      "Batch 242/1970 | Losses => i: [0.6586] v: [0.6852] t: [0.7044] ivt: [0.0858]\n",
      "Batch 243/1970 | Losses => i: [0.6376] v: [0.6875] t: [0.7006] ivt: [0.0880]\n",
      "Batch 244/1970 | Losses => i: [0.6431] v: [0.6892] t: [0.7029] ivt: [0.0851]\n",
      "Batch 245/1970 | Losses => i: [0.6353] v: [0.6859] t: [0.7068] ivt: [0.0867]\n",
      "Batch 246/1970 | Losses => i: [0.6357] v: [0.6938] t: [0.7056] ivt: [0.0918]\n",
      "Batch 247/1970 | Losses => i: [0.6443] v: [0.6995] t: [0.7154] ivt: [0.0869]\n",
      "Batch 248/1970 | Losses => i: [0.6431] v: [0.6798] t: [0.6898] ivt: [0.0923]\n",
      "Batch 249/1970 | Losses => i: [0.6398] v: [0.6708] t: [0.6888] ivt: [0.0877]\n",
      "Batch 250/1970 | Losses => i: [0.6372] v: [0.6759] t: [0.6843] ivt: [0.0848]\n",
      "Batch 251/1970 | Losses => i: [0.6405] v: [0.6749] t: [0.6871] ivt: [0.0880]\n",
      "Batch 252/1970 | Losses => i: [0.6348] v: [0.6810] t: [0.6944] ivt: [0.0860]\n",
      "Batch 253/1970 | Losses => i: [0.6355] v: [0.6840] t: [0.6962] ivt: [0.0900]\n",
      "Batch 254/1970 | Losses => i: [0.6400] v: [0.6849] t: [0.6967] ivt: [0.0882]\n",
      "Batch 255/1970 | Losses => i: [0.6223] v: [0.6661] t: [0.6843] ivt: [0.0875]\n",
      "Batch 256/1970 | Losses => i: [0.6436] v: [0.6735] t: [0.6930] ivt: [0.0834]\n",
      "Batch 257/1970 | Losses => i: [0.6329] v: [0.6796] t: [0.6945] ivt: [0.0802]\n",
      "Batch 258/1970 | Losses => i: [0.6257] v: [0.6630] t: [0.6785] ivt: [0.0897]\n",
      "Batch 259/1970 | Losses => i: [0.6321] v: [0.6746] t: [0.6881] ivt: [0.0820]\n",
      "Batch 260/1970 | Losses => i: [0.6164] v: [0.6669] t: [0.6908] ivt: [0.0871]\n",
      "Batch 261/1970 | Losses => i: [0.6275] v: [0.6658] t: [0.6818] ivt: [0.0876]\n",
      "Batch 262/1970 | Losses => i: [0.6264] v: [0.6737] t: [0.6868] ivt: [0.0880]\n",
      "Batch 263/1970 | Losses => i: [0.6298] v: [0.6653] t: [0.6817] ivt: [0.0830]\n",
      "Batch 264/1970 | Losses => i: [0.6344] v: [0.6734] t: [0.6890] ivt: [0.0911]\n",
      "Batch 265/1970 | Losses => i: [0.6230] v: [0.6641] t: [0.6827] ivt: [0.0836]\n",
      "Batch 266/1970 | Losses => i: [0.6181] v: [0.6594] t: [0.6767] ivt: [0.0867]\n",
      "Batch 267/1970 | Losses => i: [0.6276] v: [0.6703] t: [0.6838] ivt: [0.0836]\n",
      "Batch 268/1970 | Losses => i: [0.6312] v: [0.6742] t: [0.6896] ivt: [0.0847]\n",
      "Batch 269/1970 | Losses => i: [0.6297] v: [0.6807] t: [0.6853] ivt: [0.0888]\n",
      "Batch 270/1970 | Losses => i: [0.6299] v: [0.6684] t: [0.6889] ivt: [0.0837]\n",
      "Batch 271/1970 | Losses => i: [0.6119] v: [0.6721] t: [0.6946] ivt: [0.0829]\n",
      "Batch 272/1970 | Losses => i: [0.6287] v: [0.6695] t: [0.6885] ivt: [0.0833]\n",
      "Batch 273/1970 | Losses => i: [0.6196] v: [0.6639] t: [0.6854] ivt: [0.0886]\n",
      "Batch 274/1970 | Losses => i: [0.6255] v: [0.6760] t: [0.6899] ivt: [0.0805]\n",
      "Batch 275/1970 | Losses => i: [0.6251] v: [0.6697] t: [0.6778] ivt: [0.0842]\n",
      "Batch 276/1970 | Losses => i: [0.6226] v: [0.6668] t: [0.6823] ivt: [0.0820]\n",
      "Batch 277/1970 | Losses => i: [0.6194] v: [0.6605] t: [0.6743] ivt: [0.0842]\n",
      "Batch 278/1970 | Losses => i: [0.6111] v: [0.6702] t: [0.6842] ivt: [0.0810]\n",
      "Batch 279/1970 | Losses => i: [0.6258] v: [0.6664] t: [0.6814] ivt: [0.0859]\n",
      "Batch 280/1970 | Losses => i: [0.6236] v: [0.6639] t: [0.6800] ivt: [0.0808]\n",
      "Batch 281/1970 | Losses => i: [0.6217] v: [0.6602] t: [0.6778] ivt: [0.0809]\n",
      "Batch 282/1970 | Losses => i: [0.6104] v: [0.6666] t: [0.6892] ivt: [0.0786]\n",
      "Batch 283/1970 | Losses => i: [0.6245] v: [0.6713] t: [0.6853] ivt: [0.0880]\n",
      "Batch 284/1970 | Losses => i: [0.6159] v: [0.6750] t: [0.6892] ivt: [0.0819]\n",
      "Batch 285/1970 | Losses => i: [0.6056] v: [0.6585] t: [0.6864] ivt: [0.0785]\n",
      "Batch 286/1970 | Losses => i: [0.6282] v: [0.6742] t: [0.6837] ivt: [0.0780]\n",
      "Batch 287/1970 | Losses => i: [0.6074] v: [0.6588] t: [0.6699] ivt: [0.0858]\n",
      "Batch 288/1970 | Losses => i: [0.6117] v: [0.6588] t: [0.6711] ivt: [0.0806]\n",
      "Batch 289/1970 | Losses => i: [0.6203] v: [0.6530] t: [0.6753] ivt: [0.0757]\n",
      "Batch 290/1970 | Losses => i: [0.6082] v: [0.6553] t: [0.6718] ivt: [0.0776]\n",
      "Batch 291/1970 | Losses => i: [0.6116] v: [0.6649] t: [0.6746] ivt: [0.0780]\n",
      "Batch 292/1970 | Losses => i: [0.6160] v: [0.6639] t: [0.6828] ivt: [0.0783]\n",
      "Batch 293/1970 | Losses => i: [0.6127] v: [0.6667] t: [0.6772] ivt: [0.0776]\n",
      "Batch 294/1970 | Losses => i: [0.6195] v: [0.6621] t: [0.6777] ivt: [0.0793]\n",
      "Batch 295/1970 | Losses => i: [0.6082] v: [0.6593] t: [0.6769] ivt: [0.0776]\n",
      "Batch 296/1970 | Losses => i: [0.6199] v: [0.6679] t: [0.6807] ivt: [0.0785]\n",
      "Batch 297/1970 | Losses => i: [0.6094] v: [0.6600] t: [0.6728] ivt: [0.0787]\n",
      "Batch 298/1970 | Losses => i: [0.6124] v: [0.6520] t: [0.6683] ivt: [0.0842]\n",
      "Batch 299/1970 | Losses => i: [0.6060] v: [0.6539] t: [0.6731] ivt: [0.0825]\n",
      "Batch 300/1970 | Losses => i: [0.6151] v: [0.6614] t: [0.6758] ivt: [0.0751]\n",
      "Batch 301/1970 | Losses => i: [0.6106] v: [0.6573] t: [0.6720] ivt: [0.0791]\n",
      "Batch 302/1970 | Losses => i: [0.6090] v: [0.6661] t: [0.6770] ivt: [0.0779]\n",
      "Batch 303/1970 | Losses => i: [0.6077] v: [0.6452] t: [0.6659] ivt: [0.0834]\n",
      "Batch 304/1970 | Losses => i: [0.6175] v: [0.6685] t: [0.6768] ivt: [0.0790]\n",
      "Batch 305/1970 | Losses => i: [0.6128] v: [0.6691] t: [0.6790] ivt: [0.0685]\n",
      "Batch 306/1970 | Losses => i: [0.6097] v: [0.6590] t: [0.6756] ivt: [0.0849]\n",
      "Batch 307/1970 | Losses => i: [0.6119] v: [0.6666] t: [0.6846] ivt: [0.0750]\n",
      "Batch 308/1970 | Losses => i: [0.6132] v: [0.6603] t: [0.6742] ivt: [0.0729]\n",
      "Batch 309/1970 | Losses => i: [0.6058] v: [0.6617] t: [0.6766] ivt: [0.0767]\n",
      "Batch 310/1970 | Losses => i: [0.6408] v: [0.6555] t: [0.6737] ivt: [0.0788]\n",
      "Batch 311/1970 | Losses => i: [0.6246] v: [0.6567] t: [0.6691] ivt: [0.0758]\n",
      "Batch 312/1970 | Losses => i: [0.6353] v: [0.6676] t: [0.6718] ivt: [0.0808]\n",
      "Batch 313/1970 | Losses => i: [0.6320] v: [0.6623] t: [0.6739] ivt: [0.0759]\n",
      "Batch 314/1970 | Losses => i: [0.6166] v: [0.6442] t: [0.6671] ivt: [0.0841]\n",
      "Batch 315/1970 | Losses => i: [0.6126] v: [0.6544] t: [0.6705] ivt: [0.0772]\n",
      "Batch 316/1970 | Losses => i: [0.6109] v: [0.6526] t: [0.6667] ivt: [0.0795]\n",
      "Batch 317/1970 | Losses => i: [0.6258] v: [0.6620] t: [0.6737] ivt: [0.0784]\n",
      "Batch 318/1970 | Losses => i: [0.6411] v: [0.6632] t: [0.6738] ivt: [0.0744]\n",
      "Batch 319/1970 | Losses => i: [0.6348] v: [0.6525] t: [0.6732] ivt: [0.0787]\n",
      "Batch 320/1970 | Losses => i: [0.6279] v: [0.6580] t: [0.6688] ivt: [0.0804]\n",
      "Batch 321/1970 | Losses => i: [0.5987] v: [0.6525] t: [0.6633] ivt: [0.0799]\n",
      "Batch 322/1970 | Losses => i: [0.6049] v: [0.6605] t: [0.6693] ivt: [0.0752]\n",
      "Batch 323/1970 | Losses => i: [0.6011] v: [0.6597] t: [0.6698] ivt: [0.0752]\n",
      "Batch 324/1970 | Losses => i: [0.6159] v: [0.6518] t: [0.6692] ivt: [0.0709]\n",
      "Batch 325/1970 | Losses => i: [0.6109] v: [0.6542] t: [0.6675] ivt: [0.0795]\n",
      "Batch 326/1970 | Losses => i: [0.6291] v: [0.6479] t: [0.6673] ivt: [0.0735]\n",
      "Batch 327/1970 | Losses => i: [0.6101] v: [0.6453] t: [0.6652] ivt: [0.0790]\n",
      "Batch 328/1970 | Losses => i: [0.5977] v: [0.6425] t: [0.6613] ivt: [0.0717]\n",
      "Batch 329/1970 | Losses => i: [0.6301] v: [0.6611] t: [0.6709] ivt: [0.0717]\n",
      "Batch 330/1970 | Losses => i: [0.6294] v: [0.6508] t: [0.6679] ivt: [0.0813]\n",
      "Batch 331/1970 | Losses => i: [0.6138] v: [0.6494] t: [0.6589] ivt: [0.0787]\n",
      "Batch 332/1970 | Losses => i: [0.6267] v: [0.6515] t: [0.6710] ivt: [0.0675]\n",
      "Batch 333/1970 | Losses => i: [0.6395] v: [0.6566] t: [0.6696] ivt: [0.0766]\n",
      "Batch 334/1970 | Losses => i: [0.5930] v: [0.6468] t: [0.6662] ivt: [0.0746]\n",
      "Batch 335/1970 | Losses => i: [0.5982] v: [0.6376] t: [0.6576] ivt: [0.0769]\n",
      "Batch 336/1970 | Losses => i: [0.6063] v: [0.6487] t: [0.6655] ivt: [0.0776]\n",
      "Batch 337/1970 | Losses => i: [0.6078] v: [0.6493] t: [0.6618] ivt: [0.0738]\n",
      "Batch 338/1970 | Losses => i: [0.6181] v: [0.6560] t: [0.6712] ivt: [0.0736]\n",
      "Batch 339/1970 | Losses => i: [0.6102] v: [0.6418] t: [0.6598] ivt: [0.0727]\n",
      "Batch 340/1970 | Losses => i: [0.5957] v: [0.6508] t: [0.6661] ivt: [0.0737]\n",
      "Batch 341/1970 | Losses => i: [0.6081] v: [0.6453] t: [0.6581] ivt: [0.0685]\n",
      "Batch 342/1970 | Losses => i: [0.5967] v: [0.6528] t: [0.6664] ivt: [0.0714]\n",
      "Batch 343/1970 | Losses => i: [0.6229] v: [0.6471] t: [0.6712] ivt: [0.0684]\n",
      "Batch 344/1970 | Losses => i: [0.6133] v: [0.6438] t: [0.6653] ivt: [0.0735]\n",
      "Batch 345/1970 | Losses => i: [0.6029] v: [0.6444] t: [0.6620] ivt: [0.0730]\n",
      "Batch 346/1970 | Losses => i: [0.6129] v: [0.6425] t: [0.6563] ivt: [0.0689]\n",
      "Batch 347/1970 | Losses => i: [0.6022] v: [0.6405] t: [0.6551] ivt: [0.0718]\n",
      "Batch 348/1970 | Losses => i: [0.6048] v: [0.6462] t: [0.6623] ivt: [0.0727]\n",
      "Batch 349/1970 | Losses => i: [0.6100] v: [0.6400] t: [0.6555] ivt: [0.0748]\n",
      "Batch 350/1970 | Losses => i: [0.6057] v: [0.6458] t: [0.6630] ivt: [0.0688]\n",
      "Batch 351/1970 | Losses => i: [0.5832] v: [0.6434] t: [0.6578] ivt: [0.0714]\n",
      "Batch 352/1970 | Losses => i: [0.5837] v: [0.6498] t: [0.6660] ivt: [0.0721]\n",
      "Batch 353/1970 | Losses => i: [0.6134] v: [0.6529] t: [0.6624] ivt: [0.0679]\n",
      "Batch 354/1970 | Losses => i: [0.6038] v: [0.6371] t: [0.6593] ivt: [0.0780]\n",
      "Batch 355/1970 | Losses => i: [0.6139] v: [0.6479] t: [0.6628] ivt: [0.0699]\n",
      "Batch 356/1970 | Losses => i: [0.5842] v: [0.6370] t: [0.6571] ivt: [0.0793]\n",
      "Batch 357/1970 | Losses => i: [0.6046] v: [0.6384] t: [0.6597] ivt: [0.0731]\n",
      "Batch 358/1970 | Losses => i: [0.5973] v: [0.6410] t: [0.6562] ivt: [0.0686]\n",
      "Batch 359/1970 | Losses => i: [0.6008] v: [0.6450] t: [0.6586] ivt: [0.0675]\n",
      "Batch 360/1970 | Losses => i: [0.5914] v: [0.6285] t: [0.6494] ivt: [0.0757]\n",
      "Batch 361/1970 | Losses => i: [0.6138] v: [0.6460] t: [0.6590] ivt: [0.0681]\n",
      "Batch 362/1970 | Losses => i: [0.5961] v: [0.6355] t: [0.6537] ivt: [0.0723]\n",
      "Batch 363/1970 | Losses => i: [0.5890] v: [0.6484] t: [0.6634] ivt: [0.0677]\n",
      "Batch 364/1970 | Losses => i: [0.5973] v: [0.6407] t: [0.6546] ivt: [0.0685]\n",
      "Batch 365/1970 | Losses => i: [0.6025] v: [0.6362] t: [0.6512] ivt: [0.0713]\n",
      "Batch 366/1970 | Losses => i: [0.5885] v: [0.6378] t: [0.6539] ivt: [0.0701]\n",
      "Batch 367/1970 | Losses => i: [0.5931] v: [0.6447] t: [0.6551] ivt: [0.0722]\n",
      "Batch 368/1970 | Losses => i: [0.5863] v: [0.6389] t: [0.6529] ivt: [0.0717]\n",
      "Batch 369/1970 | Losses => i: [0.5834] v: [0.6334] t: [0.6508] ivt: [0.0694]\n",
      "Batch 370/1970 | Losses => i: [0.6107] v: [0.6426] t: [0.6632] ivt: [0.0746]\n",
      "Batch 371/1970 | Losses => i: [0.5867] v: [0.6448] t: [0.6604] ivt: [0.0645]\n",
      "Batch 372/1970 | Losses => i: [0.5938] v: [0.6351] t: [0.6533] ivt: [0.0683]\n",
      "Batch 373/1970 | Losses => i: [0.5934] v: [0.6340] t: [0.6501] ivt: [0.0747]\n",
      "Batch 374/1970 | Losses => i: [0.5980] v: [0.6374] t: [0.6535] ivt: [0.0710]\n",
      "Batch 375/1970 | Losses => i: [0.6106] v: [0.6420] t: [0.6596] ivt: [0.0663]\n",
      "Batch 376/1970 | Losses => i: [0.5921] v: [0.6363] t: [0.6520] ivt: [0.0752]\n",
      "Batch 377/1970 | Losses => i: [0.5890] v: [0.6431] t: [0.6543] ivt: [0.0735]\n",
      "Batch 378/1970 | Losses => i: [0.5894] v: [0.6319] t: [0.6547] ivt: [0.0677]\n",
      "Batch 379/1970 | Losses => i: [0.5857] v: [0.6375] t: [0.6548] ivt: [0.0738]\n",
      "Batch 380/1970 | Losses => i: [0.5980] v: [0.6358] t: [0.6528] ivt: [0.0698]\n",
      "Batch 381/1970 | Losses => i: [0.5910] v: [0.6297] t: [0.6518] ivt: [0.0732]\n",
      "Batch 382/1970 | Losses => i: [0.6020] v: [0.6319] t: [0.6537] ivt: [0.0678]\n",
      "Batch 383/1970 | Losses => i: [0.5893] v: [0.6293] t: [0.6486] ivt: [0.0684]\n",
      "Batch 384/1970 | Losses => i: [0.5805] v: [0.6318] t: [0.6502] ivt: [0.0706]\n",
      "Batch 385/1970 | Losses => i: [0.5865] v: [0.6275] t: [0.6532] ivt: [0.0739]\n",
      "Batch 386/1970 | Losses => i: [0.5886] v: [0.6273] t: [0.6474] ivt: [0.0658]\n",
      "Batch 387/1970 | Losses => i: [0.5906] v: [0.6287] t: [0.6485] ivt: [0.0616]\n",
      "Batch 388/1970 | Losses => i: [0.5904] v: [0.6271] t: [0.6466] ivt: [0.0671]\n",
      "Batch 389/1970 | Losses => i: [0.6010] v: [0.6476] t: [0.6562] ivt: [0.0658]\n",
      "Batch 390/1970 | Losses => i: [0.5864] v: [0.6375] t: [0.6538] ivt: [0.0661]\n",
      "Batch 391/1970 | Losses => i: [0.5667] v: [0.6299] t: [0.6466] ivt: [0.0682]\n",
      "Batch 392/1970 | Losses => i: [0.6005] v: [0.6301] t: [0.6487] ivt: [0.0629]\n",
      "Batch 393/1970 | Losses => i: [0.6042] v: [0.6394] t: [0.6524] ivt: [0.0665]\n",
      "Batch 394/1970 | Losses => i: [0.5931] v: [0.6284] t: [0.6482] ivt: [0.0642]\n",
      "Batch 395/1970 | Losses => i: [0.5789] v: [0.6322] t: [0.6458] ivt: [0.0698]\n",
      "Batch 396/1970 | Losses => i: [0.5619] v: [0.6259] t: [0.6455] ivt: [0.0740]\n",
      "Batch 397/1970 | Losses => i: [0.5788] v: [0.6259] t: [0.6470] ivt: [0.0713]\n",
      "Batch 398/1970 | Losses => i: [0.5747] v: [0.6326] t: [0.6484] ivt: [0.0631]\n",
      "Batch 399/1970 | Losses => i: [0.5601] v: [0.6247] t: [0.6486] ivt: [0.0642]\n",
      "Batch 400/1970 | Losses => i: [0.5755] v: [0.6142] t: [0.6396] ivt: [0.0695]\n",
      "Batch 401/1970 | Losses => i: [0.5821] v: [0.6252] t: [0.6450] ivt: [0.0685]\n",
      "Batch 402/1970 | Losses => i: [0.5856] v: [0.6175] t: [0.6493] ivt: [0.0692]\n",
      "Batch 403/1970 | Losses => i: [0.5650] v: [0.6249] t: [0.6433] ivt: [0.0672]\n",
      "Batch 404/1970 | Losses => i: [0.5888] v: [0.6336] t: [0.6485] ivt: [0.0658]\n",
      "Batch 405/1970 | Losses => i: [0.5835] v: [0.6293] t: [0.6440] ivt: [0.0659]\n",
      "Batch 406/1970 | Losses => i: [0.5655] v: [0.6293] t: [0.6389] ivt: [0.0670]\n",
      "Batch 407/1970 | Losses => i: [0.5898] v: [0.6310] t: [0.6522] ivt: [0.0726]\n",
      "Batch 408/1970 | Losses => i: [0.5886] v: [0.6334] t: [0.6472] ivt: [0.0670]\n",
      "Batch 409/1970 | Losses => i: [0.5722] v: [0.6302] t: [0.6377] ivt: [0.0754]\n",
      "Batch 410/1970 | Losses => i: [0.5814] v: [0.6297] t: [0.6451] ivt: [0.0656]\n",
      "Batch 411/1970 | Losses => i: [0.5961] v: [0.6366] t: [0.6549] ivt: [0.0607]\n",
      "Batch 412/1970 | Losses => i: [0.5730] v: [0.6220] t: [0.6389] ivt: [0.0679]\n",
      "Batch 413/1970 | Losses => i: [0.5797] v: [0.6205] t: [0.6386] ivt: [0.0642]\n",
      "Batch 414/1970 | Losses => i: [0.5767] v: [0.6294] t: [0.6451] ivt: [0.0639]\n",
      "Batch 415/1970 | Losses => i: [0.5763] v: [0.6181] t: [0.6378] ivt: [0.0680]\n",
      "Batch 416/1970 | Losses => i: [0.5851] v: [0.6256] t: [0.6379] ivt: [0.0684]\n",
      "Batch 417/1970 | Losses => i: [0.5803] v: [0.6243] t: [0.6394] ivt: [0.0646]\n",
      "Batch 418/1970 | Losses => i: [0.5742] v: [0.6284] t: [0.6407] ivt: [0.0669]\n",
      "Batch 419/1970 | Losses => i: [0.5792] v: [0.6292] t: [0.6434] ivt: [0.0635]\n",
      "Batch 420/1970 | Losses => i: [0.5725] v: [0.6307] t: [0.6475] ivt: [0.0619]\n",
      "Batch 421/1970 | Losses => i: [0.5717] v: [0.6212] t: [0.6405] ivt: [0.0633]\n",
      "Batch 422/1970 | Losses => i: [0.5742] v: [0.6201] t: [0.6387] ivt: [0.0672]\n",
      "Batch 423/1970 | Losses => i: [0.5774] v: [0.6201] t: [0.6392] ivt: [0.0668]\n",
      "Batch 424/1970 | Losses => i: [0.5774] v: [0.6213] t: [0.6389] ivt: [0.0682]\n",
      "Batch 425/1970 | Losses => i: [0.5705] v: [0.6196] t: [0.6388] ivt: [0.0624]\n",
      "Batch 426/1970 | Losses => i: [0.5706] v: [0.6231] t: [0.6426] ivt: [0.0612]\n",
      "Batch 427/1970 | Losses => i: [0.5886] v: [0.6237] t: [0.6365] ivt: [0.0705]\n",
      "Batch 428/1970 | Losses => i: [0.5761] v: [0.6251] t: [0.6387] ivt: [0.0611]\n",
      "Batch 429/1970 | Losses => i: [0.5702] v: [0.6182] t: [0.6387] ivt: [0.0621]\n",
      "Batch 430/1970 | Losses => i: [0.5715] v: [0.6103] t: [0.6382] ivt: [0.0617]\n",
      "Batch 431/1970 | Losses => i: [0.5719] v: [0.6209] t: [0.6351] ivt: [0.0637]\n",
      "Batch 432/1970 | Losses => i: [0.5771] v: [0.6230] t: [0.6382] ivt: [0.0589]\n",
      "Batch 433/1970 | Losses => i: [0.5716] v: [0.6165] t: [0.6428] ivt: [0.0609]\n",
      "Batch 434/1970 | Losses => i: [0.5844] v: [0.6250] t: [0.6384] ivt: [0.0614]\n",
      "Batch 435/1970 | Losses => i: [0.5795] v: [0.6260] t: [0.6410] ivt: [0.0568]\n",
      "Batch 436/1970 | Losses => i: [0.5679] v: [0.6224] t: [0.6401] ivt: [0.0703]\n",
      "Batch 437/1970 | Losses => i: [0.5656] v: [0.6147] t: [0.6401] ivt: [0.0572]\n",
      "Batch 438/1970 | Losses => i: [0.5647] v: [0.6196] t: [0.6382] ivt: [0.0595]\n",
      "Batch 439/1970 | Losses => i: [0.5882] v: [0.6299] t: [0.6429] ivt: [0.0565]\n",
      "Batch 440/1970 | Losses => i: [0.5817] v: [0.6225] t: [0.6410] ivt: [0.0699]\n",
      "Batch 441/1970 | Losses => i: [0.5730] v: [0.6195] t: [0.6366] ivt: [0.0608]\n",
      "Batch 442/1970 | Losses => i: [0.5673] v: [0.6112] t: [0.6276] ivt: [0.0714]\n",
      "Batch 443/1970 | Losses => i: [0.5806] v: [0.6245] t: [0.6406] ivt: [0.0571]\n",
      "Batch 444/1970 | Losses => i: [0.5637] v: [0.6234] t: [0.6339] ivt: [0.0669]\n",
      "Batch 445/1970 | Losses => i: [0.5897] v: [0.6162] t: [0.6347] ivt: [0.0621]\n",
      "Batch 446/1970 | Losses => i: [0.5727] v: [0.6235] t: [0.6356] ivt: [0.0625]\n",
      "Batch 447/1970 | Losses => i: [0.5716] v: [0.6269] t: [0.6395] ivt: [0.0570]\n",
      "Batch 448/1970 | Losses => i: [0.5738] v: [0.6163] t: [0.6412] ivt: [0.0608]\n",
      "Batch 449/1970 | Losses => i: [0.5736] v: [0.6165] t: [0.6347] ivt: [0.0660]\n",
      "Batch 450/1970 | Losses => i: [0.5751] v: [0.6122] t: [0.6282] ivt: [0.0683]\n",
      "Batch 451/1970 | Losses => i: [0.5626] v: [0.6126] t: [0.6316] ivt: [0.0613]\n",
      "Batch 452/1970 | Losses => i: [0.5662] v: [0.6180] t: [0.6338] ivt: [0.0633]\n",
      "Batch 453/1970 | Losses => i: [0.5767] v: [0.6210] t: [0.6281] ivt: [0.0701]\n",
      "Batch 454/1970 | Losses => i: [0.5645] v: [0.6120] t: [0.6330] ivt: [0.0526]\n",
      "Batch 455/1970 | Losses => i: [0.5734] v: [0.6168] t: [0.6312] ivt: [0.0598]\n",
      "Batch 456/1970 | Losses => i: [0.5746] v: [0.6162] t: [0.6325] ivt: [0.0613]\n",
      "Batch 457/1970 | Losses => i: [0.5813] v: [0.6178] t: [0.6332] ivt: [0.0591]\n",
      "Batch 458/1970 | Losses => i: [0.5720] v: [0.6155] t: [0.6363] ivt: [0.0565]\n",
      "Batch 459/1970 | Losses => i: [0.5670] v: [0.6183] t: [0.6303] ivt: [0.0664]\n",
      "Batch 460/1970 | Losses => i: [0.5641] v: [0.6137] t: [0.6306] ivt: [0.0609]\n",
      "Batch 461/1970 | Losses => i: [0.5804] v: [0.6162] t: [0.6335] ivt: [0.0628]\n",
      "Batch 462/1970 | Losses => i: [0.5667] v: [0.6101] t: [0.6323] ivt: [0.0561]\n",
      "Batch 463/1970 | Losses => i: [0.5772] v: [0.6191] t: [0.6347] ivt: [0.0590]\n",
      "Batch 464/1970 | Losses => i: [0.5818] v: [0.6161] t: [0.6313] ivt: [0.0642]\n",
      "Batch 465/1970 | Losses => i: [0.5692] v: [0.6148] t: [0.6334] ivt: [0.0589]\n",
      "Batch 466/1970 | Losses => i: [0.5816] v: [0.6156] t: [0.6292] ivt: [0.0641]\n",
      "Batch 467/1970 | Losses => i: [0.5683] v: [0.6082] t: [0.6248] ivt: [0.0635]\n",
      "Batch 468/1970 | Losses => i: [0.5600] v: [0.6155] t: [0.6320] ivt: [0.0590]\n",
      "Batch 469/1970 | Losses => i: [0.5656] v: [0.6114] t: [0.6351] ivt: [0.0591]\n",
      "Batch 470/1970 | Losses => i: [0.5623] v: [0.6086] t: [0.6257] ivt: [0.0587]\n",
      "Batch 471/1970 | Losses => i: [0.5660] v: [0.6032] t: [0.6261] ivt: [0.0595]\n",
      "Batch 472/1970 | Losses => i: [0.5612] v: [0.6039] t: [0.6258] ivt: [0.0581]\n",
      "Batch 473/1970 | Losses => i: [0.5631] v: [0.6145] t: [0.6316] ivt: [0.0574]\n",
      "Batch 474/1970 | Losses => i: [0.5551] v: [0.6153] t: [0.6275] ivt: [0.0582]\n",
      "Batch 475/1970 | Losses => i: [0.5594] v: [0.6089] t: [0.6246] ivt: [0.0625]\n",
      "Batch 476/1970 | Losses => i: [0.5751] v: [0.6189] t: [0.6318] ivt: [0.0585]\n",
      "Batch 477/1970 | Losses => i: [0.5734] v: [0.6153] t: [0.6335] ivt: [0.0562]\n",
      "Batch 478/1970 | Losses => i: [0.5644] v: [0.6131] t: [0.6327] ivt: [0.0605]\n",
      "Batch 479/1970 | Losses => i: [0.5660] v: [0.6079] t: [0.6271] ivt: [0.0557]\n",
      "Batch 480/1970 | Losses => i: [0.5550] v: [0.6119] t: [0.6355] ivt: [0.0646]\n",
      "Batch 481/1970 | Losses => i: [0.5642] v: [0.6084] t: [0.6227] ivt: [0.0578]\n",
      "Batch 482/1970 | Losses => i: [0.5580] v: [0.6107] t: [0.6260] ivt: [0.0581]\n",
      "Batch 483/1970 | Losses => i: [0.5572] v: [0.6115] t: [0.6265] ivt: [0.0607]\n",
      "Batch 484/1970 | Losses => i: [0.5580] v: [0.6070] t: [0.6208] ivt: [0.0642]\n",
      "Batch 485/1970 | Losses => i: [0.5605] v: [0.6094] t: [0.6266] ivt: [0.0575]\n",
      "Batch 486/1970 | Losses => i: [0.5721] v: [0.6088] t: [0.6263] ivt: [0.0615]\n",
      "Batch 487/1970 | Losses => i: [0.5713] v: [0.6089] t: [0.6198] ivt: [0.0611]\n",
      "Batch 488/1970 | Losses => i: [0.5560] v: [0.6070] t: [0.6244] ivt: [0.0586]\n",
      "Batch 489/1970 | Losses => i: [0.5621] v: [0.6102] t: [0.6227] ivt: [0.0615]\n",
      "Batch 490/1970 | Losses => i: [0.5583] v: [0.6130] t: [0.6207] ivt: [0.0628]\n",
      "Batch 491/1970 | Losses => i: [0.5476] v: [0.6098] t: [0.6271] ivt: [0.0631]\n",
      "Batch 492/1970 | Losses => i: [0.5555] v: [0.6022] t: [0.6198] ivt: [0.0712]\n",
      "Batch 493/1970 | Losses => i: [0.5707] v: [0.6195] t: [0.6278] ivt: [0.0568]\n",
      "Batch 494/1970 | Losses => i: [0.5505] v: [0.6045] t: [0.6218] ivt: [0.0535]\n",
      "Batch 495/1970 | Losses => i: [0.5719] v: [0.6102] t: [0.6294] ivt: [0.0537]\n",
      "Batch 496/1970 | Losses => i: [0.5556] v: [0.6030] t: [0.6201] ivt: [0.0610]\n",
      "Batch 497/1970 | Losses => i: [0.5692] v: [0.6120] t: [0.6298] ivt: [0.0521]\n",
      "Batch 498/1970 | Losses => i: [0.5611] v: [0.6116] t: [0.6209] ivt: [0.0662]\n",
      "Batch 499/1970 | Losses => i: [0.5595] v: [0.6011] t: [0.6231] ivt: [0.0623]\n",
      "Batch 500/1970 | Losses => i: [0.5708] v: [0.6086] t: [0.6253] ivt: [0.0624]\n",
      "Batch 501/1970 | Losses => i: [0.5586] v: [0.6047] t: [0.6182] ivt: [0.0621]\n",
      "Batch 502/1970 | Losses => i: [0.5771] v: [0.6058] t: [0.6193] ivt: [0.0605]\n",
      "Batch 503/1970 | Losses => i: [0.5473] v: [0.6021] t: [0.6160] ivt: [0.0596]\n",
      "Batch 504/1970 | Losses => i: [0.5625] v: [0.6052] t: [0.6257] ivt: [0.0521]\n",
      "Batch 505/1970 | Losses => i: [0.5630] v: [0.6119] t: [0.6212] ivt: [0.0615]\n",
      "Batch 506/1970 | Losses => i: [0.5616] v: [0.6094] t: [0.6178] ivt: [0.0567]\n",
      "Batch 507/1970 | Losses => i: [0.5473] v: [0.6017] t: [0.6160] ivt: [0.0596]\n",
      "Batch 508/1970 | Losses => i: [0.5603] v: [0.6103] t: [0.6256] ivt: [0.0543]\n",
      "Batch 509/1970 | Losses => i: [0.5716] v: [0.6071] t: [0.6228] ivt: [0.0628]\n",
      "Batch 510/1970 | Losses => i: [0.5619] v: [0.6094] t: [0.6202] ivt: [0.0578]\n",
      "Batch 511/1970 | Losses => i: [0.5644] v: [0.6102] t: [0.6212] ivt: [0.0602]\n",
      "Batch 512/1970 | Losses => i: [0.5569] v: [0.6002] t: [0.6145] ivt: [0.0588]\n",
      "Batch 513/1970 | Losses => i: [0.5593] v: [0.6112] t: [0.6172] ivt: [0.0588]\n",
      "Batch 514/1970 | Losses => i: [0.5450] v: [0.6028] t: [0.6192] ivt: [0.0537]\n",
      "Batch 515/1970 | Losses => i: [0.5656] v: [0.5997] t: [0.6182] ivt: [0.0587]\n",
      "Batch 516/1970 | Losses => i: [0.5557] v: [0.5931] t: [0.6126] ivt: [0.0591]\n",
      "Batch 517/1970 | Losses => i: [0.5432] v: [0.5954] t: [0.6127] ivt: [0.0620]\n",
      "Batch 518/1970 | Losses => i: [0.5517] v: [0.5992] t: [0.6152] ivt: [0.0575]\n",
      "Batch 519/1970 | Losses => i: [0.5639] v: [0.6040] t: [0.6165] ivt: [0.0626]\n",
      "Batch 520/1970 | Losses => i: [0.5500] v: [0.6056] t: [0.6163] ivt: [0.0582]\n",
      "Batch 521/1970 | Losses => i: [0.5657] v: [0.5948] t: [0.6120] ivt: [0.0587]\n",
      "Batch 522/1970 | Losses => i: [0.5530] v: [0.6020] t: [0.6207] ivt: [0.0547]\n",
      "Batch 523/1970 | Losses => i: [0.5422] v: [0.5925] t: [0.6105] ivt: [0.0590]\n",
      "Batch 524/1970 | Losses => i: [0.5554] v: [0.5969] t: [0.6144] ivt: [0.0560]\n",
      "Batch 525/1970 | Losses => i: [0.5674] v: [0.6044] t: [0.6171] ivt: [0.0663]\n",
      "Batch 526/1970 | Losses => i: [0.5565] v: [0.5965] t: [0.6162] ivt: [0.0554]\n",
      "Batch 527/1970 | Losses => i: [0.5612] v: [0.6087] t: [0.6159] ivt: [0.0582]\n",
      "Batch 528/1970 | Losses => i: [0.5555] v: [0.5966] t: [0.6172] ivt: [0.0601]\n",
      "Batch 529/1970 | Losses => i: [0.5602] v: [0.6011] t: [0.6180] ivt: [0.0556]\n",
      "Batch 530/1970 | Losses => i: [0.5587] v: [0.6031] t: [0.6166] ivt: [0.0548]\n",
      "Batch 531/1970 | Losses => i: [0.5725] v: [0.6122] t: [0.6187] ivt: [0.0542]\n",
      "Batch 532/1970 | Losses => i: [0.5407] v: [0.5974] t: [0.6178] ivt: [0.0517]\n",
      "Batch 533/1970 | Losses => i: [0.5655] v: [0.5999] t: [0.6176] ivt: [0.0575]\n",
      "Batch 534/1970 | Losses => i: [0.5690] v: [0.6066] t: [0.6139] ivt: [0.0631]\n",
      "Batch 535/1970 | Losses => i: [0.5516] v: [0.5949] t: [0.6164] ivt: [0.0502]\n",
      "Batch 536/1970 | Losses => i: [0.5799] v: [0.6077] t: [0.6197] ivt: [0.0614]\n",
      "Batch 537/1970 | Losses => i: [0.5408] v: [0.5935] t: [0.6096] ivt: [0.0535]\n",
      "Batch 538/1970 | Losses => i: [0.5526] v: [0.5984] t: [0.6114] ivt: [0.0626]\n",
      "Batch 539/1970 | Losses => i: [0.5395] v: [0.5950] t: [0.6077] ivt: [0.0570]\n",
      "Batch 540/1970 | Losses => i: [0.5325] v: [0.5895] t: [0.6106] ivt: [0.0569]\n",
      "Batch 541/1970 | Losses => i: [0.5630] v: [0.5952] t: [0.6083] ivt: [0.0553]\n",
      "Batch 542/1970 | Losses => i: [0.5554] v: [0.5871] t: [0.6092] ivt: [0.0579]\n",
      "Batch 543/1970 | Losses => i: [0.5710] v: [0.6040] t: [0.6097] ivt: [0.0564]\n",
      "Batch 544/1970 | Losses => i: [0.5409] v: [0.5975] t: [0.6116] ivt: [0.0637]\n",
      "Batch 545/1970 | Losses => i: [0.5473] v: [0.5878] t: [0.6101] ivt: [0.0543]\n",
      "Batch 546/1970 | Losses => i: [0.5342] v: [0.5835] t: [0.6064] ivt: [0.0563]\n",
      "Batch 547/1970 | Losses => i: [0.5635] v: [0.5947] t: [0.6133] ivt: [0.0556]\n",
      "Batch 548/1970 | Losses => i: [0.5629] v: [0.5996] t: [0.6163] ivt: [0.0556]\n",
      "Batch 549/1970 | Losses => i: [0.5616] v: [0.5971] t: [0.6110] ivt: [0.0546]\n",
      "Batch 550/1970 | Losses => i: [0.5516] v: [0.5915] t: [0.6093] ivt: [0.0588]\n",
      "Batch 551/1970 | Losses => i: [0.5419] v: [0.6073] t: [0.6131] ivt: [0.0512]\n",
      "Batch 552/1970 | Losses => i: [0.5422] v: [0.5976] t: [0.6079] ivt: [0.0548]\n",
      "Batch 553/1970 | Losses => i: [0.5465] v: [0.5885] t: [0.6062] ivt: [0.0636]\n",
      "Batch 554/1970 | Losses => i: [0.5712] v: [0.5932] t: [0.6095] ivt: [0.0591]\n",
      "Batch 555/1970 | Losses => i: [0.5487] v: [0.5888] t: [0.6057] ivt: [0.0590]\n",
      "Batch 556/1970 | Losses => i: [0.5486] v: [0.5973] t: [0.6080] ivt: [0.0636]\n",
      "Batch 557/1970 | Losses => i: [0.5519] v: [0.5948] t: [0.6161] ivt: [0.0538]\n",
      "Batch 558/1970 | Losses => i: [0.5348] v: [0.5837] t: [0.6082] ivt: [0.0616]\n",
      "Batch 559/1970 | Losses => i: [0.5473] v: [0.5947] t: [0.6125] ivt: [0.0488]\n",
      "Batch 560/1970 | Losses => i: [0.5516] v: [0.5887] t: [0.6066] ivt: [0.0653]\n",
      "Batch 561/1970 | Losses => i: [0.5537] v: [0.5958] t: [0.6113] ivt: [0.0537]\n",
      "Batch 562/1970 | Losses => i: [0.5624] v: [0.5918] t: [0.6069] ivt: [0.0583]\n",
      "Batch 563/1970 | Losses => i: [0.5586] v: [0.5917] t: [0.6101] ivt: [0.0516]\n",
      "Batch 564/1970 | Losses => i: [0.5587] v: [0.5921] t: [0.6085] ivt: [0.0586]\n",
      "Batch 565/1970 | Losses => i: [0.5443] v: [0.5975] t: [0.6078] ivt: [0.0517]\n",
      "Batch 566/1970 | Losses => i: [0.5605] v: [0.5977] t: [0.6125] ivt: [0.0516]\n",
      "Batch 567/1970 | Losses => i: [0.5537] v: [0.5950] t: [0.6070] ivt: [0.0618]\n",
      "Batch 568/1970 | Losses => i: [0.5375] v: [0.5880] t: [0.6046] ivt: [0.0554]\n",
      "Batch 569/1970 | Losses => i: [0.5403] v: [0.5959] t: [0.6045] ivt: [0.0602]\n",
      "Batch 570/1970 | Losses => i: [0.5385] v: [0.5875] t: [0.6056] ivt: [0.0541]\n",
      "Batch 571/1970 | Losses => i: [0.5408] v: [0.5827] t: [0.6004] ivt: [0.0622]\n",
      "Batch 572/1970 | Losses => i: [0.5548] v: [0.5833] t: [0.6064] ivt: [0.0601]\n",
      "Batch 573/1970 | Losses => i: [0.5516] v: [0.6023] t: [0.6164] ivt: [0.0683]\n",
      "Batch 574/1970 | Losses => i: [0.5456] v: [0.5897] t: [0.6028] ivt: [0.0539]\n",
      "Batch 575/1970 | Losses => i: [0.5474] v: [0.5893] t: [0.6074] ivt: [0.0534]\n",
      "Batch 576/1970 | Losses => i: [0.5492] v: [0.5921] t: [0.6072] ivt: [0.0510]\n",
      "Batch 577/1970 | Losses => i: [0.5454] v: [0.5909] t: [0.6078] ivt: [0.0542]\n",
      "Batch 578/1970 | Losses => i: [0.5492] v: [0.5880] t: [0.6088] ivt: [0.0603]\n",
      "Batch 579/1970 | Losses => i: [0.5484] v: [0.6001] t: [0.6180] ivt: [0.0487]\n",
      "Batch 580/1970 | Losses => i: [0.5353] v: [0.5837] t: [0.6052] ivt: [0.0512]\n",
      "Batch 581/1970 | Losses => i: [0.5464] v: [0.5884] t: [0.6052] ivt: [0.0589]\n",
      "Batch 582/1970 | Losses => i: [0.5378] v: [0.5874] t: [0.6037] ivt: [0.0615]\n",
      "Batch 583/1970 | Losses => i: [0.5427] v: [0.5825] t: [0.6031] ivt: [0.0561]\n",
      "Batch 584/1970 | Losses => i: [0.5619] v: [0.5862] t: [0.6032] ivt: [0.0537]\n",
      "Batch 585/1970 | Losses => i: [0.5341] v: [0.5834] t: [0.6004] ivt: [0.0586]\n",
      "Batch 586/1970 | Losses => i: [0.5461] v: [0.5858] t: [0.5970] ivt: [0.0585]\n",
      "Batch 587/1970 | Losses => i: [0.5534] v: [0.5914] t: [0.6063] ivt: [0.0587]\n",
      "Batch 588/1970 | Losses => i: [0.5434] v: [0.5746] t: [0.5978] ivt: [0.0559]\n",
      "Batch 589/1970 | Losses => i: [0.5555] v: [0.5915] t: [0.6044] ivt: [0.0569]\n",
      "Batch 590/1970 | Losses => i: [0.5447] v: [0.5875] t: [0.6018] ivt: [0.0541]\n",
      "Batch 591/1970 | Losses => i: [0.5387] v: [0.5899] t: [0.6022] ivt: [0.0541]\n",
      "Batch 592/1970 | Losses => i: [0.5402] v: [0.5825] t: [0.6023] ivt: [0.0554]\n",
      "Batch 593/1970 | Losses => i: [0.5392] v: [0.5876] t: [0.6029] ivt: [0.0605]\n",
      "Batch 594/1970 | Losses => i: [0.5408] v: [0.5904] t: [0.6048] ivt: [0.0522]\n",
      "Batch 595/1970 | Losses => i: [0.5447] v: [0.5832] t: [0.6022] ivt: [0.0533]\n",
      "Batch 596/1970 | Losses => i: [0.5378] v: [0.6019] t: [0.6022] ivt: [0.0566]\n",
      "Batch 597/1970 | Losses => i: [0.5341] v: [0.5790] t: [0.6003] ivt: [0.0544]\n",
      "Batch 598/1970 | Losses => i: [0.5507] v: [0.5930] t: [0.6071] ivt: [0.0472]\n",
      "Batch 599/1970 | Losses => i: [0.5230] v: [0.5851] t: [0.5986] ivt: [0.0539]\n",
      "Batch 600/1970 | Losses => i: [0.5447] v: [0.5916] t: [0.6094] ivt: [0.0581]\n",
      "Batch 601/1970 | Losses => i: [0.5312] v: [0.5829] t: [0.5974] ivt: [0.0517]\n",
      "Batch 602/1970 | Losses => i: [0.5461] v: [0.5811] t: [0.5995] ivt: [0.0579]\n",
      "Batch 603/1970 | Losses => i: [0.5526] v: [0.5803] t: [0.5999] ivt: [0.0579]\n",
      "Batch 604/1970 | Losses => i: [0.5508] v: [0.5889] t: [0.5998] ivt: [0.0528]\n",
      "Batch 605/1970 | Losses => i: [0.5424] v: [0.5838] t: [0.5965] ivt: [0.0611]\n",
      "Batch 606/1970 | Losses => i: [0.5113] v: [0.5768] t: [0.5944] ivt: [0.0574]\n",
      "Batch 607/1970 | Losses => i: [0.5258] v: [0.5757] t: [0.5962] ivt: [0.0590]\n",
      "Batch 608/1970 | Losses => i: [0.5297] v: [0.5761] t: [0.5970] ivt: [0.0575]\n",
      "Batch 609/1970 | Losses => i: [0.5361] v: [0.5828] t: [0.5969] ivt: [0.0496]\n",
      "Batch 610/1970 | Losses => i: [0.5338] v: [0.5714] t: [0.5943] ivt: [0.0586]\n",
      "Batch 611/1970 | Losses => i: [0.5410] v: [0.5808] t: [0.5985] ivt: [0.0604]\n",
      "Batch 612/1970 | Losses => i: [0.5567] v: [0.5783] t: [0.5962] ivt: [0.0553]\n",
      "Batch 613/1970 | Losses => i: [0.5362] v: [0.5732] t: [0.5963] ivt: [0.0519]\n",
      "Batch 614/1970 | Losses => i: [0.5422] v: [0.5849] t: [0.5986] ivt: [0.0607]\n",
      "Batch 615/1970 | Losses => i: [0.5361] v: [0.5866] t: [0.6024] ivt: [0.0572]\n",
      "Batch 616/1970 | Losses => i: [0.5470] v: [0.5855] t: [0.5932] ivt: [0.0544]\n",
      "Batch 617/1970 | Losses => i: [0.5457] v: [0.5851] t: [0.5950] ivt: [0.0490]\n",
      "Batch 618/1970 | Losses => i: [0.5400] v: [0.5825] t: [0.5981] ivt: [0.0497]\n",
      "Batch 619/1970 | Losses => i: [0.5306] v: [0.5772] t: [0.5956] ivt: [0.0590]\n",
      "Batch 620/1970 | Losses => i: [0.5326] v: [0.5789] t: [0.6029] ivt: [0.0450]\n",
      "Batch 621/1970 | Losses => i: [0.5366] v: [0.5809] t: [0.5973] ivt: [0.0560]\n",
      "Batch 622/1970 | Losses => i: [0.5351] v: [0.5862] t: [0.5958] ivt: [0.0498]\n",
      "Batch 623/1970 | Losses => i: [0.5196] v: [0.5719] t: [0.5894] ivt: [0.0473]\n",
      "Batch 624/1970 | Losses => i: [0.5506] v: [0.5764] t: [0.5896] ivt: [0.0552]\n",
      "Batch 625/1970 | Losses => i: [0.5368] v: [0.5749] t: [0.5915] ivt: [0.0636]\n",
      "Batch 626/1970 | Losses => i: [0.5366] v: [0.5758] t: [0.5869] ivt: [0.0589]\n",
      "Batch 627/1970 | Losses => i: [0.5297] v: [0.5769] t: [0.5932] ivt: [0.0556]\n",
      "Batch 628/1970 | Losses => i: [0.5420] v: [0.5784] t: [0.5923] ivt: [0.0508]\n",
      "Batch 629/1970 | Losses => i: [0.5337] v: [0.5841] t: [0.5933] ivt: [0.0515]\n",
      "Batch 630/1970 | Losses => i: [0.5401] v: [0.5845] t: [0.5934] ivt: [0.0550]\n",
      "Batch 631/1970 | Losses => i: [0.5304] v: [0.5810] t: [0.5950] ivt: [0.0474]\n",
      "Batch 632/1970 | Losses => i: [0.5331] v: [0.5842] t: [0.5918] ivt: [0.0528]\n",
      "Batch 633/1970 | Losses => i: [0.5427] v: [0.5792] t: [0.5987] ivt: [0.0484]\n",
      "Batch 634/1970 | Losses => i: [0.5131] v: [0.5678] t: [0.5877] ivt: [0.0482]\n",
      "Batch 635/1970 | Losses => i: [0.5475] v: [0.5783] t: [0.5940] ivt: [0.0576]\n",
      "Batch 636/1970 | Losses => i: [0.5321] v: [0.5742] t: [0.5870] ivt: [0.0523]\n",
      "Batch 637/1970 | Losses => i: [0.5283] v: [0.5787] t: [0.5941] ivt: [0.0538]\n",
      "Batch 638/1970 | Losses => i: [0.5373] v: [0.5753] t: [0.5897] ivt: [0.0527]\n",
      "Batch 639/1970 | Losses => i: [0.5202] v: [0.5790] t: [0.5917] ivt: [0.0523]\n",
      "Batch 640/1970 | Losses => i: [0.5332] v: [0.5756] t: [0.5931] ivt: [0.0530]\n",
      "Batch 641/1970 | Losses => i: [0.5212] v: [0.5680] t: [0.5887] ivt: [0.0468]\n",
      "Batch 642/1970 | Losses => i: [0.5250] v: [0.5702] t: [0.5862] ivt: [0.0501]\n",
      "Batch 643/1970 | Losses => i: [0.5104] v: [0.5747] t: [0.5938] ivt: [0.0572]\n",
      "Batch 644/1970 | Losses => i: [0.5119] v: [0.5769] t: [0.5881] ivt: [0.0520]\n",
      "Batch 645/1970 | Losses => i: [0.5327] v: [0.5754] t: [0.5898] ivt: [0.0519]\n",
      "Batch 646/1970 | Losses => i: [0.5314] v: [0.5776] t: [0.5892] ivt: [0.0504]\n",
      "Batch 647/1970 | Losses => i: [0.5291] v: [0.5698] t: [0.5877] ivt: [0.0581]\n",
      "Batch 648/1970 | Losses => i: [0.5253] v: [0.5697] t: [0.5846] ivt: [0.0511]\n",
      "Batch 649/1970 | Losses => i: [0.5271] v: [0.5706] t: [0.5851] ivt: [0.0615]\n",
      "Batch 650/1970 | Losses => i: [0.5252] v: [0.5877] t: [0.5936] ivt: [0.0572]\n",
      "Batch 651/1970 | Losses => i: [0.5296] v: [0.5754] t: [0.5859] ivt: [0.0473]\n",
      "Batch 652/1970 | Losses => i: [0.5347] v: [0.5657] t: [0.5846] ivt: [0.0616]\n",
      "Batch 653/1970 | Losses => i: [0.5287] v: [0.5739] t: [0.5870] ivt: [0.0473]\n",
      "Batch 654/1970 | Losses => i: [0.5283] v: [0.5702] t: [0.5827] ivt: [0.0552]\n",
      "Batch 655/1970 | Losses => i: [0.5238] v: [0.5698] t: [0.5917] ivt: [0.0528]\n",
      "Batch 656/1970 | Losses => i: [0.5246] v: [0.5630] t: [0.5799] ivt: [0.0490]\n",
      "Batch 657/1970 | Losses => i: [0.5193] v: [0.5643] t: [0.5799] ivt: [0.0588]\n",
      "Batch 658/1970 | Losses => i: [0.5354] v: [0.5758] t: [0.5857] ivt: [0.0476]\n",
      "Batch 659/1970 | Losses => i: [0.5279] v: [0.5619] t: [0.5826] ivt: [0.0562]\n",
      "Batch 660/1970 | Losses => i: [0.5119] v: [0.5663] t: [0.5811] ivt: [0.0528]\n",
      "Batch 661/1970 | Losses => i: [0.5150] v: [0.5672] t: [0.5828] ivt: [0.0534]\n",
      "Batch 662/1970 | Losses => i: [0.5202] v: [0.5690] t: [0.5903] ivt: [0.0490]\n",
      "Batch 663/1970 | Losses => i: [0.5259] v: [0.5663] t: [0.5829] ivt: [0.0589]\n",
      "Batch 664/1970 | Losses => i: [0.5282] v: [0.5716] t: [0.5828] ivt: [0.0506]\n",
      "Batch 665/1970 | Losses => i: [0.5326] v: [0.5735] t: [0.5945] ivt: [0.0545]\n",
      "Batch 666/1970 | Losses => i: [0.5277] v: [0.5661] t: [0.5851] ivt: [0.0555]\n",
      "Batch 667/1970 | Losses => i: [0.5247] v: [0.5654] t: [0.5811] ivt: [0.0596]\n",
      "Batch 668/1970 | Losses => i: [0.5013] v: [0.5597] t: [0.5733] ivt: [0.0502]\n",
      "Batch 669/1970 | Losses => i: [0.5239] v: [0.5716] t: [0.5783] ivt: [0.0532]\n",
      "Batch 670/1970 | Losses => i: [0.5296] v: [0.5670] t: [0.5846] ivt: [0.0537]\n",
      "Batch 671/1970 | Losses => i: [0.5297] v: [0.5647] t: [0.5785] ivt: [0.0551]\n",
      "Batch 672/1970 | Losses => i: [0.5264] v: [0.5629] t: [0.5810] ivt: [0.0583]\n",
      "Batch 673/1970 | Losses => i: [0.5132] v: [0.5654] t: [0.5771] ivt: [0.0624]\n",
      "Batch 674/1970 | Losses => i: [0.5176] v: [0.5654] t: [0.5746] ivt: [0.0530]\n",
      "Batch 675/1970 | Losses => i: [0.5327] v: [0.5672] t: [0.5881] ivt: [0.0505]\n",
      "Batch 676/1970 | Losses => i: [0.5134] v: [0.5668] t: [0.5806] ivt: [0.0599]\n",
      "Batch 677/1970 | Losses => i: [0.5358] v: [0.5720] t: [0.5831] ivt: [0.0554]\n",
      "Batch 678/1970 | Losses => i: [0.5093] v: [0.5719] t: [0.5793] ivt: [0.0580]\n",
      "Batch 679/1970 | Losses => i: [0.5093] v: [0.5602] t: [0.5754] ivt: [0.0452]\n",
      "Batch 680/1970 | Losses => i: [0.5359] v: [0.5674] t: [0.5788] ivt: [0.0540]\n",
      "Batch 681/1970 | Losses => i: [0.5195] v: [0.5673] t: [0.5801] ivt: [0.0574]\n",
      "Batch 682/1970 | Losses => i: [0.5125] v: [0.5638] t: [0.5799] ivt: [0.0485]\n",
      "Batch 683/1970 | Losses => i: [0.5141] v: [0.5569] t: [0.5727] ivt: [0.0556]\n",
      "Batch 684/1970 | Losses => i: [0.5198] v: [0.5696] t: [0.5735] ivt: [0.0560]\n",
      "Batch 685/1970 | Losses => i: [0.5458] v: [0.5739] t: [0.5807] ivt: [0.0586]\n",
      "Batch 686/1970 | Losses => i: [0.5124] v: [0.5684] t: [0.5840] ivt: [0.0504]\n",
      "Batch 687/1970 | Losses => i: [0.5162] v: [0.5588] t: [0.5778] ivt: [0.0547]\n",
      "Batch 688/1970 | Losses => i: [0.5297] v: [0.5663] t: [0.5775] ivt: [0.0593]\n",
      "Batch 689/1970 | Losses => i: [0.5096] v: [0.5581] t: [0.5645] ivt: [0.0559]\n",
      "Batch 690/1970 | Losses => i: [0.5060] v: [0.5593] t: [0.5728] ivt: [0.0471]\n",
      "Batch 691/1970 | Losses => i: [0.5253] v: [0.5640] t: [0.5813] ivt: [0.0464]\n",
      "Batch 692/1970 | Losses => i: [0.5118] v: [0.5607] t: [0.5746] ivt: [0.0507]\n",
      "Batch 693/1970 | Losses => i: [0.5202] v: [0.5604] t: [0.5801] ivt: [0.0493]\n",
      "Batch 694/1970 | Losses => i: [0.5236] v: [0.5610] t: [0.5721] ivt: [0.0481]\n",
      "Batch 695/1970 | Losses => i: [0.5134] v: [0.5642] t: [0.5741] ivt: [0.0564]\n",
      "Batch 696/1970 | Losses => i: [0.5118] v: [0.5614] t: [0.5810] ivt: [0.0548]\n",
      "Batch 697/1970 | Losses => i: [0.5259] v: [0.5669] t: [0.5765] ivt: [0.0498]\n",
      "Batch 698/1970 | Losses => i: [0.5377] v: [0.5645] t: [0.5760] ivt: [0.0472]\n",
      "Batch 699/1970 | Losses => i: [0.5124] v: [0.5595] t: [0.5741] ivt: [0.0508]\n",
      "Batch 700/1970 | Losses => i: [0.5132] v: [0.5623] t: [0.5687] ivt: [0.0532]\n",
      "Batch 701/1970 | Losses => i: [0.5172] v: [0.5567] t: [0.5671] ivt: [0.0513]\n",
      "Batch 702/1970 | Losses => i: [0.5177] v: [0.5605] t: [0.5740] ivt: [0.0439]\n",
      "Batch 703/1970 | Losses => i: [0.5313] v: [0.5628] t: [0.5719] ivt: [0.0499]\n",
      "Batch 704/1970 | Losses => i: [0.5058] v: [0.5584] t: [0.5699] ivt: [0.0450]\n",
      "Batch 705/1970 | Losses => i: [0.5204] v: [0.5608] t: [0.5666] ivt: [0.0534]\n",
      "Batch 706/1970 | Losses => i: [0.5290] v: [0.5623] t: [0.5757] ivt: [0.0481]\n",
      "Batch 707/1970 | Losses => i: [0.5039] v: [0.5577] t: [0.5689] ivt: [0.0499]\n",
      "Batch 708/1970 | Losses => i: [0.5087] v: [0.5559] t: [0.5694] ivt: [0.0578]\n",
      "Batch 709/1970 | Losses => i: [0.5289] v: [0.5647] t: [0.5725] ivt: [0.0569]\n",
      "Batch 710/1970 | Losses => i: [0.5176] v: [0.5571] t: [0.5682] ivt: [0.0497]\n",
      "Batch 711/1970 | Losses => i: [0.5291] v: [0.5655] t: [0.5758] ivt: [0.0487]\n",
      "Batch 712/1970 | Losses => i: [0.5105] v: [0.5610] t: [0.5750] ivt: [0.0522]\n",
      "Batch 713/1970 | Losses => i: [0.5120] v: [0.5585] t: [0.5640] ivt: [0.0504]\n",
      "Batch 714/1970 | Losses => i: [0.5204] v: [0.5596] t: [0.5681] ivt: [0.0532]\n",
      "Batch 715/1970 | Losses => i: [0.5218] v: [0.5577] t: [0.5658] ivt: [0.0529]\n",
      "Batch 716/1970 | Losses => i: [0.5157] v: [0.5600] t: [0.5680] ivt: [0.0484]\n",
      "Batch 717/1970 | Losses => i: [0.5273] v: [0.5624] t: [0.5688] ivt: [0.0513]\n",
      "Batch 718/1970 | Losses => i: [0.5195] v: [0.5545] t: [0.5699] ivt: [0.0466]\n",
      "Batch 719/1970 | Losses => i: [0.5252] v: [0.5695] t: [0.5719] ivt: [0.0528]\n",
      "Batch 720/1970 | Losses => i: [0.5133] v: [0.5601] t: [0.5668] ivt: [0.0481]\n",
      "Batch 721/1970 | Losses => i: [0.5113] v: [0.5560] t: [0.5665] ivt: [0.0475]\n",
      "Batch 722/1970 | Losses => i: [0.5119] v: [0.5531] t: [0.5652] ivt: [0.0522]\n",
      "Batch 723/1970 | Losses => i: [0.5110] v: [0.5557] t: [0.5556] ivt: [0.0519]\n",
      "Batch 724/1970 | Losses => i: [0.5292] v: [0.5707] t: [0.5712] ivt: [0.0481]\n",
      "Batch 725/1970 | Losses => i: [0.4997] v: [0.5516] t: [0.5655] ivt: [0.0483]\n",
      "Batch 726/1970 | Losses => i: [0.5181] v: [0.5647] t: [0.5652] ivt: [0.0510]\n",
      "Batch 727/1970 | Losses => i: [0.5096] v: [0.5484] t: [0.5637] ivt: [0.0500]\n",
      "Batch 728/1970 | Losses => i: [0.5235] v: [0.5528] t: [0.5682] ivt: [0.0576]\n",
      "Batch 729/1970 | Losses => i: [0.5140] v: [0.5567] t: [0.5643] ivt: [0.0449]\n",
      "Batch 730/1970 | Losses => i: [0.5188] v: [0.5514] t: [0.5660] ivt: [0.0519]\n",
      "Batch 731/1970 | Losses => i: [0.5051] v: [0.5535] t: [0.5605] ivt: [0.0586]\n",
      "Batch 732/1970 | Losses => i: [0.5131] v: [0.5575] t: [0.5615] ivt: [0.0516]\n",
      "Batch 733/1970 | Losses => i: [0.5209] v: [0.5537] t: [0.5597] ivt: [0.0451]\n",
      "Batch 734/1970 | Losses => i: [0.5216] v: [0.5539] t: [0.5584] ivt: [0.0507]\n",
      "Batch 735/1970 | Losses => i: [0.5218] v: [0.5534] t: [0.5596] ivt: [0.0461]\n",
      "Batch 736/1970 | Losses => i: [0.4978] v: [0.5483] t: [0.5667] ivt: [0.0528]\n",
      "Batch 737/1970 | Losses => i: [0.5316] v: [0.5531] t: [0.5656] ivt: [0.0522]\n",
      "Batch 738/1970 | Losses => i: [0.5020] v: [0.5481] t: [0.5544] ivt: [0.0456]\n",
      "Batch 739/1970 | Losses => i: [0.4972] v: [0.5483] t: [0.5555] ivt: [0.0533]\n",
      "Batch 740/1970 | Losses => i: [0.5211] v: [0.5618] t: [0.5589] ivt: [0.0520]\n",
      "Batch 741/1970 | Losses => i: [0.5167] v: [0.5482] t: [0.5558] ivt: [0.0540]\n",
      "Batch 742/1970 | Losses => i: [0.5115] v: [0.5471] t: [0.5602] ivt: [0.0454]\n",
      "Batch 743/1970 | Losses => i: [0.5090] v: [0.5468] t: [0.5565] ivt: [0.0448]\n",
      "Batch 744/1970 | Losses => i: [0.5040] v: [0.5532] t: [0.5545] ivt: [0.0576]\n",
      "Batch 745/1970 | Losses => i: [0.5155] v: [0.5514] t: [0.5552] ivt: [0.0542]\n",
      "Batch 746/1970 | Losses => i: [0.5194] v: [0.5499] t: [0.5575] ivt: [0.0509]\n",
      "Batch 747/1970 | Losses => i: [0.4955] v: [0.5469] t: [0.5609] ivt: [0.0438]\n",
      "Batch 748/1970 | Losses => i: [0.5073] v: [0.5561] t: [0.5605] ivt: [0.0480]\n",
      "Batch 749/1970 | Losses => i: [0.5131] v: [0.5493] t: [0.5568] ivt: [0.0463]\n",
      "Batch 750/1970 | Losses => i: [0.5237] v: [0.5546] t: [0.5594] ivt: [0.0467]\n",
      "Batch 751/1970 | Losses => i: [0.5041] v: [0.5536] t: [0.5607] ivt: [0.0465]\n",
      "Batch 752/1970 | Losses => i: [0.5127] v: [0.5412] t: [0.5489] ivt: [0.0558]\n",
      "Batch 753/1970 | Losses => i: [0.5002] v: [0.5439] t: [0.5483] ivt: [0.0460]\n",
      "Batch 754/1970 | Losses => i: [0.5247] v: [0.5570] t: [0.5612] ivt: [0.0561]\n",
      "Batch 755/1970 | Losses => i: [0.5174] v: [0.5503] t: [0.5498] ivt: [0.0423]\n",
      "Batch 756/1970 | Losses => i: [0.5108] v: [0.5503] t: [0.5575] ivt: [0.0558]\n",
      "Batch 757/1970 | Losses => i: [0.5098] v: [0.5531] t: [0.5581] ivt: [0.0453]\n",
      "Batch 758/1970 | Losses => i: [0.5110] v: [0.5436] t: [0.5518] ivt: [0.0503]\n",
      "Batch 759/1970 | Losses => i: [0.5122] v: [0.5442] t: [0.5472] ivt: [0.0569]\n",
      "Batch 760/1970 | Losses => i: [0.5129] v: [0.5440] t: [0.5540] ivt: [0.0514]\n",
      "Batch 761/1970 | Losses => i: [0.4983] v: [0.5511] t: [0.5517] ivt: [0.0564]\n",
      "Batch 762/1970 | Losses => i: [0.5195] v: [0.5520] t: [0.5459] ivt: [0.0430]\n",
      "Batch 763/1970 | Losses => i: [0.5091] v: [0.5449] t: [0.5493] ivt: [0.0473]\n",
      "Batch 764/1970 | Losses => i: [0.5058] v: [0.5522] t: [0.5554] ivt: [0.0486]\n",
      "Batch 765/1970 | Losses => i: [0.4999] v: [0.5437] t: [0.5511] ivt: [0.0522]\n",
      "Batch 766/1970 | Losses => i: [0.5039] v: [0.5514] t: [0.5509] ivt: [0.0459]\n",
      "Batch 767/1970 | Losses => i: [0.4963] v: [0.5404] t: [0.5451] ivt: [0.0447]\n",
      "Batch 768/1970 | Losses => i: [0.4908] v: [0.5385] t: [0.5403] ivt: [0.0503]\n",
      "Batch 769/1970 | Losses => i: [0.5131] v: [0.5478] t: [0.5510] ivt: [0.0427]\n",
      "Batch 770/1970 | Losses => i: [0.4941] v: [0.5404] t: [0.5429] ivt: [0.0535]\n",
      "Batch 771/1970 | Losses => i: [0.4943] v: [0.5550] t: [0.5519] ivt: [0.0530]\n",
      "Batch 772/1970 | Losses => i: [0.5211] v: [0.5477] t: [0.5422] ivt: [0.0531]\n",
      "Batch 773/1970 | Losses => i: [0.4993] v: [0.5472] t: [0.5387] ivt: [0.0492]\n",
      "Batch 774/1970 | Losses => i: [0.5041] v: [0.5552] t: [0.5554] ivt: [0.0434]\n",
      "Batch 775/1970 | Losses => i: [0.5011] v: [0.5427] t: [0.5360] ivt: [0.0523]\n",
      "Batch 776/1970 | Losses => i: [0.4823] v: [0.5329] t: [0.5340] ivt: [0.0567]\n",
      "Batch 777/1970 | Losses => i: [0.5192] v: [0.5529] t: [0.5527] ivt: [0.0560]\n",
      "Batch 778/1970 | Losses => i: [0.5185] v: [0.5517] t: [0.5482] ivt: [0.0548]\n",
      "Batch 779/1970 | Losses => i: [0.5108] v: [0.5416] t: [0.5401] ivt: [0.0519]\n",
      "Batch 780/1970 | Losses => i: [0.5041] v: [0.5420] t: [0.5443] ivt: [0.0463]\n",
      "Batch 781/1970 | Losses => i: [0.4977] v: [0.5394] t: [0.5462] ivt: [0.0576]\n",
      "Batch 782/1970 | Losses => i: [0.5271] v: [0.5487] t: [0.5468] ivt: [0.0537]\n",
      "Batch 783/1970 | Losses => i: [0.5037] v: [0.5414] t: [0.5406] ivt: [0.0510]\n",
      "Batch 784/1970 | Losses => i: [0.4986] v: [0.5486] t: [0.5424] ivt: [0.0594]\n",
      "Batch 785/1970 | Losses => i: [0.4986] v: [0.5510] t: [0.5511] ivt: [0.0477]\n",
      "Batch 786/1970 | Losses => i: [0.5366] v: [0.5479] t: [0.5419] ivt: [0.0491]\n",
      "Batch 787/1970 | Losses => i: [0.5127] v: [0.5417] t: [0.5387] ivt: [0.0487]\n",
      "Batch 788/1970 | Losses => i: [0.4957] v: [0.5335] t: [0.5297] ivt: [0.0495]\n",
      "Batch 789/1970 | Losses => i: [0.5168] v: [0.5453] t: [0.5380] ivt: [0.0504]\n",
      "Batch 790/1970 | Losses => i: [0.4993] v: [0.5312] t: [0.5353] ivt: [0.0542]\n",
      "Batch 791/1970 | Losses => i: [0.5121] v: [0.5491] t: [0.5428] ivt: [0.0561]\n",
      "Batch 792/1970 | Losses => i: [0.5084] v: [0.5442] t: [0.5391] ivt: [0.0507]\n",
      "Batch 793/1970 | Losses => i: [0.4963] v: [0.5359] t: [0.5269] ivt: [0.0548]\n",
      "Batch 794/1970 | Losses => i: [0.4893] v: [0.5316] t: [0.5246] ivt: [0.0516]\n",
      "Batch 795/1970 | Losses => i: [0.5274] v: [0.5400] t: [0.5345] ivt: [0.0588]\n",
      "Batch 796/1970 | Losses => i: [0.5104] v: [0.5408] t: [0.5336] ivt: [0.0537]\n",
      "Batch 797/1970 | Losses => i: [0.5174] v: [0.5421] t: [0.5316] ivt: [0.0546]\n",
      "Batch 798/1970 | Losses => i: [0.5192] v: [0.5429] t: [0.5362] ivt: [0.0523]\n",
      "Batch 799/1970 | Losses => i: [0.4850] v: [0.5390] t: [0.5297] ivt: [0.0415]\n",
      "Batch 800/1970 | Losses => i: [0.5061] v: [0.5345] t: [0.5366] ivt: [0.0522]\n",
      "Batch 801/1970 | Losses => i: [0.4788] v: [0.5336] t: [0.5348] ivt: [0.0505]\n",
      "Batch 802/1970 | Losses => i: [0.5069] v: [0.5382] t: [0.5371] ivt: [0.0514]\n",
      "Batch 803/1970 | Losses => i: [0.5147] v: [0.5386] t: [0.5280] ivt: [0.0518]\n",
      "Batch 804/1970 | Losses => i: [0.4883] v: [0.5335] t: [0.5234] ivt: [0.0482]\n",
      "Batch 805/1970 | Losses => i: [0.4859] v: [0.5356] t: [0.5243] ivt: [0.0450]\n",
      "Batch 806/1970 | Losses => i: [0.5030] v: [0.5375] t: [0.5270] ivt: [0.0497]\n",
      "Batch 807/1970 | Losses => i: [0.4986] v: [0.5437] t: [0.5320] ivt: [0.0495]\n",
      "Batch 808/1970 | Losses => i: [0.4965] v: [0.5322] t: [0.5240] ivt: [0.0499]\n",
      "Batch 809/1970 | Losses => i: [0.5001] v: [0.5310] t: [0.5291] ivt: [0.0413]\n",
      "Batch 810/1970 | Losses => i: [0.4805] v: [0.5324] t: [0.5216] ivt: [0.0494]\n",
      "Batch 811/1970 | Losses => i: [0.4948] v: [0.5357] t: [0.5212] ivt: [0.0405]\n",
      "Batch 812/1970 | Losses => i: [0.5020] v: [0.5433] t: [0.5309] ivt: [0.0502]\n",
      "Batch 813/1970 | Losses => i: [0.5161] v: [0.5327] t: [0.5199] ivt: [0.0544]\n",
      "Batch 814/1970 | Losses => i: [0.4949] v: [0.5300] t: [0.5201] ivt: [0.0471]\n",
      "Batch 815/1970 | Losses => i: [0.5118] v: [0.5358] t: [0.5261] ivt: [0.0496]\n",
      "Batch 816/1970 | Losses => i: [0.5155] v: [0.5299] t: [0.5190] ivt: [0.0527]\n",
      "Batch 817/1970 | Losses => i: [0.5060] v: [0.5349] t: [0.5249] ivt: [0.0525]\n",
      "Batch 818/1970 | Losses => i: [0.4984] v: [0.5380] t: [0.5260] ivt: [0.0447]\n",
      "Batch 819/1970 | Losses => i: [0.5035] v: [0.5317] t: [0.5143] ivt: [0.0564]\n",
      "Batch 820/1970 | Losses => i: [0.4998] v: [0.5400] t: [0.5186] ivt: [0.0453]\n",
      "Batch 821/1970 | Losses => i: [0.5012] v: [0.5327] t: [0.5263] ivt: [0.0553]\n",
      "Batch 822/1970 | Losses => i: [0.4965] v: [0.5386] t: [0.5165] ivt: [0.0501]\n",
      "Batch 823/1970 | Losses => i: [0.4880] v: [0.5330] t: [0.5165] ivt: [0.0508]\n",
      "Batch 824/1970 | Losses => i: [0.4976] v: [0.5297] t: [0.5142] ivt: [0.0591]\n",
      "Batch 825/1970 | Losses => i: [0.4863] v: [0.5271] t: [0.5183] ivt: [0.0496]\n",
      "Batch 826/1970 | Losses => i: [0.5063] v: [0.5304] t: [0.5196] ivt: [0.0590]\n",
      "Batch 827/1970 | Losses => i: [0.5275] v: [0.5316] t: [0.5164] ivt: [0.0594]\n",
      "Batch 828/1970 | Losses => i: [0.5329] v: [0.5337] t: [0.5158] ivt: [0.0569]\n",
      "Batch 829/1970 | Losses => i: [0.5010] v: [0.5361] t: [0.5251] ivt: [0.0490]\n",
      "Batch 830/1970 | Losses => i: [0.4948] v: [0.5317] t: [0.5184] ivt: [0.0456]\n",
      "Batch 831/1970 | Losses => i: [0.4981] v: [0.5246] t: [0.5103] ivt: [0.0495]\n",
      "Batch 832/1970 | Losses => i: [0.5097] v: [0.5254] t: [0.5069] ivt: [0.0517]\n",
      "Batch 833/1970 | Losses => i: [0.4985] v: [0.5283] t: [0.5096] ivt: [0.0513]\n",
      "Batch 834/1970 | Losses => i: [0.5024] v: [0.5277] t: [0.5108] ivt: [0.0488]\n",
      "Batch 835/1970 | Losses => i: [0.5017] v: [0.5273] t: [0.5167] ivt: [0.0487]\n",
      "Batch 836/1970 | Losses => i: [0.5118] v: [0.5302] t: [0.5138] ivt: [0.0475]\n",
      "Batch 837/1970 | Losses => i: [0.4976] v: [0.5290] t: [0.5071] ivt: [0.0549]\n",
      "Batch 838/1970 | Losses => i: [0.4944] v: [0.5419] t: [0.5114] ivt: [0.0544]\n",
      "Batch 839/1970 | Losses => i: [0.4900] v: [0.5260] t: [0.5090] ivt: [0.0470]\n",
      "Batch 840/1970 | Losses => i: [0.4867] v: [0.5190] t: [0.5061] ivt: [0.0521]\n",
      "Batch 841/1970 | Losses => i: [0.4954] v: [0.5224] t: [0.5024] ivt: [0.0472]\n",
      "Batch 842/1970 | Losses => i: [0.4983] v: [0.5359] t: [0.5106] ivt: [0.0516]\n",
      "Batch 843/1970 | Losses => i: [0.5190] v: [0.5321] t: [0.5063] ivt: [0.0510]\n",
      "Batch 844/1970 | Losses => i: [0.4914] v: [0.5252] t: [0.4982] ivt: [0.0458]\n",
      "Batch 845/1970 | Losses => i: [0.4902] v: [0.5192] t: [0.4951] ivt: [0.0555]\n",
      "Batch 846/1970 | Losses => i: [0.5005] v: [0.5306] t: [0.5007] ivt: [0.0604]\n",
      "Batch 847/1970 | Losses => i: [0.4860] v: [0.5290] t: [0.4994] ivt: [0.0567]\n",
      "Batch 848/1970 | Losses => i: [0.4909] v: [0.5204] t: [0.4991] ivt: [0.0478]\n",
      "Batch 849/1970 | Losses => i: [0.5272] v: [0.5273] t: [0.5074] ivt: [0.0562]\n",
      "Batch 850/1970 | Losses => i: [0.4993] v: [0.5245] t: [0.5054] ivt: [0.0533]\n",
      "Batch 851/1970 | Losses => i: [0.5101] v: [0.5221] t: [0.4970] ivt: [0.0472]\n",
      "Batch 852/1970 | Losses => i: [0.4869] v: [0.5228] t: [0.4943] ivt: [0.0545]\n",
      "Batch 853/1970 | Losses => i: [0.4840] v: [0.5185] t: [0.4868] ivt: [0.0478]\n",
      "Batch 854/1970 | Losses => i: [0.4900] v: [0.5186] t: [0.4966] ivt: [0.0510]\n",
      "Batch 855/1970 | Losses => i: [0.4968] v: [0.5335] t: [0.4970] ivt: [0.0489]\n",
      "Batch 856/1970 | Losses => i: [0.5128] v: [0.5205] t: [0.4854] ivt: [0.0530]\n",
      "Batch 857/1970 | Losses => i: [0.5025] v: [0.5294] t: [0.4951] ivt: [0.0580]\n",
      "Batch 858/1970 | Losses => i: [0.4930] v: [0.5147] t: [0.4875] ivt: [0.0546]\n",
      "Batch 859/1970 | Losses => i: [0.4947] v: [0.5178] t: [0.4848] ivt: [0.0582]\n",
      "Batch 860/1970 | Losses => i: [0.5011] v: [0.5252] t: [0.4917] ivt: [0.0517]\n",
      "Batch 861/1970 | Losses => i: [0.4865] v: [0.5211] t: [0.4858] ivt: [0.0531]\n",
      "Batch 862/1970 | Losses => i: [0.5225] v: [0.5229] t: [0.4986] ivt: [0.0468]\n",
      "Batch 863/1970 | Losses => i: [0.5207] v: [0.5299] t: [0.4848] ivt: [0.0470]\n",
      "Batch 864/1970 | Losses => i: [0.4985] v: [0.5206] t: [0.4869] ivt: [0.0543]\n",
      "Batch 865/1970 | Losses => i: [0.4977] v: [0.5172] t: [0.4790] ivt: [0.0468]\n",
      "Batch 866/1970 | Losses => i: [0.5007] v: [0.5232] t: [0.4795] ivt: [0.0488]\n",
      "Batch 867/1970 | Losses => i: [0.4896] v: [0.5213] t: [0.4779] ivt: [0.0470]\n",
      "Batch 868/1970 | Losses => i: [0.4828] v: [0.5152] t: [0.4745] ivt: [0.0466]\n",
      "Batch 869/1970 | Losses => i: [0.4720] v: [0.5065] t: [0.4673] ivt: [0.0527]\n",
      "Batch 870/1970 | Losses => i: [0.4856] v: [0.5140] t: [0.4776] ivt: [0.0595]\n",
      "Batch 871/1970 | Losses => i: [0.4772] v: [0.5145] t: [0.4727] ivt: [0.0545]\n",
      "Batch 872/1970 | Losses => i: [0.4948] v: [0.5385] t: [0.4802] ivt: [0.0476]\n",
      "Batch 873/1970 | Losses => i: [0.4752] v: [0.5089] t: [0.4771] ivt: [0.0521]\n",
      "Batch 874/1970 | Losses => i: [0.4859] v: [0.5075] t: [0.4668] ivt: [0.0440]\n",
      "Batch 875/1970 | Losses => i: [0.4811] v: [0.5199] t: [0.4775] ivt: [0.0478]\n",
      "Batch 876/1970 | Losses => i: [0.4882] v: [0.5139] t: [0.4650] ivt: [0.0508]\n",
      "Batch 877/1970 | Losses => i: [0.4907] v: [0.5268] t: [0.4777] ivt: [0.0469]\n",
      "Batch 878/1970 | Losses => i: [0.4867] v: [0.5180] t: [0.4769] ivt: [0.0578]\n",
      "Batch 879/1970 | Losses => i: [0.4939] v: [0.5132] t: [0.4670] ivt: [0.0498]\n",
      "Batch 880/1970 | Losses => i: [0.5098] v: [0.5172] t: [0.4659] ivt: [0.0535]\n",
      "Batch 881/1970 | Losses => i: [0.5021] v: [0.5141] t: [0.4641] ivt: [0.0529]\n",
      "Batch 882/1970 | Losses => i: [0.4832] v: [0.5082] t: [0.4672] ivt: [0.0434]\n",
      "Batch 883/1970 | Losses => i: [0.4922] v: [0.5103] t: [0.4700] ivt: [0.0473]\n",
      "Batch 884/1970 | Losses => i: [0.4879] v: [0.5092] t: [0.4617] ivt: [0.0509]\n",
      "Batch 885/1970 | Losses => i: [0.4931] v: [0.5122] t: [0.4602] ivt: [0.0503]\n",
      "Batch 886/1970 | Losses => i: [0.4800] v: [0.5013] t: [0.4591] ivt: [0.0542]\n",
      "Batch 887/1970 | Losses => i: [0.4903] v: [0.5100] t: [0.4661] ivt: [0.0519]\n",
      "Batch 888/1970 | Losses => i: [0.4761] v: [0.5065] t: [0.4454] ivt: [0.0519]\n",
      "Batch 889/1970 | Losses => i: [0.4865] v: [0.5232] t: [0.4684] ivt: [0.0421]\n",
      "Batch 890/1970 | Losses => i: [0.4996] v: [0.5097] t: [0.4602] ivt: [0.0494]\n",
      "Batch 891/1970 | Losses => i: [0.4954] v: [0.5237] t: [0.4470] ivt: [0.0591]\n",
      "Batch 892/1970 | Losses => i: [0.4936] v: [0.5093] t: [0.4618] ivt: [0.0512]\n",
      "Batch 893/1970 | Losses => i: [0.4951] v: [0.5032] t: [0.4426] ivt: [0.0536]\n",
      "Batch 894/1970 | Losses => i: [0.4628] v: [0.5038] t: [0.4461] ivt: [0.0418]\n",
      "Batch 895/1970 | Losses => i: [0.4878] v: [0.5056] t: [0.4473] ivt: [0.0490]\n",
      "Batch 896/1970 | Losses => i: [0.4905] v: [0.5060] t: [0.4453] ivt: [0.0540]\n",
      "Batch 897/1970 | Losses => i: [0.5136] v: [0.5065] t: [0.4505] ivt: [0.0529]\n",
      "Batch 898/1970 | Losses => i: [0.4856] v: [0.5060] t: [0.4461] ivt: [0.0542]\n",
      "Batch 899/1970 | Losses => i: [0.4767] v: [0.5038] t: [0.4292] ivt: [0.0512]\n",
      "Batch 900/1970 | Losses => i: [0.4680] v: [0.5152] t: [0.4512] ivt: [0.0543]\n",
      "Batch 901/1970 | Losses => i: [0.4799] v: [0.5067] t: [0.4406] ivt: [0.0514]\n",
      "Batch 902/1970 | Losses => i: [0.4726] v: [0.5004] t: [0.4377] ivt: [0.0473]\n",
      "Batch 903/1970 | Losses => i: [0.4717] v: [0.4940] t: [0.4379] ivt: [0.0534]\n",
      "Batch 904/1970 | Losses => i: [0.4821] v: [0.4965] t: [0.4390] ivt: [0.0498]\n",
      "Batch 905/1970 | Losses => i: [0.4649] v: [0.5019] t: [0.4295] ivt: [0.0541]\n",
      "Batch 906/1970 | Losses => i: [0.4747] v: [0.4909] t: [0.4207] ivt: [0.0508]\n",
      "Batch 907/1970 | Losses => i: [0.4898] v: [0.5086] t: [0.4442] ivt: [0.0523]\n",
      "Batch 908/1970 | Losses => i: [0.5032] v: [0.5048] t: [0.4310] ivt: [0.0525]\n",
      "Batch 909/1970 | Losses => i: [0.4951] v: [0.5038] t: [0.4280] ivt: [0.0540]\n",
      "Batch 910/1970 | Losses => i: [0.4630] v: [0.4937] t: [0.4187] ivt: [0.0463]\n",
      "Batch 911/1970 | Losses => i: [0.4916] v: [0.5030] t: [0.4337] ivt: [0.0529]\n",
      "Batch 912/1970 | Losses => i: [0.4881] v: [0.4987] t: [0.4286] ivt: [0.0511]\n",
      "Batch 913/1970 | Losses => i: [0.5007] v: [0.4976] t: [0.4236] ivt: [0.0483]\n",
      "Batch 914/1970 | Losses => i: [0.4781] v: [0.4978] t: [0.4300] ivt: [0.0530]\n",
      "Batch 915/1970 | Losses => i: [0.4815] v: [0.4946] t: [0.4169] ivt: [0.0537]\n",
      "Batch 916/1970 | Losses => i: [0.4880] v: [0.4874] t: [0.4146] ivt: [0.0530]\n",
      "Batch 917/1970 | Losses => i: [0.4511] v: [0.4855] t: [0.4006] ivt: [0.0474]\n",
      "Batch 918/1970 | Losses => i: [0.4761] v: [0.4907] t: [0.4148] ivt: [0.0421]\n",
      "Batch 919/1970 | Losses => i: [0.4803] v: [0.4908] t: [0.4103] ivt: [0.0510]\n",
      "Batch 920/1970 | Losses => i: [0.4921] v: [0.4889] t: [0.4111] ivt: [0.0513]\n",
      "Batch 921/1970 | Losses => i: [0.4723] v: [0.4914] t: [0.4061] ivt: [0.0402]\n",
      "Batch 922/1970 | Losses => i: [0.4686] v: [0.4867] t: [0.3965] ivt: [0.0440]\n",
      "Batch 923/1970 | Losses => i: [0.5021] v: [0.4953] t: [0.4103] ivt: [0.0511]\n",
      "Batch 924/1970 | Losses => i: [0.4777] v: [0.4850] t: [0.4052] ivt: [0.0484]\n",
      "Batch 925/1970 | Losses => i: [0.4710] v: [0.4766] t: [0.3909] ivt: [0.0471]\n",
      "Batch 926/1970 | Losses => i: [0.4870] v: [0.4929] t: [0.4072] ivt: [0.0494]\n",
      "Batch 927/1970 | Losses => i: [0.4768] v: [0.4792] t: [0.3898] ivt: [0.0519]\n",
      "Batch 928/1970 | Losses => i: [0.4805] v: [0.4878] t: [0.3886] ivt: [0.0474]\n",
      "Batch 929/1970 | Losses => i: [0.4822] v: [0.4920] t: [0.4071] ivt: [0.0570]\n",
      "Batch 930/1970 | Losses => i: [0.4823] v: [0.4891] t: [0.4077] ivt: [0.0609]\n",
      "Batch 931/1970 | Losses => i: [0.4677] v: [0.4867] t: [0.3945] ivt: [0.0466]\n",
      "Batch 932/1970 | Losses => i: [0.4943] v: [0.4836] t: [0.3925] ivt: [0.0567]\n",
      "Batch 933/1970 | Losses => i: [0.4738] v: [0.4864] t: [0.3882] ivt: [0.0467]\n",
      "Batch 934/1970 | Losses => i: [0.4817] v: [0.4863] t: [0.3846] ivt: [0.0428]\n",
      "Batch 935/1970 | Losses => i: [0.4547] v: [0.4708] t: [0.3789] ivt: [0.0496]\n",
      "Batch 936/1970 | Losses => i: [0.4950] v: [0.5002] t: [0.3911] ivt: [0.0579]\n",
      "Batch 937/1970 | Losses => i: [0.4747] v: [0.4852] t: [0.3831] ivt: [0.0550]\n",
      "Batch 938/1970 | Losses => i: [0.4696] v: [0.4786] t: [0.3757] ivt: [0.0473]\n",
      "Batch 939/1970 | Losses => i: [0.4680] v: [0.4812] t: [0.3836] ivt: [0.0531]\n",
      "Batch 940/1970 | Losses => i: [0.4884] v: [0.4764] t: [0.3721] ivt: [0.0543]\n",
      "Batch 941/1970 | Losses => i: [0.4571] v: [0.4803] t: [0.3765] ivt: [0.0475]\n",
      "Batch 942/1970 | Losses => i: [0.4796] v: [0.4851] t: [0.3801] ivt: [0.0540]\n",
      "Batch 943/1970 | Losses => i: [0.4715] v: [0.4747] t: [0.3704] ivt: [0.0498]\n",
      "Batch 944/1970 | Losses => i: [0.4911] v: [0.4840] t: [0.3696] ivt: [0.0468]\n",
      "Batch 945/1970 | Losses => i: [0.4787] v: [0.4793] t: [0.3703] ivt: [0.0492]\n",
      "Batch 946/1970 | Losses => i: [0.4685] v: [0.4691] t: [0.3585] ivt: [0.0498]\n",
      "Batch 947/1970 | Losses => i: [0.4734] v: [0.4747] t: [0.3603] ivt: [0.0495]\n",
      "Batch 948/1970 | Losses => i: [0.4705] v: [0.4789] t: [0.3782] ivt: [0.0591]\n",
      "Batch 949/1970 | Losses => i: [0.4659] v: [0.4649] t: [0.3530] ivt: [0.0433]\n",
      "Batch 950/1970 | Losses => i: [0.4604] v: [0.4629] t: [0.3480] ivt: [0.0438]\n",
      "Batch 951/1970 | Losses => i: [0.4625] v: [0.4676] t: [0.3537] ivt: [0.0514]\n",
      "Batch 952/1970 | Losses => i: [0.4727] v: [0.4717] t: [0.3608] ivt: [0.0521]\n",
      "Batch 953/1970 | Losses => i: [0.4934] v: [0.4677] t: [0.3551] ivt: [0.0504]\n",
      "Batch 954/1970 | Losses => i: [0.4559] v: [0.4585] t: [0.3585] ivt: [0.0518]\n",
      "Batch 955/1970 | Losses => i: [0.5279] v: [0.4782] t: [0.3481] ivt: [0.0591]\n",
      "Batch 956/1970 | Losses => i: [0.4649] v: [0.4745] t: [0.3638] ivt: [0.0496]\n",
      "Batch 957/1970 | Losses => i: [0.4501] v: [0.4609] t: [0.3475] ivt: [0.0558]\n",
      "Batch 958/1970 | Losses => i: [0.4751] v: [0.4645] t: [0.3607] ivt: [0.0522]\n",
      "Batch 959/1970 | Losses => i: [0.4858] v: [0.4672] t: [0.3457] ivt: [0.0538]\n",
      "Batch 960/1970 | Losses => i: [0.4720] v: [0.4723] t: [0.3551] ivt: [0.0486]\n",
      "Batch 961/1970 | Losses => i: [0.4738] v: [0.4692] t: [0.3306] ivt: [0.0436]\n",
      "Batch 962/1970 | Losses => i: [0.4570] v: [0.4660] t: [0.3340] ivt: [0.0463]\n",
      "Batch 963/1970 | Losses => i: [0.4829] v: [0.4683] t: [0.3375] ivt: [0.0516]\n",
      "Batch 964/1970 | Losses => i: [0.4601] v: [0.4574] t: [0.3306] ivt: [0.0497]\n",
      "Batch 965/1970 | Losses => i: [0.4534] v: [0.4608] t: [0.3344] ivt: [0.0464]\n",
      "Batch 966/1970 | Losses => i: [0.4745] v: [0.4694] t: [0.3318] ivt: [0.0528]\n",
      "Batch 967/1970 | Losses => i: [0.4750] v: [0.4602] t: [0.3381] ivt: [0.0579]\n",
      "Batch 968/1970 | Losses => i: [0.4723] v: [0.4650] t: [0.3362] ivt: [0.0455]\n",
      "Batch 969/1970 | Losses => i: [0.4583] v: [0.4640] t: [0.3257] ivt: [0.0418]\n",
      "Batch 970/1970 | Losses => i: [0.4659] v: [0.4561] t: [0.3164] ivt: [0.0502]\n",
      "Batch 971/1970 | Losses => i: [0.4569] v: [0.4664] t: [0.3295] ivt: [0.0572]\n",
      "Batch 972/1970 | Losses => i: [0.4900] v: [0.4588] t: [0.3261] ivt: [0.0459]\n",
      "Batch 973/1970 | Losses => i: [0.4626] v: [0.4556] t: [0.3218] ivt: [0.0521]\n",
      "Batch 974/1970 | Losses => i: [0.4674] v: [0.4570] t: [0.3234] ivt: [0.0541]\n",
      "Batch 975/1970 | Losses => i: [0.4732] v: [0.4515] t: [0.3149] ivt: [0.0553]\n",
      "Batch 976/1970 | Losses => i: [0.4552] v: [0.4455] t: [0.3119] ivt: [0.0468]\n",
      "Batch 977/1970 | Losses => i: [0.4731] v: [0.4594] t: [0.3318] ivt: [0.0570]\n",
      "Batch 978/1970 | Losses => i: [0.4860] v: [0.4495] t: [0.3196] ivt: [0.0591]\n",
      "Batch 979/1970 | Losses => i: [0.4704] v: [0.4442] t: [0.3068] ivt: [0.0493]\n",
      "Batch 980/1970 | Losses => i: [0.4646] v: [0.4416] t: [0.2889] ivt: [0.0426]\n",
      "Batch 981/1970 | Losses => i: [0.4653] v: [0.4425] t: [0.2901] ivt: [0.0528]\n",
      "Batch 982/1970 | Losses => i: [0.4880] v: [0.4455] t: [0.3061] ivt: [0.0512]\n",
      "Batch 983/1970 | Losses => i: [0.4702] v: [0.4519] t: [0.3051] ivt: [0.0515]\n",
      "Batch 984/1970 | Losses => i: [0.4714] v: [0.4419] t: [0.3030] ivt: [0.0508]\n",
      "Batch 985/1970 | Losses => i: [0.4617] v: [0.4338] t: [0.2920] ivt: [0.0536]\n",
      "Batch 986/1970 | Losses => i: [0.4594] v: [0.4250] t: [0.2725] ivt: [0.0469]\n",
      "Batch 987/1970 | Losses => i: [0.4529] v: [0.4291] t: [0.2787] ivt: [0.0484]\n",
      "Batch 988/1970 | Losses => i: [0.4516] v: [0.4320] t: [0.2949] ivt: [0.0496]\n",
      "Batch 989/1970 | Losses => i: [0.4511] v: [0.4264] t: [0.2843] ivt: [0.0465]\n",
      "Batch 990/1970 | Losses => i: [0.4787] v: [0.4385] t: [0.3036] ivt: [0.0557]\n",
      "Batch 991/1970 | Losses => i: [0.4657] v: [0.4377] t: [0.2843] ivt: [0.0518]\n",
      "Batch 992/1970 | Losses => i: [0.4599] v: [0.4326] t: [0.2708] ivt: [0.0463]\n",
      "Batch 993/1970 | Losses => i: [0.4709] v: [0.4360] t: [0.2745] ivt: [0.0527]\n",
      "Batch 994/1970 | Losses => i: [0.4568] v: [0.4299] t: [0.2801] ivt: [0.0596]\n",
      "Batch 995/1970 | Losses => i: [0.4622] v: [0.4258] t: [0.2736] ivt: [0.0462]\n",
      "Batch 996/1970 | Losses => i: [0.4660] v: [0.4318] t: [0.2811] ivt: [0.0444]\n",
      "Batch 997/1970 | Losses => i: [0.4650] v: [0.4254] t: [0.2653] ivt: [0.0473]\n",
      "Batch 998/1970 | Losses => i: [0.4573] v: [0.4171] t: [0.2663] ivt: [0.0481]\n",
      "Batch 999/1970 | Losses => i: [0.4614] v: [0.4229] t: [0.2944] ivt: [0.0554]\n",
      "Batch 1000/1970 | Losses => i: [0.4454] v: [0.4259] t: [0.2848] ivt: [0.0542]\n",
      "Batch 1001/1970 | Losses => i: [0.4598] v: [0.4256] t: [0.2786] ivt: [0.0519]\n",
      "Batch 1002/1970 | Losses => i: [0.4620] v: [0.4400] t: [0.2830] ivt: [0.0547]\n",
      "Batch 1003/1970 | Losses => i: [0.4738] v: [0.4206] t: [0.2584] ivt: [0.0466]\n",
      "Batch 1004/1970 | Losses => i: [0.4676] v: [0.4205] t: [0.2723] ivt: [0.0477]\n",
      "Batch 1005/1970 | Losses => i: [0.4502] v: [0.4117] t: [0.2666] ivt: [0.0528]\n",
      "Batch 1006/1970 | Losses => i: [0.4968] v: [0.4320] t: [0.2683] ivt: [0.0507]\n",
      "Batch 1007/1970 | Losses => i: [0.4768] v: [0.4170] t: [0.2549] ivt: [0.0459]\n",
      "Batch 1008/1970 | Losses => i: [0.4676] v: [0.3998] t: [0.2364] ivt: [0.0488]\n",
      "Batch 1009/1970 | Losses => i: [0.4884] v: [0.4156] t: [0.2538] ivt: [0.0438]\n",
      "Batch 1010/1970 | Losses => i: [0.4524] v: [0.3988] t: [0.2550] ivt: [0.0470]\n",
      "Batch 1011/1970 | Losses => i: [0.4937] v: [0.4178] t: [0.2715] ivt: [0.0570]\n",
      "Batch 1012/1970 | Losses => i: [0.4494] v: [0.3968] t: [0.2426] ivt: [0.0483]\n",
      "Batch 1013/1970 | Losses => i: [0.4858] v: [0.4101] t: [0.2646] ivt: [0.0514]\n",
      "Batch 1014/1970 | Losses => i: [0.4794] v: [0.4039] t: [0.2596] ivt: [0.0586]\n",
      "Batch 1015/1970 | Losses => i: [0.4446] v: [0.4027] t: [0.2426] ivt: [0.0513]\n",
      "Batch 1016/1970 | Losses => i: [0.4581] v: [0.4022] t: [0.2351] ivt: [0.0507]\n",
      "Batch 1017/1970 | Losses => i: [0.4307] v: [0.3853] t: [0.2272] ivt: [0.0439]\n",
      "Batch 1018/1970 | Losses => i: [0.4672] v: [0.3996] t: [0.2326] ivt: [0.0529]\n",
      "Batch 1019/1970 | Losses => i: [0.4499] v: [0.3913] t: [0.2427] ivt: [0.0452]\n",
      "Batch 1020/1970 | Losses => i: [0.4548] v: [0.3957] t: [0.2366] ivt: [0.0469]\n",
      "Batch 1021/1970 | Losses => i: [0.4721] v: [0.3979] t: [0.2374] ivt: [0.0473]\n",
      "Batch 1022/1970 | Losses => i: [0.4812] v: [0.3913] t: [0.2249] ivt: [0.0557]\n",
      "Batch 1023/1970 | Losses => i: [0.4679] v: [0.3988] t: [0.2430] ivt: [0.0546]\n",
      "Batch 1024/1970 | Losses => i: [0.4521] v: [0.3900] t: [0.2353] ivt: [0.0472]\n",
      "Batch 1025/1970 | Losses => i: [0.4765] v: [0.3868] t: [0.2268] ivt: [0.0480]\n",
      "Batch 1026/1970 | Losses => i: [0.4593] v: [0.3891] t: [0.2307] ivt: [0.0550]\n",
      "Batch 1027/1970 | Losses => i: [0.4763] v: [0.3915] t: [0.2386] ivt: [0.0537]\n",
      "Batch 1028/1970 | Losses => i: [0.4769] v: [0.3861] t: [0.2195] ivt: [0.0429]\n",
      "Batch 1029/1970 | Losses => i: [0.4623] v: [0.3823] t: [0.2316] ivt: [0.0567]\n",
      "Batch 1030/1970 | Losses => i: [0.4685] v: [0.3882] t: [0.2152] ivt: [0.0450]\n",
      "Batch 1031/1970 | Losses => i: [0.4786] v: [0.3828] t: [0.2217] ivt: [0.0555]\n",
      "Batch 1032/1970 | Losses => i: [0.4619] v: [0.3811] t: [0.2250] ivt: [0.0513]\n",
      "Batch 1033/1970 | Losses => i: [0.4296] v: [0.3939] t: [0.2094] ivt: [0.0534]\n",
      "Batch 1034/1970 | Losses => i: [0.4471] v: [0.3688] t: [0.1969] ivt: [0.0478]\n",
      "Batch 1035/1970 | Losses => i: [0.4703] v: [0.3818] t: [0.2248] ivt: [0.0513]\n",
      "Batch 1036/1970 | Losses => i: [0.4685] v: [0.3687] t: [0.2075] ivt: [0.0542]\n",
      "Batch 1037/1970 | Losses => i: [0.4560] v: [0.3686] t: [0.2104] ivt: [0.0499]\n",
      "Batch 1038/1970 | Losses => i: [0.4595] v: [0.3693] t: [0.2107] ivt: [0.0496]\n",
      "Batch 1039/1970 | Losses => i: [0.4616] v: [0.3652] t: [0.2063] ivt: [0.0523]\n",
      "Batch 1040/1970 | Losses => i: [0.4602] v: [0.3709] t: [0.2214] ivt: [0.0497]\n",
      "Batch 1041/1970 | Losses => i: [0.4578] v: [0.3681] t: [0.2004] ivt: [0.0466]\n",
      "Batch 1042/1970 | Losses => i: [0.4689] v: [0.3624] t: [0.2057] ivt: [0.0519]\n",
      "Batch 1043/1970 | Losses => i: [0.4731] v: [0.3670] t: [0.2090] ivt: [0.0511]\n",
      "Batch 1044/1970 | Losses => i: [0.4606] v: [0.3774] t: [0.2168] ivt: [0.0530]\n",
      "Batch 1045/1970 | Losses => i: [0.4466] v: [0.3534] t: [0.1861] ivt: [0.0424]\n",
      "Batch 1046/1970 | Losses => i: [0.4413] v: [0.3521] t: [0.2178] ivt: [0.0533]\n",
      "Batch 1047/1970 | Losses => i: [0.4904] v: [0.3680] t: [0.2221] ivt: [0.0549]\n",
      "Batch 1048/1970 | Losses => i: [0.4575] v: [0.3586] t: [0.2123] ivt: [0.0488]\n",
      "Batch 1049/1970 | Losses => i: [0.4562] v: [0.3617] t: [0.1943] ivt: [0.0538]\n",
      "Batch 1050/1970 | Losses => i: [0.5084] v: [0.3612] t: [0.1954] ivt: [0.0558]\n",
      "Batch 1051/1970 | Losses => i: [0.4497] v: [0.3457] t: [0.1973] ivt: [0.0525]\n",
      "Batch 1052/1970 | Losses => i: [0.4850] v: [0.3527] t: [0.1871] ivt: [0.0510]\n",
      "Batch 1053/1970 | Losses => i: [0.4688] v: [0.3597] t: [0.1989] ivt: [0.0521]\n",
      "Batch 1054/1970 | Losses => i: [0.4659] v: [0.3435] t: [0.2064] ivt: [0.0521]\n",
      "Batch 1055/1970 | Losses => i: [0.5101] v: [0.3594] t: [0.1931] ivt: [0.0558]\n",
      "Batch 1056/1970 | Losses => i: [0.4708] v: [0.3589] t: [0.1949] ivt: [0.0512]\n",
      "Batch 1057/1970 | Losses => i: [0.4508] v: [0.3545] t: [0.2031] ivt: [0.0490]\n",
      "Batch 1058/1970 | Losses => i: [0.4612] v: [0.3348] t: [0.1837] ivt: [0.0478]\n",
      "Batch 1059/1970 | Losses => i: [0.4542] v: [0.3403] t: [0.1715] ivt: [0.0486]\n",
      "Batch 1060/1970 | Losses => i: [0.5165] v: [0.3742] t: [0.2008] ivt: [0.0555]\n",
      "Batch 1061/1970 | Losses => i: [0.4827] v: [0.3520] t: [0.1931] ivt: [0.0563]\n",
      "Batch 1062/1970 | Losses => i: [0.4673] v: [0.3369] t: [0.1673] ivt: [0.0405]\n",
      "Batch 1063/1970 | Losses => i: [0.4554] v: [0.3376] t: [0.1830] ivt: [0.0497]\n",
      "Batch 1064/1970 | Losses => i: [0.4555] v: [0.3420] t: [0.1789] ivt: [0.0471]\n",
      "Batch 1065/1970 | Losses => i: [0.4545] v: [0.3268] t: [0.1739] ivt: [0.0455]\n",
      "Batch 1066/1970 | Losses => i: [0.4863] v: [0.3391] t: [0.1831] ivt: [0.0530]\n",
      "Batch 1067/1970 | Losses => i: [0.4682] v: [0.3298] t: [0.1831] ivt: [0.0480]\n",
      "Batch 1068/1970 | Losses => i: [0.4551] v: [0.3238] t: [0.1886] ivt: [0.0534]\n",
      "Batch 1069/1970 | Losses => i: [0.4682] v: [0.3342] t: [0.1669] ivt: [0.0469]\n",
      "Batch 1070/1970 | Losses => i: [0.4678] v: [0.3423] t: [0.1666] ivt: [0.0484]\n",
      "Batch 1071/1970 | Losses => i: [0.4276] v: [0.3089] t: [0.1752] ivt: [0.0507]\n",
      "Batch 1072/1970 | Losses => i: [0.4517] v: [0.3230] t: [0.1785] ivt: [0.0519]\n",
      "Batch 1073/1970 | Losses => i: [0.4709] v: [0.3275] t: [0.1598] ivt: [0.0413]\n",
      "Batch 1074/1970 | Losses => i: [0.4676] v: [0.3218] t: [0.1633] ivt: [0.0467]\n",
      "Batch 1075/1970 | Losses => i: [0.4706] v: [0.3240] t: [0.1829] ivt: [0.0518]\n",
      "Batch 1076/1970 | Losses => i: [0.4644] v: [0.3208] t: [0.1732] ivt: [0.0506]\n",
      "Batch 1077/1970 | Losses => i: [0.4580] v: [0.3338] t: [0.1740] ivt: [0.0540]\n",
      "Batch 1078/1970 | Losses => i: [0.4659] v: [0.3146] t: [0.1638] ivt: [0.0473]\n",
      "Batch 1079/1970 | Losses => i: [0.4518] v: [0.3046] t: [0.1684] ivt: [0.0517]\n",
      "Batch 1080/1970 | Losses => i: [0.4752] v: [0.3216] t: [0.1750] ivt: [0.0508]\n",
      "Batch 1081/1970 | Losses => i: [0.4803] v: [0.3135] t: [0.1762] ivt: [0.0507]\n",
      "Batch 1082/1970 | Losses => i: [0.4848] v: [0.3170] t: [0.1661] ivt: [0.0440]\n",
      "Batch 1083/1970 | Losses => i: [0.4811] v: [0.3174] t: [0.1699] ivt: [0.0528]\n",
      "Batch 1084/1970 | Losses => i: [0.4623] v: [0.3122] t: [0.1593] ivt: [0.0441]\n",
      "Batch 1085/1970 | Losses => i: [0.4434] v: [0.3212] t: [0.1723] ivt: [0.0515]\n",
      "Batch 1086/1970 | Losses => i: [0.5039] v: [0.3237] t: [0.1578] ivt: [0.0496]\n",
      "Batch 1087/1970 | Losses => i: [0.4494] v: [0.3091] t: [0.1596] ivt: [0.0513]\n",
      "Batch 1088/1970 | Losses => i: [0.4601] v: [0.3021] t: [0.1686] ivt: [0.0544]\n",
      "Batch 1089/1970 | Losses => i: [0.4497] v: [0.3127] t: [0.1645] ivt: [0.0474]\n",
      "Batch 1090/1970 | Losses => i: [0.4766] v: [0.3110] t: [0.1480] ivt: [0.0476]\n",
      "Batch 1091/1970 | Losses => i: [0.4457] v: [0.3001] t: [0.1471] ivt: [0.0496]\n",
      "Batch 1092/1970 | Losses => i: [0.4811] v: [0.3240] t: [0.2074] ivt: [0.0591]\n",
      "Batch 1093/1970 | Losses => i: [0.4710] v: [0.3266] t: [0.1726] ivt: [0.0592]\n",
      "Batch 1094/1970 | Losses => i: [0.4523] v: [0.2964] t: [0.1437] ivt: [0.0486]\n",
      "Batch 1095/1970 | Losses => i: [0.4403] v: [0.2826] t: [0.1303] ivt: [0.0419]\n",
      "Batch 1096/1970 | Losses => i: [0.4604] v: [0.2964] t: [0.1717] ivt: [0.0543]\n",
      "Batch 1097/1970 | Losses => i: [0.4492] v: [0.2917] t: [0.1453] ivt: [0.0467]\n",
      "Batch 1098/1970 | Losses => i: [0.4672] v: [0.3007] t: [0.1647] ivt: [0.0520]\n",
      "Batch 1099/1970 | Losses => i: [0.4393] v: [0.2927] t: [0.1800] ivt: [0.0464]\n",
      "Batch 1100/1970 | Losses => i: [0.4695] v: [0.3146] t: [0.1646] ivt: [0.0562]\n",
      "Batch 1101/1970 | Losses => i: [0.4617] v: [0.2924] t: [0.1445] ivt: [0.0463]\n",
      "Batch 1102/1970 | Losses => i: [0.4645] v: [0.2957] t: [0.1612] ivt: [0.0523]\n",
      "Batch 1103/1970 | Losses => i: [0.4572] v: [0.2961] t: [0.1400] ivt: [0.0498]\n",
      "Batch 1104/1970 | Losses => i: [0.4382] v: [0.2909] t: [0.1589] ivt: [0.0540]\n",
      "Batch 1105/1970 | Losses => i: [0.4345] v: [0.2762] t: [0.1425] ivt: [0.0487]\n",
      "Batch 1106/1970 | Losses => i: [0.4415] v: [0.2873] t: [0.1545] ivt: [0.0560]\n",
      "Batch 1107/1970 | Losses => i: [0.4600] v: [0.2861] t: [0.1451] ivt: [0.0497]\n",
      "Batch 1108/1970 | Losses => i: [0.4618] v: [0.2871] t: [0.1513] ivt: [0.0477]\n",
      "Batch 1109/1970 | Losses => i: [0.4465] v: [0.2814] t: [0.1566] ivt: [0.0504]\n",
      "Batch 1110/1970 | Losses => i: [0.4519] v: [0.2697] t: [0.1541] ivt: [0.0554]\n",
      "Batch 1111/1970 | Losses => i: [0.4656] v: [0.2813] t: [0.1404] ivt: [0.0480]\n",
      "Batch 1112/1970 | Losses => i: [0.4689] v: [0.2901] t: [0.1544] ivt: [0.0495]\n",
      "Batch 1113/1970 | Losses => i: [0.4666] v: [0.2897] t: [0.1848] ivt: [0.0561]\n",
      "Batch 1114/1970 | Losses => i: [0.4638] v: [0.3113] t: [0.1820] ivt: [0.0614]\n",
      "Batch 1115/1970 | Losses => i: [0.4756] v: [0.2877] t: [0.1483] ivt: [0.0451]\n",
      "Batch 1116/1970 | Losses => i: [0.4508] v: [0.2678] t: [0.1600] ivt: [0.0558]\n",
      "Batch 1117/1970 | Losses => i: [0.4444] v: [0.2617] t: [0.1558] ivt: [0.0545]\n",
      "Batch 1118/1970 | Losses => i: [0.4622] v: [0.2648] t: [0.1372] ivt: [0.0445]\n",
      "Batch 1119/1970 | Losses => i: [0.4503] v: [0.2715] t: [0.1387] ivt: [0.0384]\n",
      "Batch 1120/1970 | Losses => i: [0.4462] v: [0.2763] t: [0.1441] ivt: [0.0496]\n",
      "Batch 1121/1970 | Losses => i: [0.4661] v: [0.2886] t: [0.1661] ivt: [0.0535]\n",
      "Batch 1122/1970 | Losses => i: [0.4518] v: [0.2564] t: [0.1475] ivt: [0.0543]\n",
      "Batch 1123/1970 | Losses => i: [0.4454] v: [0.2772] t: [0.1539] ivt: [0.0489]\n",
      "Batch 1124/1970 | Losses => i: [0.4346] v: [0.2538] t: [0.1341] ivt: [0.0458]\n",
      "Batch 1125/1970 | Losses => i: [0.4342] v: [0.2561] t: [0.1317] ivt: [0.0448]\n",
      "Batch 1126/1970 | Losses => i: [0.4209] v: [0.2546] t: [0.1365] ivt: [0.0490]\n",
      "Batch 1127/1970 | Losses => i: [0.4527] v: [0.2581] t: [0.1443] ivt: [0.0422]\n",
      "Batch 1128/1970 | Losses => i: [0.4432] v: [0.2627] t: [0.1370] ivt: [0.0452]\n",
      "Batch 1129/1970 | Losses => i: [0.4372] v: [0.2494] t: [0.1316] ivt: [0.0473]\n",
      "Batch 1130/1970 | Losses => i: [0.4581] v: [0.2681] t: [0.1369] ivt: [0.0493]\n",
      "Batch 1131/1970 | Losses => i: [0.4593] v: [0.2518] t: [0.1380] ivt: [0.0482]\n",
      "Batch 1132/1970 | Losses => i: [0.4453] v: [0.2874] t: [0.1611] ivt: [0.0550]\n",
      "Batch 1133/1970 | Losses => i: [0.4678] v: [0.2562] t: [0.1709] ivt: [0.0522]\n",
      "Batch 1134/1970 | Losses => i: [0.4458] v: [0.2806] t: [0.1417] ivt: [0.0471]\n",
      "Batch 1135/1970 | Losses => i: [0.4446] v: [0.2540] t: [0.1272] ivt: [0.0450]\n",
      "Batch 1136/1970 | Losses => i: [0.4358] v: [0.2449] t: [0.1430] ivt: [0.0464]\n",
      "Batch 1137/1970 | Losses => i: [0.4747] v: [0.2674] t: [0.1397] ivt: [0.0506]\n",
      "Batch 1138/1970 | Losses => i: [0.4719] v: [0.2555] t: [0.1206] ivt: [0.0447]\n",
      "Batch 1139/1970 | Losses => i: [0.4522] v: [0.2509] t: [0.1321] ivt: [0.0436]\n",
      "Batch 1140/1970 | Losses => i: [0.4535] v: [0.2622] t: [0.1745] ivt: [0.0605]\n",
      "Batch 1141/1970 | Losses => i: [0.4186] v: [0.2423] t: [0.1587] ivt: [0.0464]\n",
      "Batch 1142/1970 | Losses => i: [0.4366] v: [0.2525] t: [0.1392] ivt: [0.0504]\n",
      "Batch 1143/1970 | Losses => i: [0.4409] v: [0.2422] t: [0.1396] ivt: [0.0465]\n",
      "Batch 1144/1970 | Losses => i: [0.4497] v: [0.2677] t: [0.1467] ivt: [0.0501]\n",
      "Batch 1145/1970 | Losses => i: [0.4340] v: [0.2375] t: [0.1292] ivt: [0.0457]\n",
      "Batch 1146/1970 | Losses => i: [0.4549] v: [0.2436] t: [0.1567] ivt: [0.0517]\n",
      "Batch 1147/1970 | Losses => i: [0.4720] v: [0.2629] t: [0.1391] ivt: [0.0482]\n",
      "Batch 1148/1970 | Losses => i: [0.4377] v: [0.2477] t: [0.1298] ivt: [0.0436]\n",
      "Batch 1149/1970 | Losses => i: [0.4462] v: [0.2463] t: [0.1330] ivt: [0.0412]\n",
      "Batch 1150/1970 | Losses => i: [0.4585] v: [0.2546] t: [0.1621] ivt: [0.0526]\n",
      "Batch 1151/1970 | Losses => i: [0.4546] v: [0.2391] t: [0.1411] ivt: [0.0420]\n",
      "Batch 1152/1970 | Losses => i: [0.4750] v: [0.2633] t: [0.1354] ivt: [0.0487]\n",
      "Batch 1153/1970 | Losses => i: [0.4583] v: [0.2551] t: [0.1679] ivt: [0.0538]\n",
      "Batch 1154/1970 | Losses => i: [0.4494] v: [0.2433] t: [0.1339] ivt: [0.0480]\n",
      "Batch 1155/1970 | Losses => i: [0.4180] v: [0.2467] t: [0.1435] ivt: [0.0526]\n",
      "Batch 1156/1970 | Losses => i: [0.4354] v: [0.2362] t: [0.1448] ivt: [0.0503]\n",
      "Batch 1157/1970 | Losses => i: [0.4245] v: [0.2223] t: [0.1255] ivt: [0.0445]\n",
      "Batch 1158/1970 | Losses => i: [0.4415] v: [0.2475] t: [0.1419] ivt: [0.0529]\n",
      "Batch 1159/1970 | Losses => i: [0.4242] v: [0.2563] t: [0.1551] ivt: [0.0571]\n",
      "Batch 1160/1970 | Losses => i: [0.4425] v: [0.2761] t: [0.1629] ivt: [0.0497]\n",
      "Batch 1161/1970 | Losses => i: [0.4558] v: [0.2705] t: [0.1680] ivt: [0.0579]\n",
      "Batch 1162/1970 | Losses => i: [0.4429] v: [0.2304] t: [0.1515] ivt: [0.0521]\n",
      "Batch 1163/1970 | Losses => i: [0.4637] v: [0.2833] t: [0.1573] ivt: [0.0577]\n",
      "Batch 1164/1970 | Losses => i: [0.4296] v: [0.2189] t: [0.1310] ivt: [0.0423]\n",
      "Batch 1165/1970 | Losses => i: [0.4444] v: [0.2461] t: [0.1546] ivt: [0.0564]\n",
      "Batch 1166/1970 | Losses => i: [0.4440] v: [0.2240] t: [0.1223] ivt: [0.0478]\n",
      "Batch 1167/1970 | Losses => i: [0.4482] v: [0.2342] t: [0.1305] ivt: [0.0422]\n",
      "Batch 1168/1970 | Losses => i: [0.4663] v: [0.2442] t: [0.1730] ivt: [0.0497]\n",
      "Batch 1169/1970 | Losses => i: [0.4440] v: [0.2233] t: [0.1613] ivt: [0.0491]\n",
      "Batch 1170/1970 | Losses => i: [0.4892] v: [0.2311] t: [0.1438] ivt: [0.0458]\n",
      "Batch 1171/1970 | Losses => i: [0.4309] v: [0.2239] t: [0.1244] ivt: [0.0487]\n",
      "Batch 1172/1970 | Losses => i: [0.4626] v: [0.2315] t: [0.1215] ivt: [0.0462]\n",
      "Batch 1173/1970 | Losses => i: [0.4466] v: [0.2368] t: [0.1361] ivt: [0.0475]\n",
      "Batch 1174/1970 | Losses => i: [0.4629] v: [0.2338] t: [0.1324] ivt: [0.0508]\n",
      "Batch 1175/1970 | Losses => i: [0.4300] v: [0.2430] t: [0.1694] ivt: [0.0613]\n",
      "Batch 1176/1970 | Losses => i: [0.4287] v: [0.2237] t: [0.1164] ivt: [0.0458]\n",
      "Batch 1177/1970 | Losses => i: [0.4633] v: [0.2377] t: [0.1540] ivt: [0.0519]\n",
      "Batch 1178/1970 | Losses => i: [0.4536] v: [0.2491] t: [0.1564] ivt: [0.0527]\n",
      "Batch 1179/1970 | Losses => i: [0.4350] v: [0.2420] t: [0.1519] ivt: [0.0555]\n",
      "Batch 1180/1970 | Losses => i: [0.4482] v: [0.2603] t: [0.1507] ivt: [0.0580]\n",
      "Batch 1181/1970 | Losses => i: [0.4327] v: [0.2289] t: [0.1528] ivt: [0.0545]\n",
      "Batch 1182/1970 | Losses => i: [0.4503] v: [0.2131] t: [0.1614] ivt: [0.0512]\n",
      "Batch 1183/1970 | Losses => i: [0.4489] v: [0.2394] t: [0.1514] ivt: [0.0531]\n",
      "Batch 1184/1970 | Losses => i: [0.4452] v: [0.2300] t: [0.1354] ivt: [0.0481]\n",
      "Batch 1185/1970 | Losses => i: [0.4513] v: [0.2267] t: [0.1414] ivt: [0.0500]\n",
      "Batch 1186/1970 | Losses => i: [0.4852] v: [0.2478] t: [0.1360] ivt: [0.0511]\n",
      "Batch 1187/1970 | Losses => i: [0.4673] v: [0.2288] t: [0.1475] ivt: [0.0491]\n",
      "Batch 1188/1970 | Losses => i: [0.4283] v: [0.2145] t: [0.1152] ivt: [0.0441]\n",
      "Batch 1189/1970 | Losses => i: [0.4504] v: [0.2095] t: [0.1508] ivt: [0.0449]\n",
      "Batch 1190/1970 | Losses => i: [0.4499] v: [0.2495] t: [0.1422] ivt: [0.0468]\n",
      "Batch 1191/1970 | Losses => i: [0.4463] v: [0.2477] t: [0.1439] ivt: [0.0532]\n",
      "Batch 1192/1970 | Losses => i: [0.4312] v: [0.2121] t: [0.1059] ivt: [0.0446]\n",
      "Batch 1193/1970 | Losses => i: [0.4412] v: [0.2518] t: [0.1715] ivt: [0.0532]\n",
      "Batch 1194/1970 | Losses => i: [0.4706] v: [0.2378] t: [0.1362] ivt: [0.0519]\n",
      "Batch 1195/1970 | Losses => i: [0.4498] v: [0.2356] t: [0.1466] ivt: [0.0516]\n",
      "Batch 1196/1970 | Losses => i: [0.4563] v: [0.2199] t: [0.1413] ivt: [0.0486]\n",
      "Batch 1197/1970 | Losses => i: [0.4476] v: [0.2263] t: [0.1621] ivt: [0.0535]\n",
      "Batch 1198/1970 | Losses => i: [0.4328] v: [0.2233] t: [0.1357] ivt: [0.0477]\n",
      "Batch 1199/1970 | Losses => i: [0.4601] v: [0.2336] t: [0.1159] ivt: [0.0481]\n",
      "Batch 1200/1970 | Losses => i: [0.4221] v: [0.2193] t: [0.1406] ivt: [0.0529]\n",
      "Batch 1201/1970 | Losses => i: [0.4488] v: [0.2430] t: [0.1320] ivt: [0.0444]\n",
      "Batch 1202/1970 | Losses => i: [0.4481] v: [0.2243] t: [0.1720] ivt: [0.0516]\n",
      "Batch 1203/1970 | Losses => i: [0.4413] v: [0.2345] t: [0.1597] ivt: [0.0490]\n",
      "Batch 1204/1970 | Losses => i: [0.4386] v: [0.1958] t: [0.1151] ivt: [0.0438]\n",
      "Batch 1205/1970 | Losses => i: [0.4358] v: [0.2240] t: [0.1600] ivt: [0.0581]\n",
      "Batch 1206/1970 | Losses => i: [0.4503] v: [0.2186] t: [0.1256] ivt: [0.0528]\n",
      "Batch 1207/1970 | Losses => i: [0.4144] v: [0.2173] t: [0.1255] ivt: [0.0505]\n",
      "Batch 1208/1970 | Losses => i: [0.4743] v: [0.2291] t: [0.1555] ivt: [0.0535]\n",
      "Batch 1209/1970 | Losses => i: [0.4400] v: [0.2342] t: [0.1291] ivt: [0.0489]\n",
      "Batch 1210/1970 | Losses => i: [0.4304] v: [0.2148] t: [0.1389] ivt: [0.0465]\n",
      "Batch 1211/1970 | Losses => i: [0.4392] v: [0.2162] t: [0.1243] ivt: [0.0504]\n",
      "Batch 1212/1970 | Losses => i: [0.4280] v: [0.2215] t: [0.1257] ivt: [0.0419]\n",
      "Batch 1213/1970 | Losses => i: [0.4498] v: [0.2519] t: [0.1612] ivt: [0.0527]\n",
      "Batch 1214/1970 | Losses => i: [0.4773] v: [0.2353] t: [0.1468] ivt: [0.0516]\n",
      "Batch 1215/1970 | Losses => i: [0.4452] v: [0.2177] t: [0.1326] ivt: [0.0450]\n",
      "Batch 1216/1970 | Losses => i: [0.4241] v: [0.2007] t: [0.1157] ivt: [0.0471]\n",
      "Batch 1217/1970 | Losses => i: [0.4532] v: [0.2524] t: [0.1493] ivt: [0.0572]\n",
      "Batch 1218/1970 | Losses => i: [0.4401] v: [0.2307] t: [0.1284] ivt: [0.0474]\n",
      "Batch 1219/1970 | Losses => i: [0.4334] v: [0.2147] t: [0.1187] ivt: [0.0452]\n",
      "Batch 1220/1970 | Losses => i: [0.4269] v: [0.2210] t: [0.1215] ivt: [0.0479]\n",
      "Batch 1221/1970 | Losses => i: [0.3987] v: [0.2086] t: [0.1105] ivt: [0.0426]\n",
      "Batch 1222/1970 | Losses => i: [0.4452] v: [0.1954] t: [0.0965] ivt: [0.0386]\n",
      "Batch 1223/1970 | Losses => i: [0.4474] v: [0.2014] t: [0.1338] ivt: [0.0445]\n",
      "Batch 1224/1970 | Losses => i: [0.4303] v: [0.2194] t: [0.1296] ivt: [0.0502]\n",
      "Batch 1225/1970 | Losses => i: [0.4539] v: [0.2130] t: [0.1335] ivt: [0.0492]\n",
      "Batch 1226/1970 | Losses => i: [0.4375] v: [0.2133] t: [0.1274] ivt: [0.0483]\n",
      "Batch 1227/1970 | Losses => i: [0.4192] v: [0.2158] t: [0.1469] ivt: [0.0545]\n",
      "Batch 1228/1970 | Losses => i: [0.4502] v: [0.2021] t: [0.1167] ivt: [0.0450]\n",
      "Batch 1229/1970 | Losses => i: [0.4163] v: [0.2131] t: [0.1291] ivt: [0.0525]\n",
      "Batch 1230/1970 | Losses => i: [0.4160] v: [0.2045] t: [0.1216] ivt: [0.0483]\n",
      "Batch 1231/1970 | Losses => i: [0.4335] v: [0.2094] t: [0.1268] ivt: [0.0474]\n",
      "Batch 1232/1970 | Losses => i: [0.4572] v: [0.2239] t: [0.1269] ivt: [0.0492]\n",
      "Batch 1233/1970 | Losses => i: [0.4166] v: [0.1918] t: [0.1146] ivt: [0.0457]\n",
      "Batch 1234/1970 | Losses => i: [0.4565] v: [0.2207] t: [0.1330] ivt: [0.0459]\n",
      "Batch 1235/1970 | Losses => i: [0.4220] v: [0.2327] t: [0.1326] ivt: [0.0532]\n",
      "Batch 1236/1970 | Losses => i: [0.4540] v: [0.2391] t: [0.1381] ivt: [0.0484]\n",
      "Batch 1237/1970 | Losses => i: [0.4231] v: [0.2181] t: [0.1255] ivt: [0.0480]\n",
      "Batch 1238/1970 | Losses => i: [0.4291] v: [0.2139] t: [0.1254] ivt: [0.0442]\n",
      "Batch 1239/1970 | Losses => i: [0.4467] v: [0.2252] t: [0.1272] ivt: [0.0495]\n",
      "Batch 1240/1970 | Losses => i: [0.4481] v: [0.2357] t: [0.1259] ivt: [0.0484]\n",
      "Batch 1241/1970 | Losses => i: [0.4284] v: [0.2224] t: [0.1334] ivt: [0.0484]\n",
      "Batch 1242/1970 | Losses => i: [0.4149] v: [0.1865] t: [0.1165] ivt: [0.0486]\n",
      "Batch 1243/1970 | Losses => i: [0.4025] v: [0.1976] t: [0.1299] ivt: [0.0450]\n",
      "Batch 1244/1970 | Losses => i: [0.4273] v: [0.2573] t: [0.1479] ivt: [0.0564]\n",
      "Batch 1245/1970 | Losses => i: [0.4179] v: [0.2096] t: [0.1377] ivt: [0.0456]\n",
      "Batch 1246/1970 | Losses => i: [0.4412] v: [0.2020] t: [0.1134] ivt: [0.0448]\n",
      "Batch 1247/1970 | Losses => i: [0.4255] v: [0.2247] t: [0.1433] ivt: [0.0526]\n",
      "Batch 1248/1970 | Losses => i: [0.4411] v: [0.2385] t: [0.1377] ivt: [0.0489]\n",
      "Batch 1249/1970 | Losses => i: [0.4588] v: [0.2217] t: [0.1289] ivt: [0.0512]\n",
      "Batch 1250/1970 | Losses => i: [0.4076] v: [0.2014] t: [0.1234] ivt: [0.0520]\n",
      "Batch 1251/1970 | Losses => i: [0.4241] v: [0.2103] t: [0.1472] ivt: [0.0509]\n",
      "Batch 1252/1970 | Losses => i: [0.4359] v: [0.1904] t: [0.1151] ivt: [0.0441]\n",
      "Batch 1253/1970 | Losses => i: [0.4356] v: [0.1938] t: [0.1353] ivt: [0.0470]\n",
      "Batch 1254/1970 | Losses => i: [0.4099] v: [0.2018] t: [0.1180] ivt: [0.0449]\n",
      "Batch 1255/1970 | Losses => i: [0.4385] v: [0.2312] t: [0.1320] ivt: [0.0485]\n",
      "Batch 1256/1970 | Losses => i: [0.3929] v: [0.1898] t: [0.1207] ivt: [0.0462]\n",
      "Batch 1257/1970 | Losses => i: [0.4405] v: [0.1870] t: [0.0936] ivt: [0.0344]\n",
      "Batch 1258/1970 | Losses => i: [0.4210] v: [0.1859] t: [0.1132] ivt: [0.0420]\n",
      "Batch 1259/1970 | Losses => i: [0.4583] v: [0.2038] t: [0.1148] ivt: [0.0438]\n",
      "Batch 1260/1970 | Losses => i: [0.3997] v: [0.2403] t: [0.1074] ivt: [0.0432]\n",
      "Batch 1261/1970 | Losses => i: [0.4529] v: [0.2284] t: [0.1298] ivt: [0.0475]\n",
      "Batch 1262/1970 | Losses => i: [0.4477] v: [0.2240] t: [0.1266] ivt: [0.0513]\n",
      "Batch 1263/1970 | Losses => i: [0.4393] v: [0.2131] t: [0.1400] ivt: [0.0556]\n",
      "Batch 1264/1970 | Losses => i: [0.4254] v: [0.2011] t: [0.1265] ivt: [0.0461]\n",
      "Batch 1265/1970 | Losses => i: [0.4073] v: [0.1932] t: [0.1214] ivt: [0.0419]\n",
      "Batch 1266/1970 | Losses => i: [0.3914] v: [0.1918] t: [0.1562] ivt: [0.0510]\n",
      "Batch 1267/1970 | Losses => i: [0.4238] v: [0.2047] t: [0.1337] ivt: [0.0410]\n",
      "Batch 1268/1970 | Losses => i: [0.4309] v: [0.2415] t: [0.1525] ivt: [0.0510]\n",
      "Batch 1269/1970 | Losses => i: [0.4129] v: [0.2155] t: [0.1203] ivt: [0.0486]\n",
      "Batch 1270/1970 | Losses => i: [0.4366] v: [0.2003] t: [0.1494] ivt: [0.0527]\n",
      "Batch 1271/1970 | Losses => i: [0.4015] v: [0.2359] t: [0.1610] ivt: [0.0544]\n",
      "Batch 1272/1970 | Losses => i: [0.4229] v: [0.2139] t: [0.1220] ivt: [0.0499]\n",
      "Batch 1273/1970 | Losses => i: [0.4171] v: [0.2165] t: [0.1308] ivt: [0.0502]\n",
      "Batch 1274/1970 | Losses => i: [0.4676] v: [0.2346] t: [0.1338] ivt: [0.0482]\n",
      "Batch 1275/1970 | Losses => i: [0.4219] v: [0.1739] t: [0.1087] ivt: [0.0427]\n",
      "Batch 1276/1970 | Losses => i: [0.4027] v: [0.1841] t: [0.1099] ivt: [0.0459]\n",
      "Batch 1277/1970 | Losses => i: [0.4152] v: [0.1991] t: [0.1122] ivt: [0.0418]\n",
      "Batch 1278/1970 | Losses => i: [0.4141] v: [0.1881] t: [0.1273] ivt: [0.0450]\n",
      "Batch 1279/1970 | Losses => i: [0.4548] v: [0.2063] t: [0.1415] ivt: [0.0493]\n",
      "Batch 1280/1970 | Losses => i: [0.4120] v: [0.2027] t: [0.1149] ivt: [0.0460]\n",
      "Batch 1281/1970 | Losses => i: [0.4257] v: [0.2190] t: [0.1569] ivt: [0.0536]\n",
      "Batch 1282/1970 | Losses => i: [0.4274] v: [0.2141] t: [0.1561] ivt: [0.0521]\n",
      "Batch 1283/1970 | Losses => i: [0.4445] v: [0.2193] t: [0.1397] ivt: [0.0551]\n",
      "Batch 1284/1970 | Losses => i: [0.4446] v: [0.2222] t: [0.1311] ivt: [0.0528]\n",
      "Batch 1285/1970 | Losses => i: [0.4001] v: [0.1851] t: [0.1308] ivt: [0.0448]\n",
      "Batch 1286/1970 | Losses => i: [0.4325] v: [0.2318] t: [0.1332] ivt: [0.0561]\n",
      "Batch 1287/1970 | Losses => i: [0.4010] v: [0.2009] t: [0.1342] ivt: [0.0509]\n",
      "Batch 1288/1970 | Losses => i: [0.4202] v: [0.1990] t: [0.1184] ivt: [0.0457]\n",
      "Batch 1289/1970 | Losses => i: [0.4193] v: [0.2488] t: [0.1564] ivt: [0.0559]\n",
      "Batch 1290/1970 | Losses => i: [0.4263] v: [0.1845] t: [0.1332] ivt: [0.0486]\n",
      "Batch 1291/1970 | Losses => i: [0.4354] v: [0.1991] t: [0.1234] ivt: [0.0513]\n",
      "Batch 1292/1970 | Losses => i: [0.4431] v: [0.2118] t: [0.1193] ivt: [0.0461]\n",
      "Batch 1293/1970 | Losses => i: [0.4355] v: [0.2103] t: [0.1110] ivt: [0.0492]\n",
      "Batch 1294/1970 | Losses => i: [0.4275] v: [0.2034] t: [0.1318] ivt: [0.0433]\n",
      "Batch 1295/1970 | Losses => i: [0.4043] v: [0.2048] t: [0.1434] ivt: [0.0441]\n",
      "Batch 1296/1970 | Losses => i: [0.4215] v: [0.1919] t: [0.1334] ivt: [0.0444]\n",
      "Batch 1297/1970 | Losses => i: [0.4349] v: [0.2174] t: [0.1436] ivt: [0.0525]\n",
      "Batch 1298/1970 | Losses => i: [0.3833] v: [0.1745] t: [0.1129] ivt: [0.0460]\n",
      "Batch 1299/1970 | Losses => i: [0.4089] v: [0.1830] t: [0.1549] ivt: [0.0465]\n",
      "Batch 1300/1970 | Losses => i: [0.4401] v: [0.2106] t: [0.1240] ivt: [0.0448]\n",
      "Batch 1301/1970 | Losses => i: [0.4204] v: [0.1781] t: [0.1112] ivt: [0.0396]\n",
      "Batch 1302/1970 | Losses => i: [0.4287] v: [0.1857] t: [0.0963] ivt: [0.0377]\n",
      "Batch 1303/1970 | Losses => i: [0.4421] v: [0.2198] t: [0.1365] ivt: [0.0469]\n",
      "Batch 1304/1970 | Losses => i: [0.4010] v: [0.1995] t: [0.1171] ivt: [0.0477]\n",
      "Batch 1305/1970 | Losses => i: [0.4162] v: [0.1927] t: [0.1319] ivt: [0.0501]\n",
      "Batch 1306/1970 | Losses => i: [0.4261] v: [0.1969] t: [0.1136] ivt: [0.0496]\n",
      "Batch 1307/1970 | Losses => i: [0.4442] v: [0.2151] t: [0.1379] ivt: [0.0522]\n",
      "Batch 1308/1970 | Losses => i: [0.4240] v: [0.2134] t: [0.1336] ivt: [0.0490]\n",
      "Batch 1309/1970 | Losses => i: [0.4178] v: [0.2268] t: [0.1195] ivt: [0.0480]\n",
      "Batch 1310/1970 | Losses => i: [0.4296] v: [0.2081] t: [0.1638] ivt: [0.0520]\n",
      "Batch 1311/1970 | Losses => i: [0.4211] v: [0.2027] t: [0.1449] ivt: [0.0444]\n",
      "Batch 1312/1970 | Losses => i: [0.4427] v: [0.1929] t: [0.1294] ivt: [0.0480]\n",
      "Batch 1313/1970 | Losses => i: [0.4295] v: [0.2124] t: [0.1239] ivt: [0.0501]\n",
      "Batch 1314/1970 | Losses => i: [0.4463] v: [0.2154] t: [0.1334] ivt: [0.0520]\n",
      "Batch 1315/1970 | Losses => i: [0.4103] v: [0.2182] t: [0.1489] ivt: [0.0535]\n",
      "Batch 1316/1970 | Losses => i: [0.4129] v: [0.2065] t: [0.1223] ivt: [0.0511]\n",
      "Batch 1317/1970 | Losses => i: [0.4180] v: [0.2059] t: [0.1646] ivt: [0.0477]\n",
      "Batch 1318/1970 | Losses => i: [0.4427] v: [0.2158] t: [0.1151] ivt: [0.0464]\n",
      "Batch 1319/1970 | Losses => i: [0.4204] v: [0.1958] t: [0.1226] ivt: [0.0511]\n",
      "Batch 1320/1970 | Losses => i: [0.3766] v: [0.1845] t: [0.1553] ivt: [0.0562]\n",
      "Batch 1321/1970 | Losses => i: [0.4095] v: [0.1721] t: [0.1024] ivt: [0.0412]\n",
      "Batch 1322/1970 | Losses => i: [0.4144] v: [0.1976] t: [0.1284] ivt: [0.0435]\n",
      "Batch 1323/1970 | Losses => i: [0.4327] v: [0.1871] t: [0.1490] ivt: [0.0487]\n",
      "Batch 1324/1970 | Losses => i: [0.4241] v: [0.1905] t: [0.1225] ivt: [0.0479]\n",
      "Batch 1325/1970 | Losses => i: [0.4027] v: [0.1885] t: [0.1061] ivt: [0.0421]\n",
      "Batch 1326/1970 | Losses => i: [0.4369] v: [0.2571] t: [0.1338] ivt: [0.0502]\n",
      "Batch 1327/1970 | Losses => i: [0.4054] v: [0.1927] t: [0.1156] ivt: [0.0429]\n",
      "Batch 1328/1970 | Losses => i: [0.4347] v: [0.2160] t: [0.1283] ivt: [0.0524]\n",
      "Batch 1329/1970 | Losses => i: [0.3910] v: [0.2068] t: [0.1370] ivt: [0.0547]\n",
      "Batch 1330/1970 | Losses => i: [0.4219] v: [0.2187] t: [0.1365] ivt: [0.0489]\n",
      "Batch 1331/1970 | Losses => i: [0.4200] v: [0.1854] t: [0.1304] ivt: [0.0521]\n",
      "Batch 1332/1970 | Losses => i: [0.4278] v: [0.2002] t: [0.1396] ivt: [0.0459]\n",
      "Batch 1333/1970 | Losses => i: [0.4200] v: [0.1947] t: [0.1285] ivt: [0.0492]\n",
      "Batch 1334/1970 | Losses => i: [0.4353] v: [0.2229] t: [0.1178] ivt: [0.0506]\n",
      "Batch 1335/1970 | Losses => i: [0.4176] v: [0.1873] t: [0.1524] ivt: [0.0601]\n",
      "Batch 1336/1970 | Losses => i: [0.4240] v: [0.2439] t: [0.1202] ivt: [0.0498]\n",
      "Batch 1337/1970 | Losses => i: [0.4403] v: [0.2176] t: [0.1306] ivt: [0.0463]\n",
      "Batch 1338/1970 | Losses => i: [0.4220] v: [0.1885] t: [0.1518] ivt: [0.0486]\n",
      "Batch 1339/1970 | Losses => i: [0.4311] v: [0.2260] t: [0.1434] ivt: [0.0537]\n",
      "Batch 1340/1970 | Losses => i: [0.4466] v: [0.2321] t: [0.1281] ivt: [0.0521]\n",
      "Batch 1341/1970 | Losses => i: [0.4490] v: [0.2432] t: [0.1392] ivt: [0.0452]\n",
      "Batch 1342/1970 | Losses => i: [0.3846] v: [0.1839] t: [0.1152] ivt: [0.0459]\n",
      "Batch 1343/1970 | Losses => i: [0.4103] v: [0.1885] t: [0.1161] ivt: [0.0503]\n",
      "Batch 1344/1970 | Losses => i: [0.4163] v: [0.1708] t: [0.1351] ivt: [0.0431]\n",
      "Batch 1345/1970 | Losses => i: [0.4184] v: [0.2195] t: [0.1463] ivt: [0.0468]\n",
      "Batch 1346/1970 | Losses => i: [0.4559] v: [0.2267] t: [0.1431] ivt: [0.0492]\n",
      "Batch 1347/1970 | Losses => i: [0.4022] v: [0.1875] t: [0.1111] ivt: [0.0473]\n",
      "Batch 1348/1970 | Losses => i: [0.4061] v: [0.1771] t: [0.0984] ivt: [0.0409]\n",
      "Batch 1349/1970 | Losses => i: [0.4126] v: [0.1729] t: [0.1382] ivt: [0.0539]\n",
      "Batch 1350/1970 | Losses => i: [0.3970] v: [0.1618] t: [0.0968] ivt: [0.0380]\n",
      "Batch 1351/1970 | Losses => i: [0.4007] v: [0.1905] t: [0.1186] ivt: [0.0449]\n",
      "Batch 1352/1970 | Losses => i: [0.4447] v: [0.2067] t: [0.1121] ivt: [0.0456]\n",
      "Batch 1353/1970 | Losses => i: [0.4240] v: [0.2099] t: [0.1348] ivt: [0.0498]\n",
      "Batch 1354/1970 | Losses => i: [0.3948] v: [0.1787] t: [0.1361] ivt: [0.0492]\n",
      "Batch 1355/1970 | Losses => i: [0.4091] v: [0.1986] t: [0.1210] ivt: [0.0496]\n",
      "Batch 1356/1970 | Losses => i: [0.4398] v: [0.2075] t: [0.1264] ivt: [0.0509]\n",
      "Batch 1357/1970 | Losses => i: [0.4166] v: [0.1738] t: [0.1536] ivt: [0.0509]\n",
      "Batch 1358/1970 | Losses => i: [0.4729] v: [0.2304] t: [0.1352] ivt: [0.0535]\n",
      "Batch 1359/1970 | Losses => i: [0.4154] v: [0.2087] t: [0.1379] ivt: [0.0532]\n",
      "Batch 1360/1970 | Losses => i: [0.4143] v: [0.1853] t: [0.1219] ivt: [0.0462]\n",
      "Batch 1361/1970 | Losses => i: [0.4392] v: [0.2069] t: [0.1224] ivt: [0.0482]\n",
      "Batch 1362/1970 | Losses => i: [0.4167] v: [0.1970] t: [0.1254] ivt: [0.0509]\n",
      "Batch 1363/1970 | Losses => i: [0.4260] v: [0.2153] t: [0.1499] ivt: [0.0527]\n",
      "Batch 1364/1970 | Losses => i: [0.4073] v: [0.2194] t: [0.1354] ivt: [0.0514]\n",
      "Batch 1365/1970 | Losses => i: [0.4383] v: [0.2116] t: [0.1477] ivt: [0.0494]\n",
      "Batch 1366/1970 | Losses => i: [0.4052] v: [0.1649] t: [0.1356] ivt: [0.0463]\n",
      "Batch 1367/1970 | Losses => i: [0.4146] v: [0.2161] t: [0.1255] ivt: [0.0491]\n",
      "Batch 1368/1970 | Losses => i: [0.3924] v: [0.1855] t: [0.1236] ivt: [0.0466]\n",
      "Batch 1369/1970 | Losses => i: [0.4050] v: [0.2286] t: [0.1257] ivt: [0.0475]\n",
      "Batch 1370/1970 | Losses => i: [0.4346] v: [0.1980] t: [0.1233] ivt: [0.0454]\n",
      "Batch 1371/1970 | Losses => i: [0.4287] v: [0.2359] t: [0.1287] ivt: [0.0495]\n",
      "Batch 1372/1970 | Losses => i: [0.4010] v: [0.1704] t: [0.1149] ivt: [0.0434]\n",
      "Batch 1373/1970 | Losses => i: [0.4437] v: [0.1706] t: [0.1074] ivt: [0.0495]\n",
      "Batch 1374/1970 | Losses => i: [0.3999] v: [0.1703] t: [0.1449] ivt: [0.0480]\n",
      "Batch 1375/1970 | Losses => i: [0.4082] v: [0.1812] t: [0.1269] ivt: [0.0475]\n",
      "Batch 1376/1970 | Losses => i: [0.3837] v: [0.1661] t: [0.1285] ivt: [0.0476]\n",
      "Batch 1377/1970 | Losses => i: [0.3885] v: [0.1887] t: [0.1451] ivt: [0.0536]\n",
      "Batch 1378/1970 | Losses => i: [0.4314] v: [0.2068] t: [0.1110] ivt: [0.0439]\n",
      "Batch 1379/1970 | Losses => i: [0.4176] v: [0.2054] t: [0.1783] ivt: [0.0515]\n",
      "Batch 1380/1970 | Losses => i: [0.3997] v: [0.2108] t: [0.1577] ivt: [0.0565]\n",
      "Batch 1381/1970 | Losses => i: [0.4169] v: [0.2043] t: [0.1530] ivt: [0.0506]\n",
      "Batch 1382/1970 | Losses => i: [0.3956] v: [0.1746] t: [0.0942] ivt: [0.0431]\n",
      "Batch 1383/1970 | Losses => i: [0.4322] v: [0.1985] t: [0.1176] ivt: [0.0447]\n",
      "Batch 1384/1970 | Losses => i: [0.4471] v: [0.2027] t: [0.1247] ivt: [0.0459]\n",
      "Batch 1385/1970 | Losses => i: [0.4210] v: [0.2226] t: [0.1383] ivt: [0.0510]\n",
      "Batch 1386/1970 | Losses => i: [0.4265] v: [0.1754] t: [0.1234] ivt: [0.0497]\n",
      "Batch 1387/1970 | Losses => i: [0.4126] v: [0.1729] t: [0.1359] ivt: [0.0474]\n",
      "Batch 1388/1970 | Losses => i: [0.4219] v: [0.1894] t: [0.1054] ivt: [0.0470]\n",
      "Batch 1389/1970 | Losses => i: [0.4456] v: [0.2040] t: [0.1422] ivt: [0.0503]\n",
      "Batch 1390/1970 | Losses => i: [0.4194] v: [0.2143] t: [0.1251] ivt: [0.0552]\n",
      "Batch 1391/1970 | Losses => i: [0.4121] v: [0.2133] t: [0.1368] ivt: [0.0539]\n",
      "Batch 1392/1970 | Losses => i: [0.4107] v: [0.1704] t: [0.1222] ivt: [0.0450]\n",
      "Batch 1393/1970 | Losses => i: [0.4344] v: [0.2007] t: [0.1435] ivt: [0.0531]\n",
      "Batch 1394/1970 | Losses => i: [0.3835] v: [0.1858] t: [0.1202] ivt: [0.0516]\n",
      "Batch 1395/1970 | Losses => i: [0.4598] v: [0.2183] t: [0.1218] ivt: [0.0488]\n",
      "Batch 1396/1970 | Losses => i: [0.4236] v: [0.2258] t: [0.1374] ivt: [0.0460]\n",
      "Batch 1397/1970 | Losses => i: [0.3955] v: [0.2038] t: [0.1316] ivt: [0.0518]\n",
      "Batch 1398/1970 | Losses => i: [0.4029] v: [0.1590] t: [0.1229] ivt: [0.0402]\n",
      "Batch 1399/1970 | Losses => i: [0.4286] v: [0.1933] t: [0.1182] ivt: [0.0486]\n",
      "Batch 1400/1970 | Losses => i: [0.4480] v: [0.2018] t: [0.1186] ivt: [0.0463]\n",
      "Batch 1401/1970 | Losses => i: [0.4361] v: [0.2074] t: [0.1125] ivt: [0.0438]\n",
      "Batch 1402/1970 | Losses => i: [0.4060] v: [0.2122] t: [0.1282] ivt: [0.0467]\n",
      "Batch 1403/1970 | Losses => i: [0.4461] v: [0.1931] t: [0.1287] ivt: [0.0452]\n",
      "Batch 1404/1970 | Losses => i: [0.4056] v: [0.1895] t: [0.1174] ivt: [0.0482]\n",
      "Batch 1405/1970 | Losses => i: [0.4237] v: [0.2265] t: [0.1388] ivt: [0.0518]\n",
      "Batch 1406/1970 | Losses => i: [0.4302] v: [0.2063] t: [0.1700] ivt: [0.0556]\n",
      "Batch 1407/1970 | Losses => i: [0.4106] v: [0.1975] t: [0.1131] ivt: [0.0466]\n",
      "Batch 1408/1970 | Losses => i: [0.4136] v: [0.1899] t: [0.1405] ivt: [0.0456]\n",
      "Batch 1409/1970 | Losses => i: [0.4278] v: [0.1941] t: [0.1070] ivt: [0.0451]\n",
      "Batch 1410/1970 | Losses => i: [0.3874] v: [0.1801] t: [0.1170] ivt: [0.0430]\n",
      "Batch 1411/1970 | Losses => i: [0.4367] v: [0.2218] t: [0.1514] ivt: [0.0515]\n",
      "Batch 1412/1970 | Losses => i: [0.4002] v: [0.1600] t: [0.1055] ivt: [0.0402]\n",
      "Batch 1413/1970 | Losses => i: [0.4168] v: [0.1632] t: [0.1016] ivt: [0.0443]\n",
      "Batch 1414/1970 | Losses => i: [0.3970] v: [0.1844] t: [0.1272] ivt: [0.0460]\n",
      "Batch 1415/1970 | Losses => i: [0.3950] v: [0.1995] t: [0.1409] ivt: [0.0526]\n",
      "Batch 1416/1970 | Losses => i: [0.4472] v: [0.2179] t: [0.1278] ivt: [0.0531]\n",
      "Batch 1417/1970 | Losses => i: [0.3919] v: [0.1997] t: [0.1344] ivt: [0.0480]\n",
      "Batch 1418/1970 | Losses => i: [0.4110] v: [0.2316] t: [0.1178] ivt: [0.0467]\n",
      "Batch 1419/1970 | Losses => i: [0.4164] v: [0.1943] t: [0.1087] ivt: [0.0407]\n",
      "Batch 1420/1970 | Losses => i: [0.3973] v: [0.2420] t: [0.1585] ivt: [0.0542]\n",
      "Batch 1421/1970 | Losses => i: [0.4321] v: [0.1872] t: [0.1425] ivt: [0.0445]\n",
      "Batch 1422/1970 | Losses => i: [0.4083] v: [0.1871] t: [0.1163] ivt: [0.0561]\n",
      "Batch 1423/1970 | Losses => i: [0.4182] v: [0.1990] t: [0.1391] ivt: [0.0493]\n",
      "Batch 1424/1970 | Losses => i: [0.4144] v: [0.1750] t: [0.1531] ivt: [0.0488]\n",
      "Batch 1425/1970 | Losses => i: [0.3948] v: [0.1662] t: [0.1066] ivt: [0.0427]\n",
      "Batch 1426/1970 | Losses => i: [0.4098] v: [0.1992] t: [0.1262] ivt: [0.0481]\n",
      "Batch 1427/1970 | Losses => i: [0.3618] v: [0.1874] t: [0.1085] ivt: [0.0420]\n",
      "Batch 1428/1970 | Losses => i: [0.4066] v: [0.1932] t: [0.1174] ivt: [0.0421]\n",
      "Batch 1429/1970 | Losses => i: [0.4276] v: [0.2249] t: [0.1420] ivt: [0.0500]\n",
      "Batch 1430/1970 | Losses => i: [0.3960] v: [0.1570] t: [0.1053] ivt: [0.0390]\n",
      "Batch 1431/1970 | Losses => i: [0.3875] v: [0.1842] t: [0.1184] ivt: [0.0450]\n",
      "Batch 1432/1970 | Losses => i: [0.3932] v: [0.1842] t: [0.1066] ivt: [0.0364]\n",
      "Batch 1433/1970 | Losses => i: [0.4123] v: [0.2036] t: [0.1423] ivt: [0.0482]\n",
      "Batch 1434/1970 | Losses => i: [0.4088] v: [0.1802] t: [0.1161] ivt: [0.0451]\n",
      "Batch 1435/1970 | Losses => i: [0.4038] v: [0.2046] t: [0.1377] ivt: [0.0488]\n",
      "Batch 1436/1970 | Losses => i: [0.3976] v: [0.1723] t: [0.1308] ivt: [0.0476]\n",
      "Batch 1437/1970 | Losses => i: [0.4292] v: [0.1947] t: [0.1438] ivt: [0.0504]\n",
      "Batch 1438/1970 | Losses => i: [0.4170] v: [0.1660] t: [0.1308] ivt: [0.0514]\n",
      "Batch 1439/1970 | Losses => i: [0.4300] v: [0.1709] t: [0.1142] ivt: [0.0464]\n",
      "Batch 1440/1970 | Losses => i: [0.4251] v: [0.2040] t: [0.1385] ivt: [0.0506]\n",
      "Batch 1441/1970 | Losses => i: [0.4160] v: [0.1611] t: [0.0813] ivt: [0.0366]\n",
      "Batch 1442/1970 | Losses => i: [0.4222] v: [0.2155] t: [0.1241] ivt: [0.0511]\n",
      "Batch 1443/1970 | Losses => i: [0.3981] v: [0.1891] t: [0.1272] ivt: [0.0459]\n",
      "Batch 1444/1970 | Losses => i: [0.4223] v: [0.2094] t: [0.1643] ivt: [0.0509]\n",
      "Batch 1445/1970 | Losses => i: [0.4125] v: [0.1983] t: [0.1086] ivt: [0.0502]\n",
      "Batch 1446/1970 | Losses => i: [0.4631] v: [0.2302] t: [0.1382] ivt: [0.0531]\n",
      "Batch 1447/1970 | Losses => i: [0.4054] v: [0.1742] t: [0.1248] ivt: [0.0487]\n",
      "Batch 1448/1970 | Losses => i: [0.3993] v: [0.2231] t: [0.1474] ivt: [0.0562]\n",
      "Batch 1449/1970 | Losses => i: [0.4269] v: [0.1978] t: [0.1664] ivt: [0.0464]\n",
      "Batch 1450/1970 | Losses => i: [0.4264] v: [0.2016] t: [0.1272] ivt: [0.0492]\n",
      "Batch 1451/1970 | Losses => i: [0.4141] v: [0.2060] t: [0.1348] ivt: [0.0491]\n",
      "Batch 1452/1970 | Losses => i: [0.4020] v: [0.1801] t: [0.1142] ivt: [0.0461]\n",
      "Batch 1453/1970 | Losses => i: [0.4017] v: [0.1795] t: [0.1344] ivt: [0.0435]\n",
      "Batch 1454/1970 | Losses => i: [0.3900] v: [0.2171] t: [0.1306] ivt: [0.0451]\n",
      "Batch 1455/1970 | Losses => i: [0.4212] v: [0.2118] t: [0.1170] ivt: [0.0526]\n",
      "Batch 1456/1970 | Losses => i: [0.4135] v: [0.1808] t: [0.1438] ivt: [0.0478]\n",
      "Batch 1457/1970 | Losses => i: [0.3940] v: [0.1763] t: [0.1178] ivt: [0.0417]\n",
      "Batch 1458/1970 | Losses => i: [0.4087] v: [0.1803] t: [0.1085] ivt: [0.0407]\n",
      "Batch 1459/1970 | Losses => i: [0.4312] v: [0.1915] t: [0.1468] ivt: [0.0461]\n",
      "Batch 1460/1970 | Losses => i: [0.4178] v: [0.1808] t: [0.1131] ivt: [0.0478]\n",
      "Batch 1461/1970 | Losses => i: [0.3878] v: [0.1704] t: [0.1052] ivt: [0.0452]\n",
      "Batch 1462/1970 | Losses => i: [0.4237] v: [0.1941] t: [0.1146] ivt: [0.0487]\n",
      "Batch 1463/1970 | Losses => i: [0.4040] v: [0.2028] t: [0.1449] ivt: [0.0531]\n",
      "Batch 1464/1970 | Losses => i: [0.3906] v: [0.1736] t: [0.1046] ivt: [0.0417]\n",
      "Batch 1465/1970 | Losses => i: [0.4505] v: [0.2105] t: [0.1379] ivt: [0.0460]\n",
      "Batch 1466/1970 | Losses => i: [0.4101] v: [0.1679] t: [0.1495] ivt: [0.0445]\n",
      "Batch 1467/1970 | Losses => i: [0.4095] v: [0.1950] t: [0.1431] ivt: [0.0448]\n",
      "Batch 1468/1970 | Losses => i: [0.4435] v: [0.2426] t: [0.1344] ivt: [0.0534]\n",
      "Batch 1469/1970 | Losses => i: [0.4020] v: [0.1764] t: [0.0980] ivt: [0.0427]\n",
      "Batch 1470/1970 | Losses => i: [0.4062] v: [0.2066] t: [0.1171] ivt: [0.0462]\n",
      "Batch 1471/1970 | Losses => i: [0.4318] v: [0.1869] t: [0.1492] ivt: [0.0535]\n",
      "Batch 1472/1970 | Losses => i: [0.3850] v: [0.1640] t: [0.1041] ivt: [0.0411]\n",
      "Batch 1473/1970 | Losses => i: [0.4073] v: [0.1979] t: [0.1335] ivt: [0.0486]\n",
      "Batch 1474/1970 | Losses => i: [0.4050] v: [0.1942] t: [0.1292] ivt: [0.0452]\n",
      "Batch 1475/1970 | Losses => i: [0.4299] v: [0.2566] t: [0.1445] ivt: [0.0591]\n",
      "Batch 1476/1970 | Losses => i: [0.4047] v: [0.1814] t: [0.1210] ivt: [0.0492]\n",
      "Batch 1477/1970 | Losses => i: [0.4102] v: [0.1779] t: [0.1260] ivt: [0.0503]\n",
      "Batch 1478/1970 | Losses => i: [0.4218] v: [0.2638] t: [0.1490] ivt: [0.0488]\n",
      "Batch 1479/1970 | Losses => i: [0.4446] v: [0.2093] t: [0.1154] ivt: [0.0530]\n",
      "Batch 1480/1970 | Losses => i: [0.4237] v: [0.2040] t: [0.1619] ivt: [0.0589]\n",
      "Batch 1481/1970 | Losses => i: [0.4017] v: [0.1891] t: [0.1172] ivt: [0.0462]\n",
      "Batch 1482/1970 | Losses => i: [0.3962] v: [0.1677] t: [0.1269] ivt: [0.0519]\n",
      "Batch 1483/1970 | Losses => i: [0.4236] v: [0.1988] t: [0.1389] ivt: [0.0522]\n",
      "Batch 1484/1970 | Losses => i: [0.4049] v: [0.1702] t: [0.1224] ivt: [0.0443]\n",
      "Batch 1485/1970 | Losses => i: [0.4055] v: [0.1810] t: [0.1497] ivt: [0.0506]\n",
      "Batch 1486/1970 | Losses => i: [0.4098] v: [0.2072] t: [0.1393] ivt: [0.0505]\n",
      "Batch 1487/1970 | Losses => i: [0.4043] v: [0.2052] t: [0.1209] ivt: [0.0481]\n",
      "Batch 1488/1970 | Losses => i: [0.4196] v: [0.1963] t: [0.1521] ivt: [0.0466]\n",
      "Batch 1489/1970 | Losses => i: [0.4135] v: [0.2116] t: [0.1319] ivt: [0.0464]\n",
      "Batch 1490/1970 | Losses => i: [0.4176] v: [0.2214] t: [0.1518] ivt: [0.0516]\n",
      "Batch 1491/1970 | Losses => i: [0.4054] v: [0.1995] t: [0.1269] ivt: [0.0546]\n",
      "Batch 1492/1970 | Losses => i: [0.4311] v: [0.2112] t: [0.1435] ivt: [0.0477]\n",
      "Batch 1493/1970 | Losses => i: [0.3837] v: [0.1449] t: [0.1000] ivt: [0.0403]\n",
      "Batch 1494/1970 | Losses => i: [0.3876] v: [0.1840] t: [0.1364] ivt: [0.0458]\n",
      "Batch 1495/1970 | Losses => i: [0.4033] v: [0.2252] t: [0.1464] ivt: [0.0493]\n",
      "Batch 1496/1970 | Losses => i: [0.4116] v: [0.2206] t: [0.1408] ivt: [0.0500]\n",
      "Batch 1497/1970 | Losses => i: [0.4034] v: [0.1943] t: [0.1141] ivt: [0.0478]\n",
      "Batch 1498/1970 | Losses => i: [0.4572] v: [0.2110] t: [0.1256] ivt: [0.0453]\n",
      "Batch 1499/1970 | Losses => i: [0.3920] v: [0.1593] t: [0.1181] ivt: [0.0495]\n",
      "Batch 1500/1970 | Losses => i: [0.3850] v: [0.1891] t: [0.1004] ivt: [0.0426]\n",
      "Batch 1501/1970 | Losses => i: [0.4284] v: [0.1885] t: [0.1080] ivt: [0.0480]\n",
      "Batch 1502/1970 | Losses => i: [0.3884] v: [0.1960] t: [0.1575] ivt: [0.0512]\n",
      "Batch 1503/1970 | Losses => i: [0.4036] v: [0.2097] t: [0.1610] ivt: [0.0527]\n",
      "Batch 1504/1970 | Losses => i: [0.3867] v: [0.1797] t: [0.1041] ivt: [0.0446]\n",
      "Batch 1505/1970 | Losses => i: [0.3977] v: [0.1907] t: [0.1208] ivt: [0.0500]\n",
      "Batch 1506/1970 | Losses => i: [0.3931] v: [0.1671] t: [0.0997] ivt: [0.0428]\n",
      "Batch 1507/1970 | Losses => i: [0.4213] v: [0.2262] t: [0.1328] ivt: [0.0532]\n",
      "Batch 1508/1970 | Losses => i: [0.4115] v: [0.1802] t: [0.1475] ivt: [0.0526]\n",
      "Batch 1509/1970 | Losses => i: [0.4135] v: [0.2013] t: [0.1290] ivt: [0.0517]\n",
      "Batch 1510/1970 | Losses => i: [0.4106] v: [0.2319] t: [0.1381] ivt: [0.0552]\n",
      "Batch 1511/1970 | Losses => i: [0.4173] v: [0.2216] t: [0.1231] ivt: [0.0499]\n",
      "Batch 1512/1970 | Losses => i: [0.4250] v: [0.2039] t: [0.1264] ivt: [0.0459]\n",
      "Batch 1513/1970 | Losses => i: [0.4084] v: [0.1831] t: [0.1304] ivt: [0.0492]\n",
      "Batch 1514/1970 | Losses => i: [0.3901] v: [0.1894] t: [0.1175] ivt: [0.0437]\n",
      "Batch 1515/1970 | Losses => i: [0.3798] v: [0.1893] t: [0.1200] ivt: [0.0428]\n",
      "Batch 1516/1970 | Losses => i: [0.4103] v: [0.1912] t: [0.1068] ivt: [0.0444]\n",
      "Batch 1517/1970 | Losses => i: [0.4417] v: [0.2416] t: [0.1650] ivt: [0.0510]\n",
      "Batch 1518/1970 | Losses => i: [0.3614] v: [0.1741] t: [0.1100] ivt: [0.0472]\n",
      "Batch 1519/1970 | Losses => i: [0.4094] v: [0.1975] t: [0.1328] ivt: [0.0519]\n",
      "Batch 1520/1970 | Losses => i: [0.4140] v: [0.2270] t: [0.1361] ivt: [0.0501]\n",
      "Batch 1521/1970 | Losses => i: [0.4149] v: [0.1998] t: [0.1196] ivt: [0.0437]\n",
      "Batch 1522/1970 | Losses => i: [0.3949] v: [0.2225] t: [0.1609] ivt: [0.0578]\n",
      "Batch 1523/1970 | Losses => i: [0.3884] v: [0.2238] t: [0.1338] ivt: [0.0558]\n",
      "Batch 1524/1970 | Losses => i: [0.3883] v: [0.2056] t: [0.1387] ivt: [0.0455]\n",
      "Batch 1525/1970 | Losses => i: [0.3946] v: [0.1795] t: [0.1057] ivt: [0.0455]\n",
      "Batch 1526/1970 | Losses => i: [0.4028] v: [0.1882] t: [0.1478] ivt: [0.0535]\n",
      "Batch 1527/1970 | Losses => i: [0.4277] v: [0.2277] t: [0.1527] ivt: [0.0492]\n",
      "Batch 1528/1970 | Losses => i: [0.4113] v: [0.2342] t: [0.1254] ivt: [0.0448]\n",
      "Batch 1529/1970 | Losses => i: [0.4092] v: [0.2231] t: [0.1544] ivt: [0.0519]\n",
      "Batch 1530/1970 | Losses => i: [0.4209] v: [0.2066] t: [0.1435] ivt: [0.0556]\n",
      "Batch 1531/1970 | Losses => i: [0.4201] v: [0.2005] t: [0.1334] ivt: [0.0476]\n",
      "Batch 1532/1970 | Losses => i: [0.4077] v: [0.1965] t: [0.1171] ivt: [0.0519]\n",
      "Batch 1533/1970 | Losses => i: [0.3897] v: [0.2422] t: [0.1524] ivt: [0.0507]\n",
      "Batch 1534/1970 | Losses => i: [0.3580] v: [0.1802] t: [0.1104] ivt: [0.0455]\n",
      "Batch 1535/1970 | Losses => i: [0.3726] v: [0.1775] t: [0.1233] ivt: [0.0428]\n",
      "Batch 1536/1970 | Losses => i: [0.4160] v: [0.2007] t: [0.1536] ivt: [0.0536]\n",
      "Batch 1537/1970 | Losses => i: [0.3641] v: [0.2160] t: [0.1304] ivt: [0.0493]\n",
      "Batch 1538/1970 | Losses => i: [0.4227] v: [0.2133] t: [0.1663] ivt: [0.0566]\n",
      "Batch 1539/1970 | Losses => i: [0.3853] v: [0.1822] t: [0.1233] ivt: [0.0464]\n",
      "Batch 1540/1970 | Losses => i: [0.3983] v: [0.2131] t: [0.1475] ivt: [0.0542]\n",
      "Batch 1541/1970 | Losses => i: [0.4162] v: [0.2493] t: [0.1672] ivt: [0.0552]\n",
      "Batch 1542/1970 | Losses => i: [0.4135] v: [0.1966] t: [0.1159] ivt: [0.0451]\n",
      "Batch 1543/1970 | Losses => i: [0.3691] v: [0.1584] t: [0.1034] ivt: [0.0417]\n",
      "Batch 1544/1970 | Losses => i: [0.3901] v: [0.2043] t: [0.1276] ivt: [0.0506]\n",
      "Batch 1545/1970 | Losses => i: [0.3802] v: [0.2152] t: [0.1472] ivt: [0.0524]\n",
      "Batch 1546/1970 | Losses => i: [0.3844] v: [0.1796] t: [0.1107] ivt: [0.0441]\n",
      "Batch 1547/1970 | Losses => i: [0.4071] v: [0.2107] t: [0.1679] ivt: [0.0521]\n",
      "Batch 1548/1970 | Losses => i: [0.4221] v: [0.1945] t: [0.1287] ivt: [0.0481]\n",
      "Batch 1549/1970 | Losses => i: [0.4091] v: [0.2159] t: [0.1466] ivt: [0.0555]\n",
      "Batch 1550/1970 | Losses => i: [0.4117] v: [0.2026] t: [0.1303] ivt: [0.0480]\n",
      "Batch 1551/1970 | Losses => i: [0.3972] v: [0.1890] t: [0.1259] ivt: [0.0541]\n",
      "Batch 1552/1970 | Losses => i: [0.4334] v: [0.2241] t: [0.1274] ivt: [0.0487]\n",
      "Batch 1553/1970 | Losses => i: [0.3814] v: [0.2093] t: [0.1355] ivt: [0.0512]\n",
      "Batch 1554/1970 | Losses => i: [0.4231] v: [0.2008] t: [0.1277] ivt: [0.0497]\n",
      "Batch 1555/1970 | Losses => i: [0.4350] v: [0.1998] t: [0.1305] ivt: [0.0509]\n",
      "Batch 1556/1970 | Losses => i: [0.4142] v: [0.1919] t: [0.1332] ivt: [0.0475]\n",
      "Batch 1557/1970 | Losses => i: [0.4084] v: [0.1903] t: [0.1043] ivt: [0.0422]\n",
      "Batch 1558/1970 | Losses => i: [0.3840] v: [0.2184] t: [0.1355] ivt: [0.0471]\n",
      "Batch 1559/1970 | Losses => i: [0.3980] v: [0.2057] t: [0.1199] ivt: [0.0460]\n",
      "Batch 1560/1970 | Losses => i: [0.3914] v: [0.2166] t: [0.1253] ivt: [0.0496]\n",
      "Batch 1561/1970 | Losses => i: [0.4309] v: [0.1945] t: [0.1324] ivt: [0.0440]\n",
      "Batch 1562/1970 | Losses => i: [0.4267] v: [0.1759] t: [0.1063] ivt: [0.0458]\n",
      "Batch 1563/1970 | Losses => i: [0.4025] v: [0.1739] t: [0.0999] ivt: [0.0420]\n",
      "Batch 1564/1970 | Losses => i: [0.4116] v: [0.2015] t: [0.1305] ivt: [0.0449]\n",
      "Batch 1565/1970 | Losses => i: [0.3934] v: [0.1739] t: [0.1099] ivt: [0.0448]\n",
      "Batch 1566/1970 | Losses => i: [0.3640] v: [0.1576] t: [0.1112] ivt: [0.0421]\n",
      "Batch 1567/1970 | Losses => i: [0.3822] v: [0.1844] t: [0.1109] ivt: [0.0417]\n",
      "Batch 1568/1970 | Losses => i: [0.3872] v: [0.2050] t: [0.1318] ivt: [0.0481]\n",
      "Batch 1569/1970 | Losses => i: [0.3867] v: [0.1768] t: [0.1282] ivt: [0.0488]\n",
      "Batch 1570/1970 | Losses => i: [0.4166] v: [0.1852] t: [0.1271] ivt: [0.0464]\n",
      "Batch 1571/1970 | Losses => i: [0.3808] v: [0.2165] t: [0.1187] ivt: [0.0434]\n",
      "Batch 1572/1970 | Losses => i: [0.3876] v: [0.1835] t: [0.1424] ivt: [0.0501]\n",
      "Batch 1573/1970 | Losses => i: [0.3858] v: [0.1703] t: [0.0985] ivt: [0.0392]\n",
      "Batch 1574/1970 | Losses => i: [0.4215] v: [0.2185] t: [0.1292] ivt: [0.0505]\n",
      "Batch 1575/1970 | Losses => i: [0.4390] v: [0.2259] t: [0.1315] ivt: [0.0504]\n",
      "Batch 1576/1970 | Losses => i: [0.3395] v: [0.1864] t: [0.1328] ivt: [0.0486]\n",
      "Batch 1577/1970 | Losses => i: [0.4053] v: [0.1930] t: [0.1162] ivt: [0.0458]\n",
      "Batch 1578/1970 | Losses => i: [0.3524] v: [0.1824] t: [0.1113] ivt: [0.0499]\n",
      "Batch 1579/1970 | Losses => i: [0.4435] v: [0.2316] t: [0.1583] ivt: [0.0536]\n",
      "Batch 1580/1970 | Losses => i: [0.4025] v: [0.1689] t: [0.0963] ivt: [0.0423]\n",
      "Batch 1581/1970 | Losses => i: [0.3842] v: [0.2359] t: [0.1364] ivt: [0.0497]\n",
      "Batch 1582/1970 | Losses => i: [0.3661] v: [0.1738] t: [0.1320] ivt: [0.0553]\n",
      "Batch 1583/1970 | Losses => i: [0.3846] v: [0.2392] t: [0.1269] ivt: [0.0537]\n",
      "Batch 1584/1970 | Losses => i: [0.3855] v: [0.1750] t: [0.1331] ivt: [0.0476]\n",
      "Batch 1585/1970 | Losses => i: [0.3893] v: [0.1563] t: [0.1113] ivt: [0.0411]\n",
      "Batch 1586/1970 | Losses => i: [0.3665] v: [0.1774] t: [0.1518] ivt: [0.0469]\n",
      "Batch 1587/1970 | Losses => i: [0.3845] v: [0.1826] t: [0.1185] ivt: [0.0453]\n",
      "Batch 1588/1970 | Losses => i: [0.4498] v: [0.2424] t: [0.1773] ivt: [0.0547]\n",
      "Batch 1589/1970 | Losses => i: [0.3984] v: [0.1896] t: [0.1267] ivt: [0.0477]\n",
      "Batch 1590/1970 | Losses => i: [0.3680] v: [0.1972] t: [0.1233] ivt: [0.0428]\n",
      "Batch 1591/1970 | Losses => i: [0.3877] v: [0.2076] t: [0.1135] ivt: [0.0452]\n",
      "Batch 1592/1970 | Losses => i: [0.3937] v: [0.1889] t: [0.1115] ivt: [0.0470]\n",
      "Batch 1593/1970 | Losses => i: [0.3698] v: [0.1665] t: [0.1216] ivt: [0.0487]\n",
      "Batch 1594/1970 | Losses => i: [0.3803] v: [0.2166] t: [0.1310] ivt: [0.0501]\n",
      "Batch 1595/1970 | Losses => i: [0.4510] v: [0.2068] t: [0.1268] ivt: [0.0482]\n",
      "Batch 1596/1970 | Losses => i: [0.4137] v: [0.2285] t: [0.1687] ivt: [0.0529]\n",
      "Batch 1597/1970 | Losses => i: [0.4128] v: [0.1833] t: [0.1386] ivt: [0.0467]\n",
      "Batch 1598/1970 | Losses => i: [0.3533] v: [0.2014] t: [0.1234] ivt: [0.0500]\n",
      "Batch 1599/1970 | Losses => i: [0.4198] v: [0.2220] t: [0.1357] ivt: [0.0532]\n",
      "Batch 1600/1970 | Losses => i: [0.4077] v: [0.2221] t: [0.1212] ivt: [0.0474]\n",
      "Batch 1601/1970 | Losses => i: [0.4193] v: [0.1784] t: [0.1200] ivt: [0.0489]\n",
      "Batch 1602/1970 | Losses => i: [0.4184] v: [0.2108] t: [0.1160] ivt: [0.0475]\n",
      "Batch 1603/1970 | Losses => i: [0.3508] v: [0.1890] t: [0.1467] ivt: [0.0518]\n",
      "Batch 1604/1970 | Losses => i: [0.4259] v: [0.2219] t: [0.1331] ivt: [0.0523]\n",
      "Batch 1605/1970 | Losses => i: [0.3868] v: [0.1621] t: [0.1070] ivt: [0.0433]\n",
      "Batch 1606/1970 | Losses => i: [0.3728] v: [0.1803] t: [0.1354] ivt: [0.0437]\n",
      "Batch 1607/1970 | Losses => i: [0.4201] v: [0.2230] t: [0.1748] ivt: [0.0588]\n",
      "Batch 1608/1970 | Losses => i: [0.4009] v: [0.2028] t: [0.1247] ivt: [0.0447]\n",
      "Batch 1609/1970 | Losses => i: [0.4007] v: [0.1755] t: [0.1331] ivt: [0.0482]\n",
      "Batch 1610/1970 | Losses => i: [0.3585] v: [0.1581] t: [0.0960] ivt: [0.0440]\n",
      "Batch 1611/1970 | Losses => i: [0.3780] v: [0.1630] t: [0.1526] ivt: [0.0458]\n",
      "Batch 1612/1970 | Losses => i: [0.3689] v: [0.1520] t: [0.0929] ivt: [0.0359]\n",
      "Batch 1613/1970 | Losses => i: [0.3896] v: [0.1980] t: [0.1077] ivt: [0.0504]\n",
      "Batch 1614/1970 | Losses => i: [0.3913] v: [0.2204] t: [0.1430] ivt: [0.0545]\n",
      "Batch 1615/1970 | Losses => i: [0.3581] v: [0.1850] t: [0.1242] ivt: [0.0485]\n",
      "Batch 1616/1970 | Losses => i: [0.3795] v: [0.2121] t: [0.1242] ivt: [0.0482]\n",
      "Batch 1617/1970 | Losses => i: [0.4176] v: [0.2312] t: [0.1575] ivt: [0.0567]\n",
      "Batch 1618/1970 | Losses => i: [0.4190] v: [0.1889] t: [0.1455] ivt: [0.0535]\n",
      "Batch 1619/1970 | Losses => i: [0.3704] v: [0.2006] t: [0.1216] ivt: [0.0450]\n",
      "Batch 1620/1970 | Losses => i: [0.4051] v: [0.2435] t: [0.1546] ivt: [0.0543]\n",
      "Batch 1621/1970 | Losses => i: [0.3722] v: [0.1809] t: [0.1128] ivt: [0.0421]\n",
      "Batch 1622/1970 | Losses => i: [0.3882] v: [0.1905] t: [0.1074] ivt: [0.0373]\n",
      "Batch 1623/1970 | Losses => i: [0.3913] v: [0.1786] t: [0.1517] ivt: [0.0503]\n",
      "Batch 1624/1970 | Losses => i: [0.4012] v: [0.1902] t: [0.1340] ivt: [0.0461]\n",
      "Batch 1625/1970 | Losses => i: [0.3837] v: [0.1968] t: [0.1345] ivt: [0.0496]\n",
      "Batch 1626/1970 | Losses => i: [0.3872] v: [0.2112] t: [0.1340] ivt: [0.0450]\n",
      "Batch 1627/1970 | Losses => i: [0.4099] v: [0.1986] t: [0.1365] ivt: [0.0499]\n",
      "Batch 1628/1970 | Losses => i: [0.3950] v: [0.2034] t: [0.1359] ivt: [0.0520]\n",
      "Batch 1629/1970 | Losses => i: [0.4050] v: [0.2215] t: [0.1189] ivt: [0.0462]\n",
      "Batch 1630/1970 | Losses => i: [0.3805] v: [0.2309] t: [0.1500] ivt: [0.0549]\n",
      "Batch 1631/1970 | Losses => i: [0.3842] v: [0.2368] t: [0.1267] ivt: [0.0503]\n",
      "Batch 1632/1970 | Losses => i: [0.4096] v: [0.1949] t: [0.1097] ivt: [0.0450]\n",
      "Batch 1633/1970 | Losses => i: [0.3728] v: [0.1738] t: [0.1023] ivt: [0.0436]\n",
      "Batch 1634/1970 | Losses => i: [0.3778] v: [0.1637] t: [0.1233] ivt: [0.0485]\n",
      "Batch 1635/1970 | Losses => i: [0.4020] v: [0.1819] t: [0.1357] ivt: [0.0476]\n",
      "Batch 1636/1970 | Losses => i: [0.4114] v: [0.1934] t: [0.1482] ivt: [0.0469]\n",
      "Batch 1637/1970 | Losses => i: [0.3956] v: [0.1695] t: [0.0930] ivt: [0.0401]\n",
      "Batch 1638/1970 | Losses => i: [0.4057] v: [0.2357] t: [0.1213] ivt: [0.0481]\n",
      "Batch 1639/1970 | Losses => i: [0.3756] v: [0.1774] t: [0.1077] ivt: [0.0512]\n",
      "Batch 1640/1970 | Losses => i: [0.3694] v: [0.1778] t: [0.1049] ivt: [0.0384]\n",
      "Batch 1641/1970 | Losses => i: [0.3992] v: [0.1870] t: [0.1615] ivt: [0.0465]\n",
      "Batch 1642/1970 | Losses => i: [0.3828] v: [0.2342] t: [0.1211] ivt: [0.0485]\n",
      "Batch 1643/1970 | Losses => i: [0.3992] v: [0.1817] t: [0.1209] ivt: [0.0460]\n",
      "Batch 1644/1970 | Losses => i: [0.3890] v: [0.2209] t: [0.1414] ivt: [0.0453]\n",
      "Batch 1645/1970 | Losses => i: [0.3910] v: [0.1826] t: [0.1423] ivt: [0.0510]\n",
      "Batch 1646/1970 | Losses => i: [0.4086] v: [0.2045] t: [0.1538] ivt: [0.0475]\n",
      "Batch 1647/1970 | Losses => i: [0.3763] v: [0.1775] t: [0.0945] ivt: [0.0391]\n",
      "Batch 1648/1970 | Losses => i: [0.4063] v: [0.2146] t: [0.1117] ivt: [0.0447]\n",
      "Batch 1649/1970 | Losses => i: [0.3954] v: [0.1869] t: [0.1476] ivt: [0.0471]\n",
      "Batch 1650/1970 | Losses => i: [0.3867] v: [0.1881] t: [0.1180] ivt: [0.0463]\n",
      "Batch 1651/1970 | Losses => i: [0.4024] v: [0.1832] t: [0.1216] ivt: [0.0438]\n",
      "Batch 1652/1970 | Losses => i: [0.3584] v: [0.1993] t: [0.1397] ivt: [0.0471]\n",
      "Batch 1653/1970 | Losses => i: [0.3978] v: [0.2303] t: [0.1196] ivt: [0.0477]\n",
      "Batch 1654/1970 | Losses => i: [0.3737] v: [0.1547] t: [0.0849] ivt: [0.0387]\n",
      "Batch 1655/1970 | Losses => i: [0.3991] v: [0.1832] t: [0.1232] ivt: [0.0496]\n",
      "Batch 1656/1970 | Losses => i: [0.3899] v: [0.1897] t: [0.1164] ivt: [0.0451]\n",
      "Batch 1657/1970 | Losses => i: [0.4051] v: [0.1857] t: [0.1137] ivt: [0.0480]\n",
      "Batch 1658/1970 | Losses => i: [0.3998] v: [0.2421] t: [0.1400] ivt: [0.0498]\n",
      "Batch 1659/1970 | Losses => i: [0.4269] v: [0.2079] t: [0.1310] ivt: [0.0524]\n",
      "Batch 1660/1970 | Losses => i: [0.3785] v: [0.1991] t: [0.1207] ivt: [0.0464]\n",
      "Batch 1661/1970 | Losses => i: [0.3550] v: [0.2028] t: [0.1086] ivt: [0.0453]\n",
      "Batch 1662/1970 | Losses => i: [0.4006] v: [0.1697] t: [0.1380] ivt: [0.0473]\n",
      "Batch 1663/1970 | Losses => i: [0.3783] v: [0.2230] t: [0.1239] ivt: [0.0526]\n",
      "Batch 1664/1970 | Losses => i: [0.3645] v: [0.2358] t: [0.1158] ivt: [0.0448]\n",
      "Batch 1665/1970 | Losses => i: [0.4053] v: [0.2274] t: [0.1371] ivt: [0.0556]\n",
      "Batch 1666/1970 | Losses => i: [0.3775] v: [0.1830] t: [0.1188] ivt: [0.0460]\n",
      "Batch 1667/1970 | Losses => i: [0.3924] v: [0.1871] t: [0.1494] ivt: [0.0462]\n",
      "Batch 1668/1970 | Losses => i: [0.3940] v: [0.2152] t: [0.1695] ivt: [0.0502]\n",
      "Batch 1669/1970 | Losses => i: [0.4212] v: [0.2391] t: [0.1345] ivt: [0.0540]\n",
      "Batch 1670/1970 | Losses => i: [0.3810] v: [0.2019] t: [0.1279] ivt: [0.0474]\n",
      "Batch 1671/1970 | Losses => i: [0.3475] v: [0.1498] t: [0.0992] ivt: [0.0412]\n",
      "Batch 1672/1970 | Losses => i: [0.3636] v: [0.1742] t: [0.1053] ivt: [0.0411]\n",
      "Batch 1673/1970 | Losses => i: [0.3918] v: [0.2176] t: [0.1668] ivt: [0.0479]\n",
      "Batch 1674/1970 | Losses => i: [0.3854] v: [0.1928] t: [0.1184] ivt: [0.0460]\n",
      "Batch 1675/1970 | Losses => i: [0.4582] v: [0.2353] t: [0.1209] ivt: [0.0532]\n",
      "Batch 1676/1970 | Losses => i: [0.3651] v: [0.1860] t: [0.1185] ivt: [0.0490]\n",
      "Batch 1677/1970 | Losses => i: [0.3654] v: [0.1621] t: [0.1190] ivt: [0.0493]\n",
      "Batch 1678/1970 | Losses => i: [0.3718] v: [0.1995] t: [0.0973] ivt: [0.0424]\n",
      "Batch 1679/1970 | Losses => i: [0.4065] v: [0.2122] t: [0.1594] ivt: [0.0511]\n",
      "Batch 1680/1970 | Losses => i: [0.4216] v: [0.1978] t: [0.1030] ivt: [0.0465]\n",
      "Batch 1681/1970 | Losses => i: [0.3729] v: [0.1895] t: [0.1274] ivt: [0.0554]\n",
      "Batch 1682/1970 | Losses => i: [0.3714] v: [0.1692] t: [0.1393] ivt: [0.0508]\n",
      "Batch 1683/1970 | Losses => i: [0.3908] v: [0.1985] t: [0.1318] ivt: [0.0475]\n",
      "Batch 1684/1970 | Losses => i: [0.3841] v: [0.1764] t: [0.0974] ivt: [0.0420]\n",
      "Batch 1685/1970 | Losses => i: [0.3634] v: [0.1801] t: [0.1145] ivt: [0.0488]\n",
      "Batch 1686/1970 | Losses => i: [0.3810] v: [0.1789] t: [0.1073] ivt: [0.0460]\n",
      "Batch 1687/1970 | Losses => i: [0.3966] v: [0.1866] t: [0.1314] ivt: [0.0449]\n",
      "Batch 1688/1970 | Losses => i: [0.3746] v: [0.1735] t: [0.1037] ivt: [0.0437]\n",
      "Batch 1689/1970 | Losses => i: [0.3787] v: [0.2004] t: [0.1556] ivt: [0.0475]\n",
      "Batch 1690/1970 | Losses => i: [0.3743] v: [0.1870] t: [0.1368] ivt: [0.0494]\n",
      "Batch 1691/1970 | Losses => i: [0.3551] v: [0.1531] t: [0.0963] ivt: [0.0376]\n",
      "Batch 1692/1970 | Losses => i: [0.4179] v: [0.2168] t: [0.1287] ivt: [0.0532]\n",
      "Batch 1693/1970 | Losses => i: [0.3369] v: [0.1521] t: [0.1263] ivt: [0.0472]\n",
      "Batch 1694/1970 | Losses => i: [0.3827] v: [0.1847] t: [0.1258] ivt: [0.0477]\n",
      "Batch 1695/1970 | Losses => i: [0.3847] v: [0.1702] t: [0.1411] ivt: [0.0485]\n",
      "Batch 1696/1970 | Losses => i: [0.3726] v: [0.1880] t: [0.1379] ivt: [0.0453]\n",
      "Batch 1697/1970 | Losses => i: [0.3735] v: [0.1785] t: [0.1273] ivt: [0.0490]\n",
      "Batch 1698/1970 | Losses => i: [0.3888] v: [0.1835] t: [0.1076] ivt: [0.0394]\n",
      "Batch 1699/1970 | Losses => i: [0.4086] v: [0.1999] t: [0.1200] ivt: [0.0460]\n",
      "Batch 1700/1970 | Losses => i: [0.3622] v: [0.1690] t: [0.1186] ivt: [0.0446]\n",
      "Batch 1701/1970 | Losses => i: [0.3870] v: [0.2035] t: [0.1706] ivt: [0.0589]\n",
      "Batch 1702/1970 | Losses => i: [0.3856] v: [0.2085] t: [0.1294] ivt: [0.0451]\n",
      "Batch 1703/1970 | Losses => i: [0.4133] v: [0.2282] t: [0.1932] ivt: [0.0507]\n",
      "Batch 1704/1970 | Losses => i: [0.3873] v: [0.2240] t: [0.1449] ivt: [0.0511]\n",
      "Batch 1705/1970 | Losses => i: [0.3873] v: [0.1651] t: [0.1134] ivt: [0.0402]\n",
      "Batch 1706/1970 | Losses => i: [0.3881] v: [0.2011] t: [0.1171] ivt: [0.0437]\n",
      "Batch 1707/1970 | Losses => i: [0.3752] v: [0.1580] t: [0.0975] ivt: [0.0433]\n",
      "Batch 1708/1970 | Losses => i: [0.3915] v: [0.2045] t: [0.1251] ivt: [0.0433]\n",
      "Batch 1709/1970 | Losses => i: [0.4006] v: [0.2108] t: [0.1749] ivt: [0.0555]\n",
      "Batch 1710/1970 | Losses => i: [0.3828] v: [0.1783] t: [0.1336] ivt: [0.0510]\n",
      "Batch 1711/1970 | Losses => i: [0.3652] v: [0.1527] t: [0.1020] ivt: [0.0381]\n",
      "Batch 1712/1970 | Losses => i: [0.4656] v: [0.2479] t: [0.1643] ivt: [0.0501]\n",
      "Batch 1713/1970 | Losses => i: [0.3700] v: [0.1972] t: [0.1290] ivt: [0.0515]\n",
      "Batch 1714/1970 | Losses => i: [0.3900] v: [0.1697] t: [0.1112] ivt: [0.0431]\n",
      "Batch 1715/1970 | Losses => i: [0.3905] v: [0.1945] t: [0.1308] ivt: [0.0485]\n",
      "Batch 1716/1970 | Losses => i: [0.3762] v: [0.1882] t: [0.1064] ivt: [0.0409]\n",
      "Batch 1717/1970 | Losses => i: [0.3931] v: [0.2025] t: [0.1414] ivt: [0.0514]\n",
      "Batch 1718/1970 | Losses => i: [0.4286] v: [0.2119] t: [0.1348] ivt: [0.0499]\n",
      "Batch 1719/1970 | Losses => i: [0.3754] v: [0.1846] t: [0.1329] ivt: [0.0455]\n",
      "Batch 1720/1970 | Losses => i: [0.3902] v: [0.1820] t: [0.1144] ivt: [0.0454]\n",
      "Batch 1721/1970 | Losses => i: [0.3403] v: [0.1497] t: [0.1089] ivt: [0.0407]\n",
      "Batch 1722/1970 | Losses => i: [0.3888] v: [0.2195] t: [0.1632] ivt: [0.0494]\n",
      "Batch 1723/1970 | Losses => i: [0.3391] v: [0.1543] t: [0.1035] ivt: [0.0409]\n",
      "Batch 1724/1970 | Losses => i: [0.3377] v: [0.1408] t: [0.1213] ivt: [0.0439]\n",
      "Batch 1725/1970 | Losses => i: [0.3802] v: [0.1735] t: [0.1284] ivt: [0.0473]\n",
      "Batch 1726/1970 | Losses => i: [0.3693] v: [0.1462] t: [0.1139] ivt: [0.0408]\n",
      "Batch 1727/1970 | Losses => i: [0.3915] v: [0.2003] t: [0.1699] ivt: [0.0569]\n",
      "Batch 1728/1970 | Losses => i: [0.3641] v: [0.1925] t: [0.1178] ivt: [0.0431]\n",
      "Batch 1729/1970 | Losses => i: [0.3572] v: [0.1637] t: [0.1233] ivt: [0.0501]\n",
      "Batch 1730/1970 | Losses => i: [0.3663] v: [0.1578] t: [0.1264] ivt: [0.0473]\n",
      "Batch 1731/1970 | Losses => i: [0.3885] v: [0.2206] t: [0.1406] ivt: [0.0515]\n",
      "Batch 1732/1970 | Losses => i: [0.4000] v: [0.2212] t: [0.1393] ivt: [0.0490]\n",
      "Batch 1733/1970 | Losses => i: [0.4038] v: [0.1833] t: [0.1548] ivt: [0.0542]\n",
      "Batch 1734/1970 | Losses => i: [0.3763] v: [0.2170] t: [0.1362] ivt: [0.0605]\n",
      "Batch 1735/1970 | Losses => i: [0.3990] v: [0.1768] t: [0.1246] ivt: [0.0400]\n",
      "Batch 1736/1970 | Losses => i: [0.3886] v: [0.2069] t: [0.1814] ivt: [0.0504]\n",
      "Batch 1737/1970 | Losses => i: [0.3991] v: [0.1968] t: [0.1242] ivt: [0.0478]\n",
      "Batch 1738/1970 | Losses => i: [0.4044] v: [0.2146] t: [0.1176] ivt: [0.0449]\n",
      "Batch 1739/1970 | Losses => i: [0.4111] v: [0.2008] t: [0.1260] ivt: [0.0461]\n",
      "Batch 1740/1970 | Losses => i: [0.3559] v: [0.1761] t: [0.1141] ivt: [0.0421]\n",
      "Batch 1741/1970 | Losses => i: [0.3902] v: [0.1961] t: [0.1104] ivt: [0.0439]\n",
      "Batch 1742/1970 | Losses => i: [0.3979] v: [0.2174] t: [0.1410] ivt: [0.0513]\n",
      "Batch 1743/1970 | Losses => i: [0.3902] v: [0.2168] t: [0.1640] ivt: [0.0480]\n",
      "Batch 1744/1970 | Losses => i: [0.3783] v: [0.2027] t: [0.1655] ivt: [0.0490]\n",
      "Batch 1745/1970 | Losses => i: [0.3659] v: [0.2047] t: [0.1383] ivt: [0.0492]\n",
      "Batch 1746/1970 | Losses => i: [0.4277] v: [0.1922] t: [0.1442] ivt: [0.0471]\n",
      "Batch 1747/1970 | Losses => i: [0.4389] v: [0.2240] t: [0.1901] ivt: [0.0566]\n",
      "Batch 1748/1970 | Losses => i: [0.4160] v: [0.1891] t: [0.1237] ivt: [0.0471]\n",
      "Batch 1749/1970 | Losses => i: [0.3758] v: [0.1649] t: [0.1143] ivt: [0.0474]\n",
      "Batch 1750/1970 | Losses => i: [0.3891] v: [0.2183] t: [0.1604] ivt: [0.0587]\n",
      "Batch 1751/1970 | Losses => i: [0.4164] v: [0.1995] t: [0.1470] ivt: [0.0476]\n",
      "Batch 1752/1970 | Losses => i: [0.3908] v: [0.2368] t: [0.1839] ivt: [0.0531]\n",
      "Batch 1753/1970 | Losses => i: [0.3746] v: [0.1838] t: [0.1341] ivt: [0.0469]\n",
      "Batch 1754/1970 | Losses => i: [0.4304] v: [0.2181] t: [0.1159] ivt: [0.0443]\n",
      "Batch 1755/1970 | Losses => i: [0.4026] v: [0.2139] t: [0.1587] ivt: [0.0473]\n",
      "Batch 1756/1970 | Losses => i: [0.3688] v: [0.1845] t: [0.1263] ivt: [0.0461]\n",
      "Batch 1757/1970 | Losses => i: [0.4114] v: [0.2763] t: [0.1545] ivt: [0.0550]\n",
      "Batch 1758/1970 | Losses => i: [0.3715] v: [0.1852] t: [0.1200] ivt: [0.0493]\n",
      "Batch 1759/1970 | Losses => i: [0.3698] v: [0.1816] t: [0.1223] ivt: [0.0451]\n",
      "Batch 1760/1970 | Losses => i: [0.3806] v: [0.1948] t: [0.1230] ivt: [0.0499]\n",
      "Batch 1761/1970 | Losses => i: [0.3619] v: [0.1634] t: [0.1352] ivt: [0.0466]\n",
      "Batch 1762/1970 | Losses => i: [0.3704] v: [0.2183] t: [0.1614] ivt: [0.0502]\n",
      "Batch 1763/1970 | Losses => i: [0.3879] v: [0.1636] t: [0.1065] ivt: [0.0486]\n",
      "Batch 1764/1970 | Losses => i: [0.3572] v: [0.2178] t: [0.1389] ivt: [0.0488]\n",
      "Batch 1765/1970 | Losses => i: [0.3626] v: [0.1934] t: [0.1322] ivt: [0.0454]\n",
      "Batch 1766/1970 | Losses => i: [0.3491] v: [0.2299] t: [0.1395] ivt: [0.0491]\n",
      "Batch 1767/1970 | Losses => i: [0.4066] v: [0.1890] t: [0.1380] ivt: [0.0533]\n",
      "Batch 1768/1970 | Losses => i: [0.3756] v: [0.1938] t: [0.1015] ivt: [0.0430]\n",
      "Batch 1769/1970 | Losses => i: [0.3942] v: [0.1673] t: [0.0997] ivt: [0.0431]\n",
      "Batch 1770/1970 | Losses => i: [0.4005] v: [0.1864] t: [0.0979] ivt: [0.0454]\n",
      "Batch 1771/1970 | Losses => i: [0.3795] v: [0.1645] t: [0.1268] ivt: [0.0428]\n",
      "Batch 1772/1970 | Losses => i: [0.3825] v: [0.2209] t: [0.1363] ivt: [0.0454]\n",
      "Batch 1773/1970 | Losses => i: [0.3913] v: [0.1988] t: [0.1304] ivt: [0.0498]\n",
      "Batch 1774/1970 | Losses => i: [0.3714] v: [0.1944] t: [0.1201] ivt: [0.0428]\n",
      "Batch 1775/1970 | Losses => i: [0.3538] v: [0.1726] t: [0.1330] ivt: [0.0476]\n",
      "Batch 1776/1970 | Losses => i: [0.3577] v: [0.1853] t: [0.1278] ivt: [0.0524]\n",
      "Batch 1777/1970 | Losses => i: [0.3442] v: [0.1754] t: [0.1340] ivt: [0.0492]\n",
      "Batch 1778/1970 | Losses => i: [0.4062] v: [0.2016] t: [0.1327] ivt: [0.0488]\n",
      "Batch 1779/1970 | Losses => i: [0.3938] v: [0.2741] t: [0.1411] ivt: [0.0552]\n",
      "Batch 1780/1970 | Losses => i: [0.3737] v: [0.1651] t: [0.1120] ivt: [0.0421]\n",
      "Batch 1781/1970 | Losses => i: [0.4551] v: [0.2576] t: [0.1487] ivt: [0.0649]\n",
      "Batch 1782/1970 | Losses => i: [0.3755] v: [0.1691] t: [0.1134] ivt: [0.0474]\n",
      "Batch 1783/1970 | Losses => i: [0.3654] v: [0.2357] t: [0.1176] ivt: [0.0461]\n",
      "Batch 1784/1970 | Losses => i: [0.3649] v: [0.1778] t: [0.1255] ivt: [0.0476]\n",
      "Batch 1785/1970 | Losses => i: [0.3802] v: [0.2050] t: [0.1446] ivt: [0.0486]\n",
      "Batch 1786/1970 | Losses => i: [0.3795] v: [0.1851] t: [0.1334] ivt: [0.0467]\n",
      "Batch 1787/1970 | Losses => i: [0.3681] v: [0.2268] t: [0.1161] ivt: [0.0474]\n",
      "Batch 1788/1970 | Losses => i: [0.3812] v: [0.2088] t: [0.1295] ivt: [0.0492]\n",
      "Batch 1789/1970 | Losses => i: [0.3663] v: [0.1759] t: [0.1182] ivt: [0.0463]\n",
      "Batch 1790/1970 | Losses => i: [0.3880] v: [0.2011] t: [0.1211] ivt: [0.0517]\n",
      "Batch 1791/1970 | Losses => i: [0.3825] v: [0.1761] t: [0.1336] ivt: [0.0462]\n",
      "Batch 1792/1970 | Losses => i: [0.3667] v: [0.1743] t: [0.1156] ivt: [0.0472]\n",
      "Batch 1793/1970 | Losses => i: [0.3995] v: [0.2146] t: [0.1623] ivt: [0.0521]\n",
      "Batch 1794/1970 | Losses => i: [0.3713] v: [0.1903] t: [0.1042] ivt: [0.0471]\n",
      "Batch 1795/1970 | Losses => i: [0.3896] v: [0.1800] t: [0.1181] ivt: [0.0441]\n",
      "Batch 1796/1970 | Losses => i: [0.4037] v: [0.2086] t: [0.1312] ivt: [0.0489]\n",
      "Batch 1797/1970 | Losses => i: [0.3164] v: [0.1604] t: [0.0988] ivt: [0.0435]\n",
      "Batch 1798/1970 | Losses => i: [0.3555] v: [0.1604] t: [0.1005] ivt: [0.0415]\n",
      "Batch 1799/1970 | Losses => i: [0.4013] v: [0.2078] t: [0.1285] ivt: [0.0463]\n",
      "Batch 1800/1970 | Losses => i: [0.3501] v: [0.2054] t: [0.1244] ivt: [0.0475]\n",
      "Batch 1801/1970 | Losses => i: [0.3670] v: [0.2156] t: [0.1276] ivt: [0.0497]\n",
      "Batch 1802/1970 | Losses => i: [0.4072] v: [0.1899] t: [0.1229] ivt: [0.0522]\n",
      "Batch 1803/1970 | Losses => i: [0.3659] v: [0.1523] t: [0.1077] ivt: [0.0404]\n",
      "Batch 1804/1970 | Losses => i: [0.3866] v: [0.1927] t: [0.1186] ivt: [0.0454]\n",
      "Batch 1805/1970 | Losses => i: [0.3886] v: [0.1957] t: [0.1122] ivt: [0.0422]\n",
      "Batch 1806/1970 | Losses => i: [0.4047] v: [0.1944] t: [0.1296] ivt: [0.0469]\n",
      "Batch 1807/1970 | Losses => i: [0.3575] v: [0.1908] t: [0.1310] ivt: [0.0464]\n",
      "Batch 1808/1970 | Losses => i: [0.3584] v: [0.2106] t: [0.1516] ivt: [0.0574]\n",
      "Batch 1809/1970 | Losses => i: [0.3503] v: [0.1350] t: [0.0924] ivt: [0.0423]\n",
      "Batch 1810/1970 | Losses => i: [0.3322] v: [0.1835] t: [0.1232] ivt: [0.0477]\n",
      "Batch 1811/1970 | Losses => i: [0.3678] v: [0.1876] t: [0.1154] ivt: [0.0491]\n",
      "Batch 1812/1970 | Losses => i: [0.3781] v: [0.1807] t: [0.1207] ivt: [0.0503]\n",
      "Batch 1813/1970 | Losses => i: [0.3502] v: [0.1975] t: [0.1494] ivt: [0.0469]\n",
      "Batch 1814/1970 | Losses => i: [0.3579] v: [0.1783] t: [0.0957] ivt: [0.0393]\n",
      "Batch 1815/1970 | Losses => i: [0.3974] v: [0.1897] t: [0.1301] ivt: [0.0504]\n",
      "Batch 1816/1970 | Losses => i: [0.4341] v: [0.2264] t: [0.1705] ivt: [0.0571]\n",
      "Batch 1817/1970 | Losses => i: [0.3907] v: [0.2047] t: [0.1377] ivt: [0.0572]\n",
      "Batch 1818/1970 | Losses => i: [0.4099] v: [0.2468] t: [0.1453] ivt: [0.0578]\n",
      "Batch 1819/1970 | Losses => i: [0.3667] v: [0.1501] t: [0.1034] ivt: [0.0422]\n",
      "Batch 1820/1970 | Losses => i: [0.3840] v: [0.1783] t: [0.1598] ivt: [0.0422]\n",
      "Batch 1821/1970 | Losses => i: [0.3647] v: [0.1936] t: [0.1141] ivt: [0.0494]\n",
      "Batch 1822/1970 | Losses => i: [0.3785] v: [0.1905] t: [0.1086] ivt: [0.0458]\n",
      "Batch 1823/1970 | Losses => i: [0.3785] v: [0.1716] t: [0.1184] ivt: [0.0455]\n",
      "Batch 1824/1970 | Losses => i: [0.3843] v: [0.1851] t: [0.1285] ivt: [0.0482]\n",
      "Batch 1825/1970 | Losses => i: [0.3982] v: [0.2311] t: [0.1274] ivt: [0.0463]\n",
      "Batch 1826/1970 | Losses => i: [0.4004] v: [0.1981] t: [0.1241] ivt: [0.0484]\n",
      "Batch 1827/1970 | Losses => i: [0.3886] v: [0.2173] t: [0.1318] ivt: [0.0454]\n",
      "Batch 1828/1970 | Losses => i: [0.4321] v: [0.2502] t: [0.1490] ivt: [0.0561]\n",
      "Batch 1829/1970 | Losses => i: [0.4010] v: [0.2347] t: [0.1474] ivt: [0.0521]\n",
      "Batch 1830/1970 | Losses => i: [0.3811] v: [0.2366] t: [0.1441] ivt: [0.0531]\n",
      "Batch 1831/1970 | Losses => i: [0.4100] v: [0.2143] t: [0.1308] ivt: [0.0518]\n",
      "Batch 1832/1970 | Losses => i: [0.3677] v: [0.2015] t: [0.1156] ivt: [0.0454]\n",
      "Batch 1833/1970 | Losses => i: [0.3925] v: [0.1997] t: [0.1241] ivt: [0.0503]\n",
      "Batch 1834/1970 | Losses => i: [0.3434] v: [0.1955] t: [0.1524] ivt: [0.0498]\n",
      "Batch 1835/1970 | Losses => i: [0.3719] v: [0.1970] t: [0.1156] ivt: [0.0433]\n",
      "Batch 1836/1970 | Losses => i: [0.3825] v: [0.2709] t: [0.1730] ivt: [0.0562]\n",
      "Batch 1837/1970 | Losses => i: [0.3711] v: [0.1838] t: [0.1207] ivt: [0.0465]\n",
      "Batch 1838/1970 | Losses => i: [0.3883] v: [0.2024] t: [0.1307] ivt: [0.0451]\n",
      "Batch 1839/1970 | Losses => i: [0.3781] v: [0.1890] t: [0.1400] ivt: [0.0483]\n",
      "Batch 1840/1970 | Losses => i: [0.3655] v: [0.1619] t: [0.1109] ivt: [0.0510]\n",
      "Batch 1841/1970 | Losses => i: [0.3733] v: [0.1839] t: [0.1207] ivt: [0.0411]\n",
      "Batch 1842/1970 | Losses => i: [0.4001] v: [0.2135] t: [0.1426] ivt: [0.0486]\n",
      "Batch 1843/1970 | Losses => i: [0.3516] v: [0.1572] t: [0.1011] ivt: [0.0383]\n",
      "Batch 1844/1970 | Losses => i: [0.3702] v: [0.1839] t: [0.1359] ivt: [0.0518]\n",
      "Batch 1845/1970 | Losses => i: [0.3761] v: [0.1641] t: [0.0989] ivt: [0.0439]\n",
      "Batch 1846/1970 | Losses => i: [0.3458] v: [0.1788] t: [0.1243] ivt: [0.0420]\n",
      "Batch 1847/1970 | Losses => i: [0.3673] v: [0.1802] t: [0.1673] ivt: [0.0468]\n",
      "Batch 1848/1970 | Losses => i: [0.3484] v: [0.1652] t: [0.1010] ivt: [0.0414]\n",
      "Batch 1849/1970 | Losses => i: [0.3832] v: [0.1939] t: [0.1391] ivt: [0.0486]\n",
      "Batch 1850/1970 | Losses => i: [0.3966] v: [0.1823] t: [0.1215] ivt: [0.0497]\n",
      "Batch 1851/1970 | Losses => i: [0.4325] v: [0.1908] t: [0.1094] ivt: [0.0417]\n",
      "Batch 1852/1970 | Losses => i: [0.4110] v: [0.2679] t: [0.1278] ivt: [0.0512]\n",
      "Batch 1853/1970 | Losses => i: [0.3620] v: [0.2107] t: [0.1340] ivt: [0.0470]\n",
      "Batch 1854/1970 | Losses => i: [0.3696] v: [0.2048] t: [0.1163] ivt: [0.0447]\n",
      "Batch 1855/1970 | Losses => i: [0.3514] v: [0.1607] t: [0.1373] ivt: [0.0473]\n",
      "Batch 1856/1970 | Losses => i: [0.3237] v: [0.1717] t: [0.1350] ivt: [0.0453]\n",
      "Batch 1857/1970 | Losses => i: [0.3981] v: [0.2096] t: [0.1304] ivt: [0.0479]\n",
      "Batch 1858/1970 | Losses => i: [0.3644] v: [0.1633] t: [0.1149] ivt: [0.0437]\n",
      "Batch 1859/1970 | Losses => i: [0.3676] v: [0.1742] t: [0.1421] ivt: [0.0453]\n",
      "Batch 1860/1970 | Losses => i: [0.3651] v: [0.1732] t: [0.1102] ivt: [0.0415]\n",
      "Batch 1861/1970 | Losses => i: [0.3426] v: [0.1632] t: [0.1224] ivt: [0.0470]\n",
      "Batch 1862/1970 | Losses => i: [0.4133] v: [0.2519] t: [0.1525] ivt: [0.0489]\n",
      "Batch 1863/1970 | Losses => i: [0.3767] v: [0.2243] t: [0.1294] ivt: [0.0506]\n",
      "Batch 1864/1970 | Losses => i: [0.3629] v: [0.1845] t: [0.1568] ivt: [0.0507]\n",
      "Batch 1865/1970 | Losses => i: [0.3825] v: [0.1825] t: [0.1500] ivt: [0.0429]\n",
      "Batch 1866/1970 | Losses => i: [0.3576] v: [0.1879] t: [0.1262] ivt: [0.0462]\n",
      "Batch 1867/1970 | Losses => i: [0.3613] v: [0.1478] t: [0.1378] ivt: [0.0470]\n",
      "Batch 1868/1970 | Losses => i: [0.3810] v: [0.1832] t: [0.1302] ivt: [0.0515]\n",
      "Batch 1869/1970 | Losses => i: [0.3978] v: [0.2052] t: [0.0977] ivt: [0.0429]\n",
      "Batch 1870/1970 | Losses => i: [0.3699] v: [0.2071] t: [0.1520] ivt: [0.0478]\n",
      "Batch 1871/1970 | Losses => i: [0.3389] v: [0.1878] t: [0.1091] ivt: [0.0403]\n",
      "Batch 1872/1970 | Losses => i: [0.3743] v: [0.1847] t: [0.1008] ivt: [0.0414]\n",
      "Batch 1873/1970 | Losses => i: [0.3636] v: [0.1475] t: [0.1129] ivt: [0.0398]\n",
      "Batch 1874/1970 | Losses => i: [0.3809] v: [0.2165] t: [0.1128] ivt: [0.0523]\n",
      "Batch 1875/1970 | Losses => i: [0.3734] v: [0.1796] t: [0.1382] ivt: [0.0450]\n",
      "Batch 1876/1970 | Losses => i: [0.3772] v: [0.2232] t: [0.1213] ivt: [0.0461]\n",
      "Batch 1877/1970 | Losses => i: [0.4007] v: [0.1970] t: [0.1387] ivt: [0.0512]\n",
      "Batch 1878/1970 | Losses => i: [0.3927] v: [0.1931] t: [0.1360] ivt: [0.0473]\n",
      "Batch 1879/1970 | Losses => i: [0.4068] v: [0.2087] t: [0.1473] ivt: [0.0545]\n",
      "Batch 1880/1970 | Losses => i: [0.3468] v: [0.1718] t: [0.1121] ivt: [0.0409]\n",
      "Batch 1881/1970 | Losses => i: [0.3747] v: [0.1894] t: [0.1330] ivt: [0.0446]\n",
      "Batch 1882/1970 | Losses => i: [0.3255] v: [0.1725] t: [0.1091] ivt: [0.0429]\n",
      "Batch 1883/1970 | Losses => i: [0.4054] v: [0.2363] t: [0.1694] ivt: [0.0583]\n",
      "Batch 1884/1970 | Losses => i: [0.3573] v: [0.1528] t: [0.1201] ivt: [0.0415]\n",
      "Batch 1885/1970 | Losses => i: [0.3885] v: [0.1778] t: [0.1194] ivt: [0.0448]\n",
      "Batch 1886/1970 | Losses => i: [0.3926] v: [0.1880] t: [0.1149] ivt: [0.0453]\n",
      "Batch 1887/1970 | Losses => i: [0.3943] v: [0.1900] t: [0.1244] ivt: [0.0494]\n",
      "Batch 1888/1970 | Losses => i: [0.4239] v: [0.2063] t: [0.1314] ivt: [0.0525]\n",
      "Batch 1889/1970 | Losses => i: [0.3997] v: [0.2096] t: [0.1134] ivt: [0.0461]\n",
      "Batch 1890/1970 | Losses => i: [0.4028] v: [0.1656] t: [0.1070] ivt: [0.0428]\n",
      "Batch 1891/1970 | Losses => i: [0.3659] v: [0.2179] t: [0.1475] ivt: [0.0483]\n",
      "Batch 1892/1970 | Losses => i: [0.3796] v: [0.2144] t: [0.1224] ivt: [0.0415]\n",
      "Batch 1893/1970 | Losses => i: [0.3622] v: [0.1801] t: [0.1368] ivt: [0.0466]\n",
      "Batch 1894/1970 | Losses => i: [0.3765] v: [0.1951] t: [0.1241] ivt: [0.0465]\n",
      "Batch 1895/1970 | Losses => i: [0.3541] v: [0.1860] t: [0.1331] ivt: [0.0483]\n",
      "Batch 1896/1970 | Losses => i: [0.3803] v: [0.1700] t: [0.1296] ivt: [0.0495]\n",
      "Batch 1897/1970 | Losses => i: [0.3494] v: [0.2116] t: [0.1299] ivt: [0.0493]\n",
      "Batch 1898/1970 | Losses => i: [0.3825] v: [0.1944] t: [0.1229] ivt: [0.0418]\n",
      "Batch 1899/1970 | Losses => i: [0.3940] v: [0.1835] t: [0.1548] ivt: [0.0450]\n",
      "Batch 1900/1970 | Losses => i: [0.3974] v: [0.2126] t: [0.1345] ivt: [0.0463]\n",
      "Batch 1901/1970 | Losses => i: [0.3654] v: [0.1795] t: [0.1232] ivt: [0.0496]\n",
      "Batch 1902/1970 | Losses => i: [0.3464] v: [0.1941] t: [0.1326] ivt: [0.0546]\n",
      "Batch 1903/1970 | Losses => i: [0.3752] v: [0.1818] t: [0.1134] ivt: [0.0441]\n",
      "Batch 1904/1970 | Losses => i: [0.3904] v: [0.1866] t: [0.1330] ivt: [0.0549]\n",
      "Batch 1905/1970 | Losses => i: [0.3861] v: [0.1678] t: [0.0943] ivt: [0.0389]\n",
      "Batch 1906/1970 | Losses => i: [0.3641] v: [0.2075] t: [0.1519] ivt: [0.0477]\n",
      "Batch 1907/1970 | Losses => i: [0.3608] v: [0.1906] t: [0.1597] ivt: [0.0514]\n",
      "Batch 1908/1970 | Losses => i: [0.3411] v: [0.1619] t: [0.1280] ivt: [0.0457]\n",
      "Batch 1909/1970 | Losses => i: [0.3423] v: [0.1693] t: [0.1055] ivt: [0.0445]\n",
      "Batch 1910/1970 | Losses => i: [0.3653] v: [0.1910] t: [0.1357] ivt: [0.0475]\n",
      "Batch 1911/1970 | Losses => i: [0.3396] v: [0.2281] t: [0.1287] ivt: [0.0501]\n",
      "Batch 1912/1970 | Losses => i: [0.3745] v: [0.2115] t: [0.1277] ivt: [0.0480]\n",
      "Batch 1913/1970 | Losses => i: [0.4051] v: [0.1718] t: [0.1135] ivt: [0.0528]\n",
      "Batch 1914/1970 | Losses => i: [0.3622] v: [0.1860] t: [0.1492] ivt: [0.0510]\n",
      "Batch 1915/1970 | Losses => i: [0.3834] v: [0.1830] t: [0.1212] ivt: [0.0460]\n",
      "Batch 1916/1970 | Losses => i: [0.3624] v: [0.1566] t: [0.0943] ivt: [0.0346]\n",
      "Batch 1917/1970 | Losses => i: [0.4300] v: [0.2345] t: [0.1631] ivt: [0.0539]\n",
      "Batch 1918/1970 | Losses => i: [0.3557] v: [0.1887] t: [0.1157] ivt: [0.0407]\n",
      "Batch 1919/1970 | Losses => i: [0.3567] v: [0.1699] t: [0.1098] ivt: [0.0398]\n",
      "Batch 1920/1970 | Losses => i: [0.4182] v: [0.2059] t: [0.1252] ivt: [0.0518]\n",
      "Batch 1921/1970 | Losses => i: [0.3208] v: [0.1897] t: [0.1225] ivt: [0.0454]\n",
      "Batch 1922/1970 | Losses => i: [0.4241] v: [0.2330] t: [0.1632] ivt: [0.0544]\n",
      "Batch 1923/1970 | Losses => i: [0.3774] v: [0.2533] t: [0.1409] ivt: [0.0509]\n",
      "Batch 1924/1970 | Losses => i: [0.3908] v: [0.2622] t: [0.1516] ivt: [0.0545]\n",
      "Batch 1925/1970 | Losses => i: [0.3800] v: [0.1894] t: [0.1284] ivt: [0.0503]\n",
      "Batch 1926/1970 | Losses => i: [0.3576] v: [0.1599] t: [0.1091] ivt: [0.0431]\n",
      "Batch 1927/1970 | Losses => i: [0.3982] v: [0.2105] t: [0.1279] ivt: [0.0495]\n",
      "Batch 1928/1970 | Losses => i: [0.4074] v: [0.1926] t: [0.1319] ivt: [0.0513]\n",
      "Batch 1929/1970 | Losses => i: [0.3920] v: [0.1979] t: [0.1182] ivt: [0.0504]\n",
      "Batch 1930/1970 | Losses => i: [0.3446] v: [0.1741] t: [0.1382] ivt: [0.0461]\n",
      "Batch 1931/1970 | Losses => i: [0.4058] v: [0.2099] t: [0.1182] ivt: [0.0484]\n",
      "Batch 1932/1970 | Losses => i: [0.3982] v: [0.2222] t: [0.1372] ivt: [0.0471]\n",
      "Batch 1933/1970 | Losses => i: [0.3241] v: [0.1927] t: [0.1327] ivt: [0.0513]\n",
      "Batch 1934/1970 | Losses => i: [0.3759] v: [0.1917] t: [0.1199] ivt: [0.0431]\n",
      "Batch 1935/1970 | Losses => i: [0.3631] v: [0.2026] t: [0.1305] ivt: [0.0480]\n",
      "Batch 1936/1970 | Losses => i: [0.3330] v: [0.1848] t: [0.1289] ivt: [0.0416]\n",
      "Batch 1937/1970 | Losses => i: [0.4319] v: [0.2288] t: [0.1327] ivt: [0.0539]\n",
      "Batch 1938/1970 | Losses => i: [0.3766] v: [0.2059] t: [0.1592] ivt: [0.0488]\n",
      "Batch 1939/1970 | Losses => i: [0.3545] v: [0.1671] t: [0.1128] ivt: [0.0419]\n",
      "Batch 1940/1970 | Losses => i: [0.4057] v: [0.2222] t: [0.1351] ivt: [0.0487]\n",
      "Batch 1941/1970 | Losses => i: [0.3574] v: [0.1721] t: [0.1265] ivt: [0.0453]\n",
      "Batch 1942/1970 | Losses => i: [0.3680] v: [0.2432] t: [0.1174] ivt: [0.0502]\n",
      "Batch 1943/1970 | Losses => i: [0.3397] v: [0.1668] t: [0.1127] ivt: [0.0405]\n",
      "Batch 1944/1970 | Losses => i: [0.3761] v: [0.2264] t: [0.1642] ivt: [0.0527]\n",
      "Batch 1945/1970 | Losses => i: [0.3427] v: [0.1696] t: [0.1126] ivt: [0.0456]\n",
      "Batch 1946/1970 | Losses => i: [0.3790] v: [0.2336] t: [0.1189] ivt: [0.0482]\n",
      "Batch 1947/1970 | Losses => i: [0.3794] v: [0.2039] t: [0.1325] ivt: [0.0524]\n",
      "Batch 1948/1970 | Losses => i: [0.3991] v: [0.2474] t: [0.1597] ivt: [0.0533]\n",
      "Batch 1949/1970 | Losses => i: [0.3495] v: [0.1541] t: [0.1321] ivt: [0.0411]\n",
      "Batch 1950/1970 | Losses => i: [0.3792] v: [0.1711] t: [0.1041] ivt: [0.0472]\n",
      "Batch 1951/1970 | Losses => i: [0.3692] v: [0.2137] t: [0.1474] ivt: [0.0492]\n",
      "Batch 1952/1970 | Losses => i: [0.3628] v: [0.1686] t: [0.1133] ivt: [0.0423]\n",
      "Batch 1953/1970 | Losses => i: [0.3967] v: [0.2002] t: [0.1209] ivt: [0.0463]\n",
      "Batch 1954/1970 | Losses => i: [0.3345] v: [0.1727] t: [0.1369] ivt: [0.0493]\n",
      "Batch 1955/1970 | Losses => i: [0.3593] v: [0.1657] t: [0.1455] ivt: [0.0520]\n",
      "Batch 1956/1970 | Losses => i: [0.3255] v: [0.1705] t: [0.1375] ivt: [0.0437]\n",
      "Batch 1957/1970 | Losses => i: [0.3772] v: [0.1608] t: [0.1475] ivt: [0.0449]\n",
      "Batch 1958/1970 | Losses => i: [0.3787] v: [0.2327] t: [0.1564] ivt: [0.0519]\n",
      "Batch 1959/1970 | Losses => i: [0.3474] v: [0.2366] t: [0.1542] ivt: [0.0562]\n",
      "Batch 1960/1970 | Losses => i: [0.3839] v: [0.2013] t: [0.1168] ivt: [0.0507]\n",
      "Batch 1961/1970 | Losses => i: [0.4003] v: [0.2294] t: [0.1522] ivt: [0.0546]\n",
      "Batch 1962/1970 | Losses => i: [0.3731] v: [0.2071] t: [0.1105] ivt: [0.0474]\n",
      "Batch 1963/1970 | Losses => i: [0.3776] v: [0.2125] t: [0.1496] ivt: [0.0495]\n",
      "Batch 1964/1970 | Losses => i: [0.3527] v: [0.1768] t: [0.1270] ivt: [0.0457]\n",
      "Batch 1965/1970 | Losses => i: [0.3863] v: [0.1894] t: [0.1229] ivt: [0.0451]\n",
      "Batch 1966/1970 | Losses => i: [0.3728] v: [0.1699] t: [0.1533] ivt: [0.0561]\n",
      "Batch 1967/1970 | Losses => i: [0.3416] v: [0.2127] t: [0.1219] ivt: [0.0419]\n",
      "Batch 1968/1970 | Losses => i: [0.3609] v: [0.1915] t: [0.1217] ivt: [0.0450]\n",
      "Batch 1969/1970 | Losses => i: [0.3561] v: [0.1777] t: [0.1370] ivt: [0.0475]\n",
      "Batch 1970/1970 | Losses => i: [0.3418] v: [0.1392] t: [0.0930] ivt: [0.0374]\n",
      "Epoch 1 completed | Average Losses--- i: [0.5112] v: [0.4227] t: [0.3820] ivt: [0.0754] | eta: 999.46 secs\n",
      "Average Precision for Epoch 1: 0.0185\n",
      "Evaluating @ epoch: 0\n",
      "--------------------------------------------------\n",
      "Test Results\n",
      "Per-category AP:\n",
      "I   : [0.76955959 0.10225505 0.67961504 0.04209457 0.02845542 0.07196342]\n",
      "V   : [0.15828737 0.63214048 0.57416378 0.10885789 0.03346423 0.04607111\n",
      " 0.0486979  0.0299855  0.00701029 0.12857669]\n",
      "T   : [0.71489502 0.07834433 0.09597156 0.05466435 0.00588949        nan\n",
      " 0.05501509 0.01694006 0.2619738  0.08131128 0.13979767 0.03751201\n",
      " 0.02324516 0.1708169  0.13149981]\n",
      "IV  : [0.07222499 0.65869116 0.00177926 0.00563977 0.04420931 0.04017366\n",
      " 0.01355623 0.03018804 0.06222175 0.01056101 0.01681985 0.56348414\n",
      " 0.01697672        nan 0.03769347        nan        nan 0.03718698\n",
      " 0.01027738 0.02564209 0.01458377 0.00885742        nan 0.04898125\n",
      " 0.01910626 0.00788384]\n",
      "IT  : [0.48917675 0.0293569  0.01438939 0.03029528        nan 0.19222251\n",
      " 0.07683084 0.01612006 0.01870343 0.05148662 0.04420931 0.04575733\n",
      " 0.02925739 0.00839531        nan 0.00550092        nan 0.00833947\n",
      " 0.05222266        nan 0.03383859        nan        nan 0.01056101\n",
      " 0.3364706  0.05800799 0.08488575 0.06265468        nan        nan\n",
      " 0.01621284 0.07412187 0.03186904 0.03769347        nan 0.01270067\n",
      " 0.01563359 0.01039429        nan 0.02675907 0.04891954        nan\n",
      "        nan 0.01027738 0.01274293 0.01532845 0.01319591        nan\n",
      "        nan 0.01458377        nan        nan        nan 0.00364633\n",
      " 0.04898125 0.01009981 0.01357616        nan 0.00788384]\n",
      "IVT : [           nan 2.88828844e-03 3.91083301e-04 3.02952756e-02\n",
      " 2.26937139e-02            nan 8.25032571e-03 2.25796632e-02\n",
      " 2.25461998e-03 1.70527827e-02 2.52921369e-02            nan\n",
      " 5.14866208e-02 5.63977316e-03            nan            nan\n",
      " 4.05878586e-02 4.74168690e-01 1.85492156e-02 1.90081131e-01\n",
      " 7.62523683e-02 1.64766047e-02 8.33946718e-03            nan\n",
      "            nan 1.31439484e-02 5.46121617e-03 3.78087978e-02\n",
      " 2.50321538e-02 4.39743731e-02 2.56324921e-02            nan\n",
      "            nan            nan 2.64997750e-03            nan\n",
      " 4.03005145e-02            nan 9.74231758e-03 6.37832937e-02\n",
      "            nan 3.61919011e-03            nan            nan\n",
      " 1.16618780e-02 6.19264892e-03            nan            nan\n",
      "            nan            nan            nan 1.02621784e-02\n",
      " 6.79128318e-03 1.19836967e-02            nan            nan\n",
      "            nan 6.26546827e-02 8.48857537e-02 5.80079870e-02\n",
      " 3.29202544e-01 6.98990293e-02 4.27879283e-02 1.47151919e-02\n",
      " 4.75205714e-03            nan 4.89195409e-02            nan\n",
      " 1.03942908e-02 1.56335869e-02 8.14850344e-03 2.67590717e-02\n",
      "            nan            nan            nan            nan\n",
      "            nan            nan 1.31959090e-02 1.53284481e-02\n",
      "            nan 1.27429305e-02 4.89812548e-02            nan\n",
      "            nan            nan            nan            nan\n",
      " 1.00998129e-02 1.99842458e-03 1.05696311e-02            nan\n",
      " 6.26621679e-03            nan 4.42093118e-02 1.05610124e-02\n",
      " 3.76934673e-02 1.02773829e-02 1.45837674e-02 7.88383707e-03]\n",
      "--------------------------------------------------\n",
      "Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT \n",
      ":::::: : 0.2823 | 0.1767 | 0.1334 | 0.0794 | 0.0518 | 0.0386\n",
      "==================================================\n",
      "Top-5 Accuracy: 0.7196\n",
      "Top-10 Accuracy: 0.8211\n"
     ]
    }
   ],
   "source": [
    "# log config\n",
    "# header1 = \"** Run: {} | Framework: PyTorch | Method: {} | Version: {} | Data: CholecT50 | Batch: {} **\".format(os.path.basename(__file__), modelname, version, batch_size)\n",
    "# header2 = \"** Time: {} | Start: {}-epoch  {}-steps | Init CKPT: {} | Save CKPT: {} **\".format(time.ctime(), 0, 0, resume_ckpt, ckpt_path)\n",
    "# header3 = \"** LR Config: Init: {} | Peak: {} | Warmup Epoch: {} | Rise: {} | Decay {} | train params {} | all params {} **\".format([float(f'{sch.get_last_lr()[0]:.6f}') for sch in lr_schedulers], [float(f'{v:.6f}') for v in wp_lr], warmups, power, decay_rate, pytorch_train_params, pytorch_total_params)\n",
    "# maxlen  = max(len(header1), len(header2), len(header3))\n",
    "# header1 = \"{}{}{}\".format('*'*((maxlen-len(header1))//2+1), header1, '*'*((maxlen-len(header1))//2+1) )\n",
    "# header2 = \"{}{}{}\".format('*'*((maxlen-len(header2))//2+1), header2, '*'*((maxlen-len(header2))//2+1) )\n",
    "# header3 = \"{}{}{}\".format('*'*((maxlen-len(header3))//2+1), header3, '*'*((maxlen-len(header3))//2+1) )\n",
    "# maxlen  = max(len(header1), len(header2), len(header3))\n",
    "# print(\"\\n\\n\\n{}\\n{}\\n{}\\n{}\\n{}\".format(\"*\"*maxlen, header1, header2, header3, \"*\"*maxlen), file=open(logfile, 'a+'))\n",
    "# print(\"Experiment started ...\\n   logging outputs to: \", logfile)\n",
    "\n",
    "print(f\"is_train: {is_train}, is_test: {is_test}\")\n",
    "is_train = True\n",
    "is_test = True\n",
    "\n",
    "# training\n",
    "if is_train:\n",
    "    for epoch in range(0,epochs):\n",
    "        try:\n",
    "            print(f\"Training | lr: {[lr.get_last_lr() for lr in lr_schedulers]} | epoch {epoch} | \")\n",
    "            train_loop(train_dataloader, model, activation, loss_fn_i, loss_fn_v, loss_fn_t, loss_fn_ivt, optimizers, lr_schedulers, epoch)\n",
    "\n",
    "            # validation\n",
    "            if epoch % val_interval == 0:\n",
    "                start = time.time()  \n",
    "                mAP.reset_global()\n",
    "#                 print(\"Evaluating @ epoch: \", epoch, file=open(logfile, 'a+'))\n",
    "                print(f\"Evaluating @ epoch: {epoch}\")\n",
    "#                 print(\"\\t\\t\\t\\t\\t\\t\\t video-wise | eta {:.2f} secs | mAP => ivt: [{:.5f}] \".format( (time.time() - start), mAP.compute_video_AP('ivt', ignore_null=set_chlg_eval)['mAP']), file=open(logfile, 'a+'))      \n",
    "#                 print(f\"Video-wise | eta {(time.time() - start):.2f} secs | mAP => ivt: [{mAP.compute_video_AP('ivt', ignore_null=set_chlg_eval)['mAP']:.5f}]\")\n",
    "                \n",
    "                # save model checkpoint\n",
    "                torch.save(model.state_dict(), '/kaggle/working/model_epoch_{}.pth'.format(epoch))\n",
    "        except KeyboardInterrupt:\n",
    "#             print(f'>> Process cancelled by user at {time.ctime()}, ...', file=open(logfile, 'a+'))    \n",
    "            sys.exit(1)\n",
    "    test_ckpt = ckpt_path\n",
    "    \n",
    "# save the final model after training\n",
    "torch.save(model.state_dict(), '/kaggle/working/model_final.pth')\n",
    "\n",
    "# test\n",
    "test_ckpt = \"/kaggle/working/model_epoch_0.pth\" # just for showing the output for 1 epoch\n",
    "if is_test:\n",
    "    total_top5_accuracy = 0.0\n",
    "    total_top10_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "#     print(\"Test weight used: \", test_ckpt)\n",
    "    model.load_state_dict(torch.load(test_ckpt))\n",
    "    \n",
    "    mAP.reset_global()\n",
    "    for test_dataloader in test_dataloaders:\n",
    "#         test_loop(test_dataloader, model, activation, final_eval=True)\n",
    "        top5_accuracy, top10_accuracy, num_samples = test_loop(test_dataloader, model, activation, final_eval=True)\n",
    "        total_top5_accuracy += top5_accuracy * num_samples\n",
    "        total_top10_accuracy += top10_accuracy * num_samples\n",
    "        total_samples += num_samples\n",
    "\n",
    "    total_top5_accuracy /= total_samples\n",
    "    total_top10_accuracy /= total_samples\n",
    "    \n",
    "    if set_chlg_eval:\n",
    "        mAP_i = mAP.compute_video_AP('i', ignore_null=set_chlg_eval)\n",
    "        mAP_v = mAP.compute_video_AP('v', ignore_null=set_chlg_eval)\n",
    "        mAP_t = mAP.compute_video_AP('t', ignore_null=set_chlg_eval)\n",
    "    else:\n",
    "        mAP_i = mAPi.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "        mAP_v = mAPv.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "        mAP_t = mAPt.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "    mAP_iv = mAP.compute_video_AP('iv', ignore_null=set_chlg_eval)\n",
    "    mAP_it = mAP.compute_video_AP('it', ignore_null=set_chlg_eval)\n",
    "    mAP_ivt = mAP.compute_video_AP('ivt', ignore_null=set_chlg_eval) \n",
    "    \n",
    "#     print('-'*50, file=open(logfile, 'a+'))\n",
    "#     print('Test Results\\nPer-category AP: ', file=open(logfile, 'a+'))\n",
    "#     print(f'I   : {mAP_i[\"AP\"]}', file=open(logfile, 'a+'))\n",
    "#     print(f'V   : {mAP_v[\"AP\"]}', file=open(logfile, 'a+'))\n",
    "#     print(f'T   : {mAP_t[\"AP\"]}', file=open(logfile, 'a+'))\n",
    "#     print(f'IV  : {mAP_iv[\"AP\"]}', file=open(logfile, 'a+'))\n",
    "#     print(f'IT  : {mAP_it[\"AP\"]}', file=open(logfile, 'a+'))\n",
    "#     print(f'IVT : {mAP_ivt[\"AP\"]}', file=open(logfile, 'a+'))\n",
    "#     print('-'*50, file=open(logfile, 'a+'))\n",
    "#     print(f'Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT ', file=open(logfile, 'a+'))\n",
    "#     print(f':::::: : {mAP_i[\"mAP\"]:.4f} | {mAP_v[\"mAP\"]:.4f} | {mAP_t[\"mAP\"]:.4f} | {mAP_iv[\"mAP\"]:.4f} | {mAP_it[\"mAP\"]:.4f} | {mAP_ivt[\"mAP\"]:.4f} ', file=open(logfile, 'a+'))\n",
    "#     print('='*50, file=open(logfile, 'a+'))\n",
    "#     print(\"Test results saved @ \", logfile)\n",
    "\n",
    "    # direct print\n",
    "    print('-'*50)\n",
    "    print('Test Results\\nPer-category AP:')\n",
    "    print(f'I   : {mAP_i[\"AP\"]}')\n",
    "    print(f'V   : {mAP_v[\"AP\"]}')\n",
    "    print(f'T   : {mAP_t[\"AP\"]}')\n",
    "    print(f'IV  : {mAP_iv[\"AP\"]}')\n",
    "    print(f'IT  : {mAP_it[\"AP\"]}')\n",
    "    print(f'IVT : {mAP_ivt[\"AP\"]}')\n",
    "    print('-'*50)\n",
    "    print(f'Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT ')\n",
    "    print(f':::::: : {mAP_i[\"mAP\"]:.4f} | {mAP_v[\"mAP\"]:.4f} | {mAP_t[\"mAP\"]:.4f} | {mAP_iv[\"mAP\"]:.4f} | {mAP_it[\"mAP\"]:.4f} | {mAP_ivt[\"mAP\"]:.4f}')\n",
    "    print('='*50)\n",
    "\n",
    "    print(f\"Top-5 Accuracy: {total_top5_accuracy:.4f}\")\n",
    "    print(f\"Top-10 Accuracy: {total_top10_accuracy:.4f}\")\n",
    "\n",
    "# print(\"All done!\\nShutting done...\\nIt is what it is ...\\nC'est finis! {}\".format(\"-\"*maxlen) , file=open(logfile, 'a+'))\n",
    "# print(\"All done!\\nShutting done...\\nIt is what it is ...\\nC'est finis! {}\".format(\"-\"*maxlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3004d8",
   "metadata": {
    "papermill": {
     "duration": 0.185857,
     "end_time": "2024-08-09T17:23:58.963054",
     "exception": false,
     "start_time": "2024-08-09T17:23:58.777197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following are the results collected from various ablation experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4859c445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T17:23:59.338435Z",
     "iopub.status.busy": "2024-08-09T17:23:59.337367Z",
     "iopub.status.idle": "2024-08-09T17:23:59.346302Z",
     "shell.execute_reply": "2024-08-09T17:23:59.345280Z"
    },
    "papermill": {
     "duration": 0.199333,
     "end_time": "2024-08-09T17:23:59.348373",
     "exception": false,
     "start_time": "2024-08-09T17:23:59.149040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Results of ablation study on the Improved WSL Module\n",
    "\n",
    "# # Original WSL:\n",
    "# --------------------------------------------------\n",
    "# Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT \n",
    "# :::::: : 0.8534 | 0.5683 | 0.3776 | 0.2993 | 0.2716 | 0.2047\n",
    "# # WSL + Conv2:\n",
    "# --------------------------------------------------\n",
    "# Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT \n",
    "# :::::: : 0.8611 | 0.5676 | 0.3782 | 0.2989 | 0.2718 | 0.2094\n",
    "# # WSL + Attention:\n",
    "# --------------------------------------------------\n",
    "# Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT \n",
    "# :::::: : 0.8867 | 0.5681 | 0.3780 | 0.3028 | 0.2802 | 0.2061\n",
    "# # WSL + Conv2 + Attention:\n",
    "# --------------------------------------------------\n",
    "# Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT \n",
    "# :::::: : 0.8952 | 0.5685 | 0.3779 | 0.3031 | 0.2794 | 0.2108\n",
    "                            \n",
    "                \n",
    "        \n",
    "# Results of ablation study on the Improved CAGAM Module\n",
    "\n",
    "# # Original CAGAM:\n",
    "# --------------------------------------------------\n",
    "# Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT \n",
    "# :::::: : 0.8527 | 0.5681 | 0.3774 | 0.2992 | 0.2713 | 0.2038\n",
    "# # CAGAM + Muti-Head Attention:\n",
    "# --------------------------------------------------\n",
    "# Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT \n",
    "# :::::: : 0.8533 | 0.5883 | 0.3927 | 0.3002 | 0.2778 | 0.2141\n",
    "# # CAGAM + Batch Normalization\n",
    "# --------------------------------------------------\n",
    "# Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT \n",
    "# :::::: : 0.8531 | 0.5726 | 0.3825 | 0.2984 | 0.2723 | 0.2071\n",
    "# # CAGAM + Both Improvement\n",
    "# --------------------------------------------------\n",
    "# Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT \n",
    "# :::::: : 0.8529 | 0.5923 | 0.3961 | 0.3011 | 0.2804 | 0.2133\n",
    "                            \n",
    "\n",
    "    \n",
    "# Results of ablation study on the Improved MHMA Module\n",
    "\n",
    "# # Original MHMA:\n",
    "# --------------------------------------------------\n",
    "# Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT \n",
    "# :::::: : 0.8525 | 0.5687 | 0.3773 | 0.2986 | 0.2726 | 0.2043\n",
    "# # MHMA + LSTM:\n",
    "# --------------------------------------------------\n",
    "# Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT \n",
    "# :::::: : 0.8602 | 0.5671 | 0.3788 | 0.3144 | 0.2965 | 0.2261\n",
    "                            \n",
    "\n",
    "    \n",
    "# # Results of ablation study on the Dynamic Convolution (DC) Module\n",
    "\n",
    "# # Rendezvous:\n",
    "# --------------------------------------------------\n",
    "# Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT \n",
    "# :::::: : 0.8531 | 0.5683 | 0.3784 | 0.2993 | 0.2718 | 0.2052\n",
    "# ==================================================\n",
    "# Top-5 Accuracy: 0.7633\n",
    "# Top-10 Accuracy: 0.8871\n",
    "# ==================================================\n",
    "\n",
    "# # Rendezvous + DC:\n",
    "# --------------------------------------------------\n",
    "# Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT \n",
    "# :::::: : 0.8544 | 0.5710 | 0.3792 | 0.3076 | 0.2853 | 0.2382\n",
    "# ==================================================\n",
    "# Top-5 Accuracy: 0.7814\n",
    "# Top-10 Accuracy: 0.9019\n",
    "# =================================================="
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5259050,
     "sourceId": 8754417,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2382.550308,
   "end_time": "2024-08-09T17:24:02.378061",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-09T16:44:19.827753",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
